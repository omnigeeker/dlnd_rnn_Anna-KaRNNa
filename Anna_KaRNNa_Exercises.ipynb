{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa 安娜-卡列尼娜\n",
    "\n",
    "In this notebook, we'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "在这本笔记本中，我们将建立一个训练有素的RNN，她是我最喜爱的书籍之一安娜·卡列宁娜（Anna Karenina）。 它将能够根据书中的文本生成新的文本。\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "这个网络是基于Andrej Karpathy的[在RNN上发帖](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 和[在Torch中实现的](https://github.com/karpathy/char-rnn)。 另外，一些信息[这里是r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html)和[Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) 在GitHub上。 以下是字符式RNN的一般架构。\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network.\n",
    "\n",
    "首先，我们将加载文本文件并将其转换为整数，以供我们的网络使用。 在这里我创建一个几个字典来转换字符到整数。 将字符编码为整数可以更容易地用作网络中的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text lenth=1985223, Chapter 1   Happy families are all alike; every unhappy family is unhappy in its own way.  Everythin\n",
      "vocab length=83, {'H', '/', 'I', 'f', 'l', 'd', 'x', 'M', '(', '0', '?', '4', 'F', 'S', 'k', 'g', '%', 'a', 'V', 'Q', 'v', 'Y', '_', '9', '.', 'P', ')', '!', 'e', ',', 'p', '5', 'm', 'o', 'j', 'R', 'G', 'q', 'w', 'B', 's', 'n', '8', 'K', ' ', ':', 'r', 'O', 'J', 'b', 'U', 'y', 'E', 'T', 'W', 'L', '3', '@', 'X', 'Z', ';', 't', '1', 'C', '\\n', 'u', 'A', 'z', '2', '7', '`', \"'\", '&', '$', 'D', 'c', '6', 'h', 'N', '*', '\"', '-', 'i'}\n",
      "vocab_to_int length=83,  {'H': 0, '/': 1, 'I': 2, 'f': 3, 'l': 4, 'd': 5, 'x': 6, 'M': 7, '(': 8, '0': 9, '?': 10, '4': 11, 'F': 12, 'S': 13, 'k': 14, 'g': 15, '%': 16, 'a': 17, 'V': 18, 'Q': 19, 'v': 20, 'Y': 21, '_': 22, '9': 23, '.': 24, 'P': 25, ')': 26, '!': 27, 'e': 28, ',': 29, 'p': 30, '5': 31, 'm': 32, 'o': 33, 'j': 34, 'R': 35, 'G': 36, 'q': 37, 'w': 38, 'B': 39, 's': 40, 'n': 41, '8': 42, 'K': 43, ' ': 44, ':': 45, 'r': 46, 'O': 47, 'J': 48, 'b': 49, 'U': 50, 'y': 51, 'E': 52, 'T': 53, 'W': 54, 'L': 55, '3': 56, '@': 57, 'X': 58, 'Z': 59, ';': 60, 't': 61, '1': 62, 'C': 63, '\\n': 64, 'u': 65, 'A': 66, 'z': 67, '2': 68, '7': 69, '`': 70, \"'\": 71, '&': 72, '$': 73, 'D': 74, 'c': 75, '6': 76, 'h': 77, 'N': 78, '*': 79, '\"': 80, '-': 81, 'i': 82}\n",
      "int_to_vocab length=83,  {0: 'H', 1: '/', 2: 'I', 3: 'f', 4: 'l', 5: 'd', 6: 'x', 7: 'M', 8: '(', 9: '0', 10: '?', 11: '4', 12: 'F', 13: 'S', 14: 'k', 15: 'g', 16: '%', 17: 'a', 18: 'V', 19: 'Q', 20: 'v', 21: 'Y', 22: '_', 23: '9', 24: '.', 25: 'P', 26: ')', 27: '!', 28: 'e', 29: ',', 30: 'p', 31: '5', 32: 'm', 33: 'o', 34: 'j', 35: 'R', 36: 'G', 37: 'q', 38: 'w', 39: 'B', 40: 's', 41: 'n', 42: '8', 43: 'K', 44: ' ', 45: ':', 46: 'r', 47: 'O', 48: 'J', 49: 'b', 50: 'U', 51: 'y', 52: 'E', 53: 'T', 54: 'W', 55: 'L', 56: '3', 57: '@', 58: 'X', 59: 'Z', 60: ';', 61: 't', 62: '1', 63: 'C', 64: '\\n', 65: 'u', 66: 'A', 67: 'z', 68: '2', 69: '7', 70: '`', 71: \"'\", 72: '&', 73: '$', 74: 'D', 75: 'c', 76: '6', 77: 'h', 78: 'N', 79: '*', 80: '\"', 81: '-', 82: 'i'}\n",
      "encoded lenth=1985223, [63 77 17 30 61 28 46 44 62 64 64 64  0 17 30 30 51 44  3 17 32 82  4 82 28\n",
      " 40 44 17 46 28 44 17  4  4 44 17  4 82 14 28 60 44 28 20 28 46 51 44 65 41\n",
      " 77 17 30 30 51 44  3 17 32 82  4 51 44 82 40 44 65 41 77 17 30 30 51 44 82\n",
      " 41 44 82 61 40 44 33 38 41 64 38 17 51 24 64 64 52 20 28 46 51 61 77 82 41]\n"
     ]
    }
   ],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "print(\"text lenth={}, {}\".format(len(text), text[:100].replace(\"\\n\",\" \")))\n",
    "vocab = set(text)\n",
    "print(\"vocab length={}, {}\".format(len(vocab), vocab))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "print(\"vocab_to_int length={},  {}\".format(len(vocab_to_int), vocab_to_int))\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "print(\"int_to_vocab length={},  {}\".format(len(int_to_vocab), int_to_vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print(\"encoded lenth={}, {}\".format(len(encoded), encoded[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever.\n",
    "\n",
    "我们来看看前100个角色，确保一切都是桃红色的。 根据[American Book Review](http://americanbookreview.org/100bestlines.asp)，这是一本书中第六好的第一行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers.\n",
    "\n",
    "我们可以看到编码为整数的字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62, 66, 60, 18, 47, 22,  1, 12, 13, 76, 76, 76,  0, 60, 18, 18, 61,\n",
       "       12, 63, 60, 80, 59, 70, 59, 22, 54, 12, 60,  1, 22, 12, 60, 70, 70,\n",
       "       12, 60, 70, 59, 42, 22, 21, 12, 22, 67, 22,  1, 61, 12, 79, 20, 66,\n",
       "       60, 18, 18, 61, 12, 63, 60, 80, 59, 70, 61, 12, 59, 54, 12, 79, 20,\n",
       "       66, 60, 18, 18, 61, 12, 59, 20, 12, 59, 47, 54, 12, 71,  6, 20, 76,\n",
       "        6, 60, 61, 17, 76, 76, 51, 67, 22,  1, 61, 47, 66, 59, 20], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from.\n",
    "\n",
    "由于网络正在处理单个角色，它类似于我们尝试预测上一个文本中的下一个字符的分类问题。 这是我们的网络有多少'classes' 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches  制作训练小批量\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "这里是我们的小批量训练。记住，我们希望我们的批次是一些所需数量的序列步骤的多个序列。考虑到一个简单的例子，我们的批次将如下所示：\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "<br>\n",
    "We have our text encoded as integers as one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "我们将文本编码为“encoded”中的一个长整型数组。让我们创建一个函数，为我们的批量提供一个迭代器。我喜欢使用[generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) 来执行此操作。然后我们可以将`encoded`传递给这个函数，并得到我们的批生成器。\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is **the batch size (the number of sequences)** and $M$ is **the number of steps**. Then, to get the number of batches we can make from some array `arr`, you divide the length of `arr` by the batch size. Once you know the number of batches and the batch size, you can get the total number of characters to keep.\n",
    "\n",
    "我们需要做的第一件事是丢弃一些文本，所以我们只有完全批量。每个批次包含 $N \\times M$ 个字符，其中 $N$ 是批量大小（序列数）**和 $M$ 是**步数**。然后，为了获取我们可以从某个数组`arr`中得到的批次数，你可以将`arr`的长度除以批量大小。一旦您知道批次数和批量大小，您可以获得要保留的字符总数。\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`n_seqs` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$ where $K$ is the number of batches.\n",
    "\n",
    "之后，我们需要将`arr`分解成 $N$ 序列。您可以使用`arr.reshape（size）`这样做，其中`size`是一个包含重构数组的维度大小的元组。我们知道我们要 $N$ 序列(`n_seqs` 下面)。下面是对于第二个维度，您可以使用“-1”作为大小的占位符，它将使用适当的数据填充数组。之后，您应该有一个数组$ N\\times (M * K)$其中$ K $是批次数。\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. You'll usually see the first input character used as the last target character, so something like this:\n",
    "```python\n",
    "y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "```\n",
    "where `x` is the input batch and `y` is the target batch.\n",
    "\n",
    "现在我们有这个数组，我们可以遍历它来获取我们的批次。这个想法是每个批次都是一个$ N \\ times M $窗口上的数组。对于每个后续批处理，窗口移动“n_steps”。我们也想创建输入和目标数组。请记住，目标是输入一个字符以上。您通常会看到用作最后一个目标字符的第一个输入字符，因此如下所示：\n",
    "```python\n",
    "y [:, -1]，y [:, -1] = x [:, 1：]，x [:, 0]\n",
    "```\n",
    "其中`x`是输入批，`y`是目标批。\n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide.\n",
    "\n",
    "我喜欢做这个窗口的方式是使用`range`来从 $0$ 到`arr.shape [1]`的大小`n_steps`的步骤，这是每个序列中的总步数。这样，你从“range”得到的整数总是指向批处理的开始，每个窗口都是`n_steps`。\n",
    "\n",
    "> **Exercise:** Write the code for creating batches in the function below. The exercises in this notebook _will not be easy_. I've provided a notebook with solutions alongside this notebook. If you get stuck, checkout the solutions. The most important thing is that you don't copy and paste the code into here, **type out the solution code yourself.**\n",
    "\n",
    "> **练习：**在下面的函数中编写用于创建批次的代码。这本笔记本的练习不会很简单。笔记本电脑为笔记本提供了解决方案。如果卡住了，请检查解决方案。最重要的是你不要将代码复制粘贴到这里，**自己输入解决方案代码**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps.\n",
    "\n",
    "现在我将做我的数据集，我们可以看看这里发生了什么。 在这里，我将使用10和50个序列步骤的批量大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[63 77 17 30 61 28 46 44 62 64]\n",
      " [44 17 32 44 41 33 61 44 15 33]\n",
      " [20 82 41 24 64 64 80 21 28 40]\n",
      " [41 44  5 65 46 82 41 15 44 77]\n",
      " [44 82 61 44 82 40 29 44 40 82]\n",
      " [44  2 61 44 38 17 40 64 33 41]\n",
      " [77 28 41 44 75 33 32 28 44  3]\n",
      " [60 44 49 65 61 44 41 33 38 44]\n",
      " [61 44 82 40 41 71 61 24 44 53]\n",
      " [44 40 17 82  5 44 61 33 44 77]]\n",
      "\n",
      "y\n",
      " [[77 17 30 61 28 46 44 62 64 64]\n",
      " [17 32 44 41 33 61 44 15 33 82]\n",
      " [82 41 24 64 64 80 21 28 40 29]\n",
      " [44  5 65 46 82 41 15 44 77 82]\n",
      " [82 61 44 82 40 29 44 40 82 46]\n",
      " [ 2 61 44 38 17 40 64 33 41  4]\n",
      " [28 41 44 75 33 32 28 44  3 33]\n",
      " [44 49 65 61 44 41 33 38 44 40]\n",
      " [44 82 40 41 71 61 24 44 53 77]\n",
      " [40 17 82  5 44 61 33 44 77 28]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model 建模\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "以下是您建立网络的地方。 我们会把它分成几部分，因此每一点都更容易理解。 然后我们可以将它们连接到整个网络。\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs 输入\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`. This will be a scalar, that is a 0-D tensor. To make a scalar, you create a placeholder without giving it a size.\n",
    "\n",
    "首先我们将创建我们的输入占位符。 像往常一样，我们需要占位符的训练数据和目标。 我们还将为名为`keep_prob`的辍学层创建一个占位符。 这将是一个标量，即0-D张量。 要制作标量，您创建一个占位符，而不给它一个大小。\n",
    "\n",
    "> **Exercise:** Create the input placeholders in the function below.\n",
    "\n",
    "> **练习：**在下面的函数中创建输入占位符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    return inputs, targets, keep_prob\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "这里我们将创建我们将在隐藏层中使用的LSTM单元格。我们将使用这个单元作为RNN的构建块。所以我们并不是在这里定义RNN，只是我们在隐藏层中使用的单元格类型。\n",
    "\n",
    "我们首先创建一个基本的LSTM单元格\n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "where `num_units` is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with \n",
    "\n",
    "其中`num_units`是单元格中隐藏层中的单位数。然后我们可以通过包装来添加退出\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "```\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell). With this, you pass in a list of cells and it will send the output of one cell into the next cell. For example,\n",
    "\n",
    "你传递一个单元格，它将自动将输出或输出添加到输出。最后，我们可以使用[`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/)将LSTM单元格堆叠成层MultiRNNCell）。通过这个，你传递一个单元格列表，它将一个单元格的输出发送到下一个单元格。例如，\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "```\n",
    "\n",
    "This might look a little weird if you know Python well because this will create a list of the same `cell` object. However, TensorFlow will create different weight matrices for all `cell` objects. Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "如果您很好地了解Python，这可能会有点奇怪，因为这将创建一个相同的 `cell`对象的列表。然而，TensorFlow将为所有`cell` 对象创建不同的权重矩阵。即使这实际上是多个LSTM单元堆叠在一起，您可以将多个层作为一个单元。\n",
    "\n",
    "我们还需要创建全零的初始单元格状态。这可以这样做\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "\n",
    "> **Exercise:** Below, implement the `build_lstm` function to create these LSTM cells and the initial state.\n",
    "\n",
    "**练习：**下面，实现`build_lstm`函数创建这些LSTM单元格和初始状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell outputs\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop]*num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output RNN输出\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text.\n",
    "\n",
    "这里我们将创建输出层。我们需要将RNN单元的输出连接到具有softmax输出的完整连接层。 softmax输出给出了我们可以用来预测下一个字符的概率分布，所以我们希望这个层的大小为 $C$，我们在文本中的类/字符数。\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$. \n",
    "\n",
    "如果我们的输入有批量大小 $N$，步数 $M$，隐藏层有 $L$ 隐藏单位，那么输出是一个尺寸为 $N \\times M \\times L$。每个LSTM单元格的输出都具有 $L$ 的大小，我们有 $M$，每个序列步长一个，我们有 $N$序列。所以总大小是 $N \\times M \\times L$。\n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells. We get the LSTM output as a list, `lstm_output`. First we need to concatenate this whole list into one array with [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat). Then, reshape it (with `tf.reshape`) to size $(M * N) \\times L$.\n",
    "\n",
    "我们正在为每个输出使用相同的完全连接的层，相同的权重。然后，为了使事情更容易，我们应该将输出重新形成一个2D张量，形状为$(M * N) \\times L$。也就是说，对于每个序列和步骤，每行的一行是来自LSTM单元格的输出。我们将LSTM输出作为列表，`lstm_output`。首先，我们需要使用[`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat) 将这个整个列表连接成一个数组。然后，重新整理(用`tf.reshape`)来缩放 $(M * N) \\times L$。\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names.\n",
    "\n",
    "我们有一个输出重新整形，我们可以用权重进行矩阵乘法。因为在LSTM单元格中创建权重，所以我们需要将权重和偏差变量包含在`tf.variable_scope(scope_name)`中。如果这里创建的权重与LSTM单元格中创建的权重具有相同的名称，则TensorFlow将抛出一个错误，它们将是默认值。为了避免这种情况，我们将变量包装在可变范围内，以便我们可以给它们唯一的名称。\n",
    "\n",
    "> **Exercise:** Implement the output layer in the function below.\n",
    "\n",
    "> **练习：**在下面的函数中实现输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.add(tf.matmul(x, softmax_w), softmax_b)\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits)\n",
    "    \n",
    "    return out, logits\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss 训练损失\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "接下来是训练损失。 我们得到对数和目标并计算softmax交叉熵损失。 首先，我们需要对目标进行一次热编码，我们将它们作为编码字符。 然后，重塑一个热目标，所以它是一个2D张量，大小为$(M*N）\\times C$，其中$ C $是我们拥有的类/字符数。 记住，我们重塑了LSTM输出，并通过一个完全连接的层以 $C$ 单位运行它们。 所以我们的logits也将有大小 $(M*N) \\times C$。\n",
    "\n",
    "Then we run the logits and targets through `tf.nn.softmax_cross_entropy_with_logits` and find the mean to get the loss.\n",
    "\n",
    "然后我们通过`tf.nn.softmax_cross_entropy_with_logits`运行对象和目标，找到得到损失的意思。\n",
    "\n",
    ">**Exercise:** Implement the loss calculation in the function below.\n",
    "\n",
    "> **练习：**在下面的函数中实现损失计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer 优化器\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step.\n",
    "\n",
    "这里我们构建优化器。 正常的RNN有问题的梯度爆炸和消失。 LSTM解决了消失问题，但梯度仍然无限制地增长。 为了解决这个问题，我们可以将梯度剪切在某个阈值以上。 也就是说，如果梯度大于该阈值，我们将其设置为阈值。 这将确保梯度不会变得过大。 然后我们使用一个AdamOptimizer来进行学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network 构建网络\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN. \n",
    "\n",
    "现在我们可以将所有的部分放在一起，并为网络建立一个类。 要通过LSTM单元实际运行数据，我们将使用[`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn)。 这个功能将适用于我们的LSTM单元格中的隐藏和单元格状态。 它为每个步骤为每个LSTM单元格输出每个序列的小批量。 它也给了我们最终的LSTM状态。 我们想把这个状态保存为`final_state`，所以我们可以把它传递给下一个小批量运行的第一个LSTM单元。 对于`tf.nn.dynamic_rnn`，我们传递我们从`build_lstm`获取的单元格和初始状态，以及我们的输入序列。 此外，我们需要在进入RNN之前对输入进行一次热编码。\n",
    "\n",
    "> **Exercise:** Use the functions you've implemented previously and `tf.nn.dynamic_rnn` to build the network.\n",
    "\n",
    "> **练习：**使用以前实现的功能和`tf.nn.dynamic_rnn`构建网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer =  build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters 超参数\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "以下是网络的超参数。\n",
    "\n",
    "* `batch_size` - 一次通过网络运行的序列数。\n",
    "* `num_steps` - 训练网络的序列中的字符数。 更大的通常更好，网络将学习更多的远程依赖。 但训练需要更长时间。 这里100通常是很好的数字。\n",
    "* `lstm_size` - 隐藏图层中的单位数。\n",
    "* `num_layers` - 要使用的隐藏的LSTM图层的数量\n",
    "* `learning_rate` - 训练学习率\n",
    "* `keep_prob` - 辍学在训练时保持概率。 如果网络过度配置，请尝试减少这个。\n",
    "\n",
    "这是Andrej Karpathy在训练网络方面的一些很好的建议。 我将在这里复制它为您的利益，但也链接到[原来来自哪里](https://github.com/karpathy/char-rnn#tips-and-tricks)。\n",
    "\n",
    "> ## Tips and Tricks  提示和技巧\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss 监控验证损失与训练损失\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    ">如果您对机器学习或神经网络有一些新意，那么可以获得一些专业知识来获得好的模型。 要跟踪的最重要的数量是您的训练损失（在训练期间打印）和验证损失（当RNN在验证数据上运行（默认情况下每1000次迭代）打印一次）之间的差异）。 尤其是：\n",
    "\n",
    "> - 如果您的训练损失远低于验证损失，那么这意味着网络可能会**过度**。 解决方案是减少您的网络大小，或增加辍学率。 例如，您可以尝试退出0.5，依此类推。\n",
    "> - 如果你的训练/验证损失大致相等，那么你的模型是**配合**。 增加你的模型的大小（层数或每层神经元的原始数量）\n",
    "\n",
    "> ### Approximate number of parameters  参数的大概数量\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    ">控制模型的两个最重要的参数是`lstm_size`和`num_layers`。我建议你总是使用`num_layers`的2/3。 `lstm_size`可以根据您拥有的数据进行调整。跟踪这里的两个重要数量是：\n",
    "\n",
    "> - 模型中的参数数量。当您开始训练时打印。\n",
    "> - 数据集的大小。 1MB文件约100万个字符。\n",
    "\n",
    ">这两个应该大致相同的数量级。这有点棘手。这里有些例子：\n",
    "\n",
    "> - 我有一个100MB数据集，我使用默认参数设置（目前打印150K参数）。我的数据大小明显更大（100 mil >> 0.15 mil），所以我预期会大量装备。我想我可以舒服地使`lstm_size`更大。\n",
    "> - 我有一个10MB的数据集，并运行一个1000万参数模型。我稍微紧张，我正在仔细监测我的验证损失。如果它比我的训练损失大，那么我可能想尝试增加辍学率，看看这是否有助于验证损失。\n",
    "\n",
    "> ### Best models strategy 最佳模型策略\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n",
    "\n",
    ">获得非常好的模型（如果您有计算时间）的获胜策略是始终错误地使网络更大（如您愿意等待计算的那样大），然后尝试不同的退出值（在0之间），1）。 无论什么型号具有最佳的验证性能（损失，写在检查点文件名，低是好的）是最后应该使用的。\n",
    "\n",
    ">在深入学习中运行许多不同的超级参数设置的不同模型是非常常见的，最终可以使任何检查点得到最佳的验证性能。\n",
    "\n",
    ">顺便说一下，你的训练和验证分裂的大小也是参数。 确保您的验证集中有大量的数据，否则验证性能将会很嘈杂，信息量不是很高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for training 是时候训练了\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I save a checkpoint.\n",
    "\n",
    "这是典型的训练代码，将输入和目标传递到网络中，然后运行优化器。 在这里，我们还可以获得最终的LSTM状态。 然后，我们将该状态传回网络，以便下一批可以继续上一批的状态。 并且经常（由`save_every_n`设置）我保存一个检查点。\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "在这里，我正在使用格式保存检查点\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}.ckpt`\n",
    "\n",
    "> **Exercise:** Set the hyperparameters above to train the network. Watch the training loss, it should be consistently dropping. Also, I highly advise running this on a GPU.\n",
    "\n",
    "> **练习：**设置上面的超级参数来训练网络。 观看训练损失，应该一直下降。 此外，我非常建议在GPU上运行这个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.4191...  0.1308 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.3261...  0.1070 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 3.8367...  0.1004 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 4.8918...  0.1088 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 4.1754...  0.0941 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.8827...  0.1005 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.7040...  0.1072 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.5549...  0.1280 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.4717...  0.1214 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.4291...  0.1014 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.3857...  0.1286 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.3840...  0.1295 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3564...  0.1301 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3495...  0.1195 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.3395...  0.1308 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.3003...  0.1230 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.2938...  0.0903 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.3100...  0.1255 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.2939...  0.1270 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2528...  0.1207 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2700...  0.1183 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2544...  0.1278 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2462...  0.1218 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.2360...  0.0902 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.2194...  0.0904 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.2374...  0.0918 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.2317...  0.0903 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.2083...  0.0904 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.2172...  0.0932 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2220...  0.0908 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2332...  0.0896 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.2014...  0.0946 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.1938...  0.1247 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.2065...  0.1239 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.1827...  0.1254 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.2026...  0.1016 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1734...  0.0910 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1657...  0.0949 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1714...  0.0944 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1689...  0.0907 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1563...  0.0899 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1706...  0.0950 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1608...  0.0900 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.1610...  0.0899 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1459...  0.0944 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1666...  0.0981 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1673...  0.0902 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.1750...  0.0941 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1614...  0.0900 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1645...  0.0906 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1590...  0.0941 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1524...  0.0938 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.1544...  0.0907 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.1454...  0.0939 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1548...  0.0900 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.1357...  0.0913 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.1428...  0.0923 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.1512...  0.0903 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1332...  0.0905 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1469...  0.0932 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.1424...  0.0901 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.1605...  0.0907 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1659...  0.0934 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1188...  0.0906 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1200...  0.0899 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1500...  0.0926 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1472...  0.0899 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.0921...  0.1075 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1176...  0.1121 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1279...  0.1100 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1224...  0.1098 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1441...  0.1065 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.1224...  0.1102 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 3.1250...  0.1122 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 3.1294...  0.1059 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 3.1382...  0.1170 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.1259...  0.0917 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.1253...  0.0933 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.1145...  0.0900 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.0984...  0.0940 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.1030...  0.0903 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1179...  0.0904 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.1257...  0.0920 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.1299...  0.0906 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.1032...  0.0925 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.1054...  0.0925 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.1017...  0.0985 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 3.1021...  0.1011 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 3.1159...  0.1004 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 3.1103...  0.1003 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 3.1024...  0.1010 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.0970...  0.1014 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.0946...  0.0954 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.0959...  0.0907 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.0882...  0.0919 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.0863...  0.0900 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.0959...  0.0905 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.0813...  0.0926 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.0832...  0.0913 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.0762...  0.0903 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.0785...  0.0965 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.0735...  0.1192 sec/batch\n",
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.0698...  0.1297 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.0586...  0.1035 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.0542...  0.0909 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.0578...  0.0932 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.0338...  0.0910 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.0383...  0.0912 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.0486...  0.0930 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 3.0065...  0.0904 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 3.0241...  0.0905 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 3.0155...  0.0961 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 3.1125...  0.1286 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 3.0336...  0.1315 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 3.0476...  0.1192 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 3.0390...  0.1161 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 3.0359...  0.1247 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 3.0422...  0.1311 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 3.0323...  0.1167 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 3.0178...  0.0903 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 3.0452...  0.0926 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 3.0251...  0.0905 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 3.0163...  0.0925 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 3.0266...  0.0903 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 2.9899...  0.0909 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 2.9750...  0.0935 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 2.9971...  0.1156 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 2.9926...  0.1240 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 2.9736...  0.1307 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 2.9743...  0.1165 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 2.9785...  0.0910 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 2.9403...  0.0906 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 2.9488...  0.0934 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 2.9402...  0.0906 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 2.9039...  0.0912 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 2.9034...  0.0931 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 2.9088...  0.0906 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 2.8915...  0.0904 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 2.9149...  0.0927 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 2.8936...  0.0906 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 2.8982...  0.0906 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 2.8580...  0.0927 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 2.8683...  0.0909 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 2.8529...  0.0936 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 2.8611...  0.0959 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 2.8758...  0.0914 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 2.8590...  0.0929 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 2.8675...  0.0943 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 2.8341...  0.0905 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.8292...  0.0926 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 2.8485...  0.0937 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 2.8605...  0.0911 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 2.8145...  0.1047 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 2.8196...  0.0958 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 2.7879...  0.0919 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 2.7873...  0.0908 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 2.7625...  0.0934 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 2.7583...  0.0911 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 2.7333...  0.0983 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 2.7665...  0.0987 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 2.7421...  0.0973 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 2.7164...  0.0968 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 2.6966...  0.0978 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 2.7208...  0.0985 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 2.7096...  0.0980 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 2.6909...  0.0997 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 2.7107...  0.0975 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 2.6704...  0.0959 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 2.6735...  0.0975 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 2.6431...  0.1027 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 2.6758...  0.0941 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 2.6778...  0.1064 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 2.6896...  0.0937 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 2.6713...  0.0938 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 2.6519...  0.0957 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 2.6265...  0.0936 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 2.6140...  0.0929 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 2.5848...  0.0945 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 2.5815...  0.0917 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 2.5629...  0.0905 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 2.5846...  0.0937 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 2.5789...  0.0903 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 2.5568...  0.0911 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 2.5860...  0.0931 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 2.6517...  0.0907 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 2.6980...  0.0907 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 2.6722...  0.0937 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 2.5474...  0.0904 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 2.5295...  0.0907 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 2.5347...  0.0925 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 2.5409...  0.0910 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 2.5048...  0.0905 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 2.5381...  0.0933 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 2.5196...  0.0917 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 2.5028...  0.0912 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 2.5000...  0.0936 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 2.4993...  0.0909 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 2.4927...  0.0920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 2.5704...  0.0936 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4714...  0.0913 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 2.4857...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 202...  Training loss: 2.4864...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 203...  Training loss: 2.4937...  0.0905 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 2.4778...  0.0927 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 2.4856...  0.0904 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 2.4913...  0.0917 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 2.5028...  0.0933 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 2.4634...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 2.4710...  0.0910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 2.4696...  0.0935 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 2.4643...  0.0909 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 2.4989...  0.0908 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 2.4724...  0.0934 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 2.4586...  0.0905 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 2.4656...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 2.4876...  0.0927 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 2.4627...  0.0929 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 2.4263...  0.0902 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 2.4304...  0.0927 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 2.4722...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 2.4477...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 2.4316...  0.0931 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 2.4270...  0.0927 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 2.4339...  0.0909 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 2.4245...  0.0928 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 2.4292...  0.0910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 2.4355...  0.0901 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 2.4329...  0.0930 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 2.4334...  0.0916 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 2.4034...  0.0914 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 2.3962...  0.0935 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 2.4223...  0.0907 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 2.4026...  0.0906 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 2.4032...  0.0921 sec/batch\n",
      "Epoch: 2/20...  Training Step: 235...  Training loss: 2.4010...  0.0914 sec/batch\n",
      "Epoch: 2/20...  Training Step: 236...  Training loss: 2.3722...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 237...  Training loss: 2.3836...  0.0936 sec/batch\n",
      "Epoch: 2/20...  Training Step: 238...  Training loss: 2.3831...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 239...  Training loss: 2.3894...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 240...  Training loss: 2.3879...  0.0935 sec/batch\n",
      "Epoch: 2/20...  Training Step: 241...  Training loss: 2.3739...  0.0908 sec/batch\n",
      "Epoch: 2/20...  Training Step: 242...  Training loss: 2.3793...  0.0910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 243...  Training loss: 2.3728...  0.0929 sec/batch\n",
      "Epoch: 2/20...  Training Step: 244...  Training loss: 2.3407...  0.0908 sec/batch\n",
      "Epoch: 2/20...  Training Step: 245...  Training loss: 2.3924...  0.0908 sec/batch\n",
      "Epoch: 2/20...  Training Step: 246...  Training loss: 2.3678...  0.0937 sec/batch\n",
      "Epoch: 2/20...  Training Step: 247...  Training loss: 2.3656...  0.0905 sec/batch\n",
      "Epoch: 2/20...  Training Step: 248...  Training loss: 2.3937...  0.0913 sec/batch\n",
      "Epoch: 2/20...  Training Step: 249...  Training loss: 2.3465...  0.0935 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.3798...  0.0906 sec/batch\n",
      "Epoch: 2/20...  Training Step: 251...  Training loss: 2.3618...  0.0907 sec/batch\n",
      "Epoch: 2/20...  Training Step: 252...  Training loss: 2.3513...  0.0932 sec/batch\n",
      "Epoch: 2/20...  Training Step: 253...  Training loss: 2.3512...  0.0904 sec/batch\n",
      "Epoch: 2/20...  Training Step: 254...  Training loss: 2.3642...  0.0906 sec/batch\n",
      "Epoch: 2/20...  Training Step: 255...  Training loss: 2.3589...  0.0930 sec/batch\n",
      "Epoch: 2/20...  Training Step: 256...  Training loss: 2.3463...  0.0905 sec/batch\n",
      "Epoch: 2/20...  Training Step: 257...  Training loss: 2.3397...  0.0910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 258...  Training loss: 2.3716...  0.0930 sec/batch\n",
      "Epoch: 2/20...  Training Step: 259...  Training loss: 2.3511...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 260...  Training loss: 2.3490...  0.0907 sec/batch\n",
      "Epoch: 2/20...  Training Step: 261...  Training loss: 2.3669...  0.0926 sec/batch\n",
      "Epoch: 2/20...  Training Step: 262...  Training loss: 2.3379...  0.0915 sec/batch\n",
      "Epoch: 2/20...  Training Step: 263...  Training loss: 2.3266...  0.0907 sec/batch\n",
      "Epoch: 2/20...  Training Step: 264...  Training loss: 2.3588...  0.0927 sec/batch\n",
      "Epoch: 2/20...  Training Step: 265...  Training loss: 2.3304...  0.0909 sec/batch\n",
      "Epoch: 2/20...  Training Step: 266...  Training loss: 2.3004...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 267...  Training loss: 2.3109...  0.0914 sec/batch\n",
      "Epoch: 2/20...  Training Step: 268...  Training loss: 2.3274...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 269...  Training loss: 2.3510...  0.0907 sec/batch\n",
      "Epoch: 2/20...  Training Step: 270...  Training loss: 2.3291...  0.0939 sec/batch\n",
      "Epoch: 2/20...  Training Step: 271...  Training loss: 2.3252...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 272...  Training loss: 2.3118...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 273...  Training loss: 2.3142...  0.0929 sec/batch\n",
      "Epoch: 2/20...  Training Step: 274...  Training loss: 2.3527...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 275...  Training loss: 2.3160...  0.0925 sec/batch\n",
      "Epoch: 2/20...  Training Step: 276...  Training loss: 2.3263...  0.0942 sec/batch\n",
      "Epoch: 2/20...  Training Step: 277...  Training loss: 2.2955...  0.0917 sec/batch\n",
      "Epoch: 2/20...  Training Step: 278...  Training loss: 2.3015...  0.0909 sec/batch\n",
      "Epoch: 2/20...  Training Step: 279...  Training loss: 2.2827...  0.0938 sec/batch\n",
      "Epoch: 2/20...  Training Step: 280...  Training loss: 2.3197...  0.0918 sec/batch\n",
      "Epoch: 2/20...  Training Step: 281...  Training loss: 2.2821...  0.0921 sec/batch\n",
      "Epoch: 2/20...  Training Step: 282...  Training loss: 2.2706...  0.0943 sec/batch\n",
      "Epoch: 2/20...  Training Step: 283...  Training loss: 2.2582...  0.0910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 284...  Training loss: 2.2797...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 285...  Training loss: 2.2807...  0.0920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 286...  Training loss: 2.2853...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 287...  Training loss: 2.2591...  0.0918 sec/batch\n",
      "Epoch: 2/20...  Training Step: 288...  Training loss: 2.2807...  0.0934 sec/batch\n",
      "Epoch: 2/20...  Training Step: 289...  Training loss: 2.2661...  0.0917 sec/batch\n",
      "Epoch: 2/20...  Training Step: 290...  Training loss: 2.2860...  0.0910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 291...  Training loss: 2.2568...  0.0939 sec/batch\n",
      "Epoch: 2/20...  Training Step: 292...  Training loss: 2.2517...  0.0918 sec/batch\n",
      "Epoch: 2/20...  Training Step: 293...  Training loss: 2.2548...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 294...  Training loss: 2.2542...  0.0933 sec/batch\n",
      "Epoch: 2/20...  Training Step: 295...  Training loss: 2.2670...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 296...  Training loss: 2.2637...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 297...  Training loss: 2.2469...  0.0937 sec/batch\n",
      "Epoch: 2/20...  Training Step: 298...  Training loss: 2.2429...  0.0910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 299...  Training loss: 2.2685...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2656...  0.0935 sec/batch\n",
      "Epoch: 2/20...  Training Step: 301...  Training loss: 2.2261...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 302...  Training loss: 2.2433...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 303...  Training loss: 2.2479...  0.0932 sec/batch\n",
      "Epoch: 2/20...  Training Step: 304...  Training loss: 2.2515...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 305...  Training loss: 2.2467...  0.0917 sec/batch\n",
      "Epoch: 2/20...  Training Step: 306...  Training loss: 2.2670...  0.0938 sec/batch\n",
      "Epoch: 2/20...  Training Step: 307...  Training loss: 2.2595...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 308...  Training loss: 2.2293...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 309...  Training loss: 2.2465...  0.0940 sec/batch\n",
      "Epoch: 2/20...  Training Step: 310...  Training loss: 2.2514...  0.0915 sec/batch\n",
      "Epoch: 2/20...  Training Step: 311...  Training loss: 2.2366...  0.0910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 312...  Training loss: 2.2234...  0.0930 sec/batch\n",
      "Epoch: 2/20...  Training Step: 313...  Training loss: 2.2149...  0.0909 sec/batch\n",
      "Epoch: 2/20...  Training Step: 314...  Training loss: 2.1939...  0.0917 sec/batch\n",
      "Epoch: 2/20...  Training Step: 315...  Training loss: 2.2271...  0.0931 sec/batch\n",
      "Epoch: 2/20...  Training Step: 316...  Training loss: 2.2267...  0.0910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 317...  Training loss: 2.2476...  0.0916 sec/batch\n",
      "Epoch: 2/20...  Training Step: 318...  Training loss: 2.2323...  0.0935 sec/batch\n",
      "Epoch: 2/20...  Training Step: 319...  Training loss: 2.2463...  0.0913 sec/batch\n",
      "Epoch: 2/20...  Training Step: 320...  Training loss: 2.2024...  0.0924 sec/batch\n",
      "Epoch: 2/20...  Training Step: 321...  Training loss: 2.2005...  0.0935 sec/batch\n",
      "Epoch: 2/20...  Training Step: 322...  Training loss: 2.2400...  0.0920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 323...  Training loss: 2.2131...  0.0914 sec/batch\n",
      "Epoch: 2/20...  Training Step: 324...  Training loss: 2.1880...  0.0936 sec/batch\n",
      "Epoch: 2/20...  Training Step: 325...  Training loss: 2.2241...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 326...  Training loss: 2.2305...  0.0917 sec/batch\n",
      "Epoch: 2/20...  Training Step: 327...  Training loss: 2.2222...  0.0930 sec/batch\n",
      "Epoch: 2/20...  Training Step: 328...  Training loss: 2.2147...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 329...  Training loss: 2.2018...  0.0914 sec/batch\n",
      "Epoch: 2/20...  Training Step: 330...  Training loss: 2.1891...  0.0934 sec/batch\n",
      "Epoch: 2/20...  Training Step: 331...  Training loss: 2.2209...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 332...  Training loss: 2.2122...  0.0915 sec/batch\n",
      "Epoch: 2/20...  Training Step: 333...  Training loss: 2.1969...  0.0940 sec/batch\n",
      "Epoch: 2/20...  Training Step: 334...  Training loss: 2.2040...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 335...  Training loss: 2.2063...  0.0914 sec/batch\n",
      "Epoch: 2/20...  Training Step: 336...  Training loss: 2.1995...  0.0941 sec/batch\n",
      "Epoch: 2/20...  Training Step: 337...  Training loss: 2.2324...  0.0916 sec/batch\n",
      "Epoch: 2/20...  Training Step: 338...  Training loss: 2.1970...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 339...  Training loss: 2.2127...  0.0941 sec/batch\n",
      "Epoch: 2/20...  Training Step: 340...  Training loss: 2.1829...  0.0921 sec/batch\n",
      "Epoch: 2/20...  Training Step: 341...  Training loss: 2.1916...  0.0915 sec/batch\n",
      "Epoch: 2/20...  Training Step: 342...  Training loss: 2.1915...  0.0927 sec/batch\n",
      "Epoch: 2/20...  Training Step: 343...  Training loss: 2.1791...  0.0917 sec/batch\n",
      "Epoch: 2/20...  Training Step: 344...  Training loss: 2.2075...  0.0920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 345...  Training loss: 2.1945...  0.0933 sec/batch\n",
      "Epoch: 2/20...  Training Step: 346...  Training loss: 2.2088...  0.0915 sec/batch\n",
      "Epoch: 2/20...  Training Step: 347...  Training loss: 2.1785...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 348...  Training loss: 2.1728...  0.0939 sec/batch\n",
      "Epoch: 2/20...  Training Step: 349...  Training loss: 2.2045...  0.0915 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.2200...  0.0917 sec/batch\n",
      "Epoch: 2/20...  Training Step: 351...  Training loss: 2.1998...  0.0936 sec/batch\n",
      "Epoch: 2/20...  Training Step: 352...  Training loss: 2.1906...  0.0905 sec/batch\n",
      "Epoch: 2/20...  Training Step: 353...  Training loss: 2.1671...  0.0913 sec/batch\n",
      "Epoch: 2/20...  Training Step: 354...  Training loss: 2.1757...  0.0936 sec/batch\n",
      "Epoch: 2/20...  Training Step: 355...  Training loss: 2.1678...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 356...  Training loss: 2.1616...  0.0914 sec/batch\n",
      "Epoch: 2/20...  Training Step: 357...  Training loss: 2.1441...  0.0934 sec/batch\n",
      "Epoch: 2/20...  Training Step: 358...  Training loss: 2.2100...  0.0920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 359...  Training loss: 2.1853...  0.0918 sec/batch\n",
      "Epoch: 2/20...  Training Step: 360...  Training loss: 2.1640...  0.0945 sec/batch\n",
      "Epoch: 2/20...  Training Step: 361...  Training loss: 2.1550...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 362...  Training loss: 2.1585...  0.0916 sec/batch\n",
      "Epoch: 2/20...  Training Step: 363...  Training loss: 2.1662...  0.0940 sec/batch\n",
      "Epoch: 2/20...  Training Step: 364...  Training loss: 2.1641...  0.0918 sec/batch\n",
      "Epoch: 2/20...  Training Step: 365...  Training loss: 2.1665...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 366...  Training loss: 2.1860...  0.0941 sec/batch\n",
      "Epoch: 2/20...  Training Step: 367...  Training loss: 2.1555...  0.0920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 368...  Training loss: 2.1458...  0.0916 sec/batch\n",
      "Epoch: 2/20...  Training Step: 369...  Training loss: 2.1378...  0.0938 sec/batch\n",
      "Epoch: 2/20...  Training Step: 370...  Training loss: 2.1496...  0.0913 sec/batch\n",
      "Epoch: 2/20...  Training Step: 371...  Training loss: 2.1666...  0.0920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 372...  Training loss: 2.1637...  0.0939 sec/batch\n",
      "Epoch: 2/20...  Training Step: 373...  Training loss: 2.1727...  0.0916 sec/batch\n",
      "Epoch: 2/20...  Training Step: 374...  Training loss: 2.1372...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 375...  Training loss: 2.1353...  0.0931 sec/batch\n",
      "Epoch: 2/20...  Training Step: 376...  Training loss: 2.1502...  0.0913 sec/batch\n",
      "Epoch: 2/20...  Training Step: 377...  Training loss: 2.1240...  0.0921 sec/batch\n",
      "Epoch: 2/20...  Training Step: 378...  Training loss: 2.1058...  0.0934 sec/batch\n",
      "Epoch: 2/20...  Training Step: 379...  Training loss: 2.1213...  0.0912 sec/batch\n",
      "Epoch: 2/20...  Training Step: 380...  Training loss: 2.1278...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 381...  Training loss: 2.1358...  0.0929 sec/batch\n",
      "Epoch: 2/20...  Training Step: 382...  Training loss: 2.1508...  0.0916 sec/batch\n",
      "Epoch: 2/20...  Training Step: 383...  Training loss: 2.1401...  0.0913 sec/batch\n",
      "Epoch: 2/20...  Training Step: 384...  Training loss: 2.1237...  0.0938 sec/batch\n",
      "Epoch: 2/20...  Training Step: 385...  Training loss: 2.1314...  0.0917 sec/batch\n",
      "Epoch: 2/20...  Training Step: 386...  Training loss: 2.1051...  0.0922 sec/batch\n",
      "Epoch: 2/20...  Training Step: 387...  Training loss: 2.1118...  0.0940 sec/batch\n",
      "Epoch: 2/20...  Training Step: 388...  Training loss: 2.1199...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 389...  Training loss: 2.1335...  0.0920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 390...  Training loss: 2.0887...  0.0943 sec/batch\n",
      "Epoch: 2/20...  Training Step: 391...  Training loss: 2.1213...  0.0919 sec/batch\n",
      "Epoch: 2/20...  Training Step: 392...  Training loss: 2.1142...  0.0918 sec/batch\n",
      "Epoch: 2/20...  Training Step: 393...  Training loss: 2.0972...  0.0943 sec/batch\n",
      "Epoch: 2/20...  Training Step: 394...  Training loss: 2.1224...  0.0916 sec/batch\n",
      "Epoch: 2/20...  Training Step: 395...  Training loss: 2.1132...  0.0911 sec/batch\n",
      "Epoch: 2/20...  Training Step: 396...  Training loss: 2.0889...  0.0935 sec/batch\n",
      "Epoch: 3/20...  Training Step: 397...  Training loss: 2.1870...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 398...  Training loss: 2.0829...  0.0921 sec/batch\n",
      "Epoch: 3/20...  Training Step: 399...  Training loss: 2.0841...  0.0936 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.0914...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 401...  Training loss: 2.0997...  0.0908 sec/batch\n",
      "Epoch: 3/20...  Training Step: 402...  Training loss: 2.0682...  0.0918 sec/batch\n",
      "Epoch: 3/20...  Training Step: 403...  Training loss: 2.1033...  0.0912 sec/batch\n",
      "Epoch: 3/20...  Training Step: 404...  Training loss: 2.0926...  0.0937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 405...  Training loss: 2.1263...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 406...  Training loss: 2.0884...  0.0920 sec/batch\n",
      "Epoch: 3/20...  Training Step: 407...  Training loss: 2.0745...  0.0936 sec/batch\n",
      "Epoch: 3/20...  Training Step: 408...  Training loss: 2.0702...  0.0912 sec/batch\n",
      "Epoch: 3/20...  Training Step: 409...  Training loss: 2.0905...  0.0911 sec/batch\n",
      "Epoch: 3/20...  Training Step: 410...  Training loss: 2.1296...  0.0943 sec/batch\n",
      "Epoch: 3/20...  Training Step: 411...  Training loss: 2.0766...  0.0922 sec/batch\n",
      "Epoch: 3/20...  Training Step: 412...  Training loss: 2.0811...  0.0922 sec/batch\n",
      "Epoch: 3/20...  Training Step: 413...  Training loss: 2.0788...  0.0937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 414...  Training loss: 2.1176...  0.0915 sec/batch\n",
      "Epoch: 3/20...  Training Step: 415...  Training loss: 2.0867...  0.0916 sec/batch\n",
      "Epoch: 3/20...  Training Step: 416...  Training loss: 2.0856...  0.0932 sec/batch\n",
      "Epoch: 3/20...  Training Step: 417...  Training loss: 2.0789...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 418...  Training loss: 2.1183...  0.0921 sec/batch\n",
      "Epoch: 3/20...  Training Step: 419...  Training loss: 2.0796...  0.0939 sec/batch\n",
      "Epoch: 3/20...  Training Step: 420...  Training loss: 2.0653...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 421...  Training loss: 2.0788...  0.0911 sec/batch\n",
      "Epoch: 3/20...  Training Step: 422...  Training loss: 2.0493...  0.0941 sec/batch\n",
      "Epoch: 3/20...  Training Step: 423...  Training loss: 2.0503...  0.0922 sec/batch\n",
      "Epoch: 3/20...  Training Step: 424...  Training loss: 2.0804...  0.0916 sec/batch\n",
      "Epoch: 3/20...  Training Step: 425...  Training loss: 2.1085...  0.0944 sec/batch\n",
      "Epoch: 3/20...  Training Step: 426...  Training loss: 2.0823...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 427...  Training loss: 2.0745...  0.0915 sec/batch\n",
      "Epoch: 3/20...  Training Step: 428...  Training loss: 2.0431...  0.0941 sec/batch\n",
      "Epoch: 3/20...  Training Step: 429...  Training loss: 2.0687...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 430...  Training loss: 2.1055...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 431...  Training loss: 2.0531...  0.0938 sec/batch\n",
      "Epoch: 3/20...  Training Step: 432...  Training loss: 2.0599...  0.0920 sec/batch\n",
      "Epoch: 3/20...  Training Step: 433...  Training loss: 2.0598...  0.0921 sec/batch\n",
      "Epoch: 3/20...  Training Step: 434...  Training loss: 2.0245...  0.0944 sec/batch\n",
      "Epoch: 3/20...  Training Step: 435...  Training loss: 2.0185...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 436...  Training loss: 2.0184...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 437...  Training loss: 2.0308...  0.0943 sec/batch\n",
      "Epoch: 3/20...  Training Step: 438...  Training loss: 2.0452...  0.0916 sec/batch\n",
      "Epoch: 3/20...  Training Step: 439...  Training loss: 2.0325...  0.0921 sec/batch\n",
      "Epoch: 3/20...  Training Step: 440...  Training loss: 2.0228...  0.0941 sec/batch\n",
      "Epoch: 3/20...  Training Step: 441...  Training loss: 2.0473...  0.0916 sec/batch\n",
      "Epoch: 3/20...  Training Step: 442...  Training loss: 1.9911...  0.0915 sec/batch\n",
      "Epoch: 3/20...  Training Step: 443...  Training loss: 2.0536...  0.0927 sec/batch\n",
      "Epoch: 3/20...  Training Step: 444...  Training loss: 2.0271...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 445...  Training loss: 2.0425...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 446...  Training loss: 2.0759...  0.0942 sec/batch\n",
      "Epoch: 3/20...  Training Step: 447...  Training loss: 2.0061...  0.0918 sec/batch\n",
      "Epoch: 3/20...  Training Step: 448...  Training loss: 2.0805...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 449...  Training loss: 2.0377...  0.0931 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 2.0349...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 451...  Training loss: 2.0258...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 452...  Training loss: 2.0479...  0.0927 sec/batch\n",
      "Epoch: 3/20...  Training Step: 453...  Training loss: 2.0296...  0.0912 sec/batch\n",
      "Epoch: 3/20...  Training Step: 454...  Training loss: 2.0268...  0.0915 sec/batch\n",
      "Epoch: 3/20...  Training Step: 455...  Training loss: 2.0127...  0.0930 sec/batch\n",
      "Epoch: 3/20...  Training Step: 456...  Training loss: 2.0693...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 457...  Training loss: 2.0347...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 458...  Training loss: 2.0651...  0.0937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 459...  Training loss: 2.0641...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 460...  Training loss: 2.0418...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 461...  Training loss: 2.0232...  0.0941 sec/batch\n",
      "Epoch: 3/20...  Training Step: 462...  Training loss: 2.0491...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 463...  Training loss: 2.0386...  0.0922 sec/batch\n",
      "Epoch: 3/20...  Training Step: 464...  Training loss: 1.9998...  0.0934 sec/batch\n",
      "Epoch: 3/20...  Training Step: 465...  Training loss: 2.0099...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 466...  Training loss: 2.0238...  0.0911 sec/batch\n",
      "Epoch: 3/20...  Training Step: 467...  Training loss: 2.0605...  0.0936 sec/batch\n",
      "Epoch: 3/20...  Training Step: 468...  Training loss: 2.0262...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 469...  Training loss: 2.0384...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 470...  Training loss: 1.9998...  0.0938 sec/batch\n",
      "Epoch: 3/20...  Training Step: 471...  Training loss: 2.0102...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 472...  Training loss: 2.0472...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 473...  Training loss: 2.0220...  0.0940 sec/batch\n",
      "Epoch: 3/20...  Training Step: 474...  Training loss: 2.0276...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 475...  Training loss: 1.9807...  0.0911 sec/batch\n",
      "Epoch: 3/20...  Training Step: 476...  Training loss: 2.0036...  0.0939 sec/batch\n",
      "Epoch: 3/20...  Training Step: 477...  Training loss: 1.9829...  0.0911 sec/batch\n",
      "Epoch: 3/20...  Training Step: 478...  Training loss: 2.0353...  0.0911 sec/batch\n",
      "Epoch: 3/20...  Training Step: 479...  Training loss: 1.9697...  0.0936 sec/batch\n",
      "Epoch: 3/20...  Training Step: 480...  Training loss: 1.9900...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 481...  Training loss: 1.9693...  0.0916 sec/batch\n",
      "Epoch: 3/20...  Training Step: 482...  Training loss: 1.9726...  0.0939 sec/batch\n",
      "Epoch: 3/20...  Training Step: 483...  Training loss: 1.9914...  0.0916 sec/batch\n",
      "Epoch: 3/20...  Training Step: 484...  Training loss: 1.9791...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 485...  Training loss: 1.9626...  0.0943 sec/batch\n",
      "Epoch: 3/20...  Training Step: 486...  Training loss: 2.0023...  0.0916 sec/batch\n",
      "Epoch: 3/20...  Training Step: 487...  Training loss: 1.9695...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 488...  Training loss: 1.9889...  0.0933 sec/batch\n",
      "Epoch: 3/20...  Training Step: 489...  Training loss: 1.9591...  0.0915 sec/batch\n",
      "Epoch: 3/20...  Training Step: 490...  Training loss: 1.9651...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 491...  Training loss: 1.9682...  0.0939 sec/batch\n",
      "Epoch: 3/20...  Training Step: 492...  Training loss: 1.9791...  0.0921 sec/batch\n",
      "Epoch: 3/20...  Training Step: 493...  Training loss: 1.9799...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 494...  Training loss: 1.9660...  0.0937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 495...  Training loss: 1.9697...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 496...  Training loss: 1.9331...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 497...  Training loss: 1.9822...  0.0937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 498...  Training loss: 1.9878...  0.0915 sec/batch\n",
      "Epoch: 3/20...  Training Step: 499...  Training loss: 1.9596...  0.0916 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9686...  0.0940 sec/batch\n",
      "Epoch: 3/20...  Training Step: 501...  Training loss: 1.9569...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 502...  Training loss: 1.9707...  0.0915 sec/batch\n",
      "Epoch: 3/20...  Training Step: 503...  Training loss: 1.9734...  0.0938 sec/batch\n",
      "Epoch: 3/20...  Training Step: 504...  Training loss: 1.9851...  0.0921 sec/batch\n",
      "Epoch: 3/20...  Training Step: 505...  Training loss: 1.9866...  0.0915 sec/batch\n",
      "Epoch: 3/20...  Training Step: 506...  Training loss: 1.9843...  0.0939 sec/batch\n",
      "Epoch: 3/20...  Training Step: 507...  Training loss: 1.9636...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 508...  Training loss: 1.9592...  0.0912 sec/batch\n",
      "Epoch: 3/20...  Training Step: 509...  Training loss: 1.9738...  0.0931 sec/batch\n",
      "Epoch: 3/20...  Training Step: 510...  Training loss: 1.9585...  0.0922 sec/batch\n",
      "Epoch: 3/20...  Training Step: 511...  Training loss: 1.9481...  0.0926 sec/batch\n",
      "Epoch: 3/20...  Training Step: 512...  Training loss: 1.9280...  0.0937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 513...  Training loss: 1.9689...  0.0918 sec/batch\n",
      "Epoch: 3/20...  Training Step: 514...  Training loss: 1.9537...  0.0925 sec/batch\n",
      "Epoch: 3/20...  Training Step: 515...  Training loss: 1.9651...  0.0939 sec/batch\n",
      "Epoch: 3/20...  Training Step: 516...  Training loss: 1.9652...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 517...  Training loss: 1.9901...  0.0923 sec/batch\n",
      "Epoch: 3/20...  Training Step: 518...  Training loss: 1.9369...  0.0940 sec/batch\n",
      "Epoch: 3/20...  Training Step: 519...  Training loss: 1.9427...  0.0916 sec/batch\n",
      "Epoch: 3/20...  Training Step: 520...  Training loss: 1.9779...  0.0921 sec/batch\n",
      "Epoch: 3/20...  Training Step: 521...  Training loss: 1.9533...  0.0938 sec/batch\n",
      "Epoch: 3/20...  Training Step: 522...  Training loss: 1.9026...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 523...  Training loss: 1.9636...  0.0923 sec/batch\n",
      "Epoch: 3/20...  Training Step: 524...  Training loss: 1.9645...  0.0940 sec/batch\n",
      "Epoch: 3/20...  Training Step: 525...  Training loss: 1.9463...  0.0918 sec/batch\n",
      "Epoch: 3/20...  Training Step: 526...  Training loss: 1.9455...  0.0920 sec/batch\n",
      "Epoch: 3/20...  Training Step: 527...  Training loss: 1.9189...  0.0942 sec/batch\n",
      "Epoch: 3/20...  Training Step: 528...  Training loss: 1.9148...  0.0913 sec/batch\n",
      "Epoch: 3/20...  Training Step: 529...  Training loss: 1.9661...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 530...  Training loss: 1.9616...  0.0943 sec/batch\n",
      "Epoch: 3/20...  Training Step: 531...  Training loss: 1.9570...  0.0923 sec/batch\n",
      "Epoch: 3/20...  Training Step: 532...  Training loss: 1.9536...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 533...  Training loss: 1.9557...  0.0943 sec/batch\n",
      "Epoch: 3/20...  Training Step: 534...  Training loss: 1.9485...  0.0924 sec/batch\n",
      "Epoch: 3/20...  Training Step: 535...  Training loss: 1.9726...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 536...  Training loss: 1.9342...  0.0943 sec/batch\n",
      "Epoch: 3/20...  Training Step: 537...  Training loss: 1.9680...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 538...  Training loss: 1.9377...  0.0923 sec/batch\n",
      "Epoch: 3/20...  Training Step: 539...  Training loss: 1.9456...  0.0940 sec/batch\n",
      "Epoch: 3/20...  Training Step: 540...  Training loss: 1.9454...  0.0920 sec/batch\n",
      "Epoch: 3/20...  Training Step: 541...  Training loss: 1.9337...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 542...  Training loss: 1.9515...  0.0944 sec/batch\n",
      "Epoch: 3/20...  Training Step: 543...  Training loss: 1.9600...  0.0923 sec/batch\n",
      "Epoch: 3/20...  Training Step: 544...  Training loss: 1.9630...  0.0918 sec/batch\n",
      "Epoch: 3/20...  Training Step: 545...  Training loss: 1.9460...  0.0946 sec/batch\n",
      "Epoch: 3/20...  Training Step: 546...  Training loss: 1.9261...  0.0924 sec/batch\n",
      "Epoch: 3/20...  Training Step: 547...  Training loss: 1.9233...  0.0915 sec/batch\n",
      "Epoch: 3/20...  Training Step: 548...  Training loss: 1.9669...  0.0937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 549...  Training loss: 1.9361...  0.0921 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 1.9397...  0.0916 sec/batch\n",
      "Epoch: 3/20...  Training Step: 551...  Training loss: 1.9371...  0.0941 sec/batch\n",
      "Epoch: 3/20...  Training Step: 552...  Training loss: 1.9310...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 553...  Training loss: 1.9394...  0.0923 sec/batch\n",
      "Epoch: 3/20...  Training Step: 554...  Training loss: 1.9177...  0.0941 sec/batch\n",
      "Epoch: 3/20...  Training Step: 555...  Training loss: 1.9051...  0.0923 sec/batch\n",
      "Epoch: 3/20...  Training Step: 556...  Training loss: 1.9635...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 557...  Training loss: 1.9500...  0.0947 sec/batch\n",
      "Epoch: 3/20...  Training Step: 558...  Training loss: 1.9282...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 559...  Training loss: 1.9364...  0.0919 sec/batch\n",
      "Epoch: 3/20...  Training Step: 560...  Training loss: 1.9336...  0.0937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 561...  Training loss: 1.9358...  0.0924 sec/batch\n",
      "Epoch: 3/20...  Training Step: 562...  Training loss: 1.9252...  0.0918 sec/batch\n",
      "Epoch: 3/20...  Training Step: 563...  Training loss: 1.9341...  0.0945 sec/batch\n",
      "Epoch: 3/20...  Training Step: 564...  Training loss: 1.9656...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 565...  Training loss: 1.9161...  0.0914 sec/batch\n",
      "Epoch: 3/20...  Training Step: 566...  Training loss: 1.9173...  0.0937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 567...  Training loss: 1.8979...  0.0922 sec/batch\n",
      "Epoch: 3/20...  Training Step: 568...  Training loss: 1.9083...  0.0925 sec/batch\n",
      "Epoch: 3/20...  Training Step: 569...  Training loss: 1.9348...  0.0947 sec/batch\n",
      "Epoch: 3/20...  Training Step: 570...  Training loss: 1.9296...  0.0921 sec/batch\n",
      "Epoch: 3/20...  Training Step: 571...  Training loss: 1.9196...  0.0923 sec/batch\n",
      "Epoch: 3/20...  Training Step: 572...  Training loss: 1.9089...  0.0935 sec/batch\n",
      "Epoch: 3/20...  Training Step: 573...  Training loss: 1.8963...  0.0927 sec/batch\n",
      "Epoch: 3/20...  Training Step: 574...  Training loss: 1.9250...  0.0928 sec/batch\n",
      "Epoch: 3/20...  Training Step: 575...  Training loss: 1.8870...  0.0951 sec/batch\n",
      "Epoch: 3/20...  Training Step: 576...  Training loss: 1.8668...  0.0918 sec/batch\n",
      "Epoch: 3/20...  Training Step: 577...  Training loss: 1.8836...  0.0920 sec/batch\n",
      "Epoch: 3/20...  Training Step: 578...  Training loss: 1.9074...  0.0946 sec/batch\n",
      "Epoch: 3/20...  Training Step: 579...  Training loss: 1.9028...  0.0917 sec/batch\n",
      "Epoch: 3/20...  Training Step: 580...  Training loss: 1.9247...  0.0926 sec/batch\n",
      "Epoch: 3/20...  Training Step: 581...  Training loss: 1.9052...  0.0945 sec/batch\n",
      "Epoch: 3/20...  Training Step: 582...  Training loss: 1.8896...  0.0939 sec/batch\n",
      "Epoch: 3/20...  Training Step: 583...  Training loss: 1.9057...  0.0928 sec/batch\n",
      "Epoch: 3/20...  Training Step: 584...  Training loss: 1.8737...  0.0948 sec/batch\n",
      "Epoch: 3/20...  Training Step: 585...  Training loss: 1.8878...  0.0927 sec/batch\n",
      "Epoch: 3/20...  Training Step: 586...  Training loss: 1.8972...  0.0922 sec/batch\n",
      "Epoch: 3/20...  Training Step: 587...  Training loss: 1.9073...  0.0937 sec/batch\n",
      "Epoch: 3/20...  Training Step: 588...  Training loss: 1.8718...  0.0922 sec/batch\n",
      "Epoch: 3/20...  Training Step: 589...  Training loss: 1.9078...  0.0918 sec/batch\n",
      "Epoch: 3/20...  Training Step: 590...  Training loss: 1.8822...  0.0942 sec/batch\n",
      "Epoch: 3/20...  Training Step: 591...  Training loss: 1.8658...  0.0928 sec/batch\n",
      "Epoch: 3/20...  Training Step: 592...  Training loss: 1.9079...  0.0918 sec/batch\n",
      "Epoch: 3/20...  Training Step: 593...  Training loss: 1.8892...  0.0946 sec/batch\n",
      "Epoch: 3/20...  Training Step: 594...  Training loss: 1.8785...  0.0917 sec/batch\n",
      "Epoch: 4/20...  Training Step: 595...  Training loss: 1.9716...  0.0927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 596...  Training loss: 1.8730...  0.0946 sec/batch\n",
      "Epoch: 4/20...  Training Step: 597...  Training loss: 1.8735...  0.0920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 598...  Training loss: 1.8774...  0.0925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 599...  Training loss: 1.8841...  0.0941 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.8430...  0.0939 sec/batch\n",
      "Epoch: 4/20...  Training Step: 601...  Training loss: 1.8856...  0.0917 sec/batch\n",
      "Epoch: 4/20...  Training Step: 602...  Training loss: 1.8695...  0.0928 sec/batch\n",
      "Epoch: 4/20...  Training Step: 603...  Training loss: 1.9159...  0.0920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 604...  Training loss: 1.8757...  0.0946 sec/batch\n",
      "Epoch: 4/20...  Training Step: 605...  Training loss: 1.8541...  0.0922 sec/batch\n",
      "Epoch: 4/20...  Training Step: 606...  Training loss: 1.8519...  0.0924 sec/batch\n",
      "Epoch: 4/20...  Training Step: 607...  Training loss: 1.8857...  0.0937 sec/batch\n",
      "Epoch: 4/20...  Training Step: 608...  Training loss: 1.9107...  0.0925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 609...  Training loss: 1.8708...  0.0922 sec/batch\n",
      "Epoch: 4/20...  Training Step: 610...  Training loss: 1.8547...  0.0946 sec/batch\n",
      "Epoch: 4/20...  Training Step: 611...  Training loss: 1.8783...  0.0932 sec/batch\n",
      "Epoch: 4/20...  Training Step: 612...  Training loss: 1.9244...  0.0918 sec/batch\n",
      "Epoch: 4/20...  Training Step: 613...  Training loss: 1.8806...  0.0943 sec/batch\n",
      "Epoch: 4/20...  Training Step: 614...  Training loss: 1.8928...  0.0923 sec/batch\n",
      "Epoch: 4/20...  Training Step: 615...  Training loss: 1.8600...  0.0918 sec/batch\n",
      "Epoch: 4/20...  Training Step: 616...  Training loss: 1.9091...  0.0939 sec/batch\n",
      "Epoch: 4/20...  Training Step: 617...  Training loss: 1.8805...  0.0931 sec/batch\n",
      "Epoch: 4/20...  Training Step: 618...  Training loss: 1.8844...  0.0917 sec/batch\n",
      "Epoch: 4/20...  Training Step: 619...  Training loss: 1.8737...  0.0943 sec/batch\n",
      "Epoch: 4/20...  Training Step: 620...  Training loss: 1.8341...  0.0916 sec/batch\n",
      "Epoch: 4/20...  Training Step: 621...  Training loss: 1.8490...  0.0918 sec/batch\n",
      "Epoch: 4/20...  Training Step: 622...  Training loss: 1.8783...  0.0938 sec/batch\n",
      "Epoch: 4/20...  Training Step: 623...  Training loss: 1.9032...  0.0921 sec/batch\n",
      "Epoch: 4/20...  Training Step: 624...  Training loss: 1.8850...  0.0921 sec/batch\n",
      "Epoch: 4/20...  Training Step: 625...  Training loss: 1.8696...  0.0948 sec/batch\n",
      "Epoch: 4/20...  Training Step: 626...  Training loss: 1.8375...  0.0929 sec/batch\n",
      "Epoch: 4/20...  Training Step: 627...  Training loss: 1.8845...  0.0929 sec/batch\n",
      "Epoch: 4/20...  Training Step: 628...  Training loss: 1.8966...  0.0937 sec/batch\n",
      "Epoch: 4/20...  Training Step: 629...  Training loss: 1.8496...  0.0921 sec/batch\n",
      "Epoch: 4/20...  Training Step: 630...  Training loss: 1.8664...  0.0912 sec/batch\n",
      "Epoch: 4/20...  Training Step: 631...  Training loss: 1.8636...  0.0935 sec/batch\n",
      "Epoch: 4/20...  Training Step: 632...  Training loss: 1.8296...  0.0920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 633...  Training loss: 1.8147...  0.0925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 634...  Training loss: 1.8298...  0.0944 sec/batch\n",
      "Epoch: 4/20...  Training Step: 635...  Training loss: 1.8337...  0.0923 sec/batch\n",
      "Epoch: 4/20...  Training Step: 636...  Training loss: 1.8747...  0.0916 sec/batch\n",
      "Epoch: 4/20...  Training Step: 637...  Training loss: 1.8404...  0.0945 sec/batch\n",
      "Epoch: 4/20...  Training Step: 638...  Training loss: 1.8200...  0.0919 sec/batch\n",
      "Epoch: 4/20...  Training Step: 639...  Training loss: 1.8665...  0.0923 sec/batch\n",
      "Epoch: 4/20...  Training Step: 640...  Training loss: 1.8059...  0.0941 sec/batch\n",
      "Epoch: 4/20...  Training Step: 641...  Training loss: 1.8465...  0.0923 sec/batch\n",
      "Epoch: 4/20...  Training Step: 642...  Training loss: 1.8256...  0.0925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 643...  Training loss: 1.8370...  0.0943 sec/batch\n",
      "Epoch: 4/20...  Training Step: 644...  Training loss: 1.8903...  0.0919 sec/batch\n",
      "Epoch: 4/20...  Training Step: 645...  Training loss: 1.8110...  0.0916 sec/batch\n",
      "Epoch: 4/20...  Training Step: 646...  Training loss: 1.9122...  0.0941 sec/batch\n",
      "Epoch: 4/20...  Training Step: 647...  Training loss: 1.8480...  0.0922 sec/batch\n",
      "Epoch: 4/20...  Training Step: 648...  Training loss: 1.8395...  0.0925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 649...  Training loss: 1.8342...  0.0936 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.8497...  0.0919 sec/batch\n",
      "Epoch: 4/20...  Training Step: 651...  Training loss: 1.8512...  0.0923 sec/batch\n",
      "Epoch: 4/20...  Training Step: 652...  Training loss: 1.8358...  0.0937 sec/batch\n",
      "Epoch: 4/20...  Training Step: 653...  Training loss: 1.8227...  0.0919 sec/batch\n",
      "Epoch: 4/20...  Training Step: 654...  Training loss: 1.8763...  0.0934 sec/batch\n",
      "Epoch: 4/20...  Training Step: 655...  Training loss: 1.8390...  0.0963 sec/batch\n",
      "Epoch: 4/20...  Training Step: 656...  Training loss: 1.8725...  0.0923 sec/batch\n",
      "Epoch: 4/20...  Training Step: 657...  Training loss: 1.8773...  0.0917 sec/batch\n",
      "Epoch: 4/20...  Training Step: 658...  Training loss: 1.8607...  0.0942 sec/batch\n",
      "Epoch: 4/20...  Training Step: 659...  Training loss: 1.8312...  0.0927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 660...  Training loss: 1.8615...  0.0918 sec/batch\n",
      "Epoch: 4/20...  Training Step: 661...  Training loss: 1.8613...  0.0944 sec/batch\n",
      "Epoch: 4/20...  Training Step: 662...  Training loss: 1.8194...  0.0922 sec/batch\n",
      "Epoch: 4/20...  Training Step: 663...  Training loss: 1.8164...  0.0919 sec/batch\n",
      "Epoch: 4/20...  Training Step: 664...  Training loss: 1.8328...  0.0948 sec/batch\n",
      "Epoch: 4/20...  Training Step: 665...  Training loss: 1.8761...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 666...  Training loss: 1.8384...  0.0930 sec/batch\n",
      "Epoch: 4/20...  Training Step: 667...  Training loss: 1.8631...  0.0946 sec/batch\n",
      "Epoch: 4/20...  Training Step: 668...  Training loss: 1.8231...  0.0935 sec/batch\n",
      "Epoch: 4/20...  Training Step: 669...  Training loss: 1.8258...  0.0921 sec/batch\n",
      "Epoch: 4/20...  Training Step: 670...  Training loss: 1.8520...  0.0963 sec/batch\n",
      "Epoch: 4/20...  Training Step: 671...  Training loss: 1.8378...  0.0924 sec/batch\n",
      "Epoch: 4/20...  Training Step: 672...  Training loss: 1.8306...  0.0933 sec/batch\n",
      "Epoch: 4/20...  Training Step: 673...  Training loss: 1.7978...  0.0957 sec/batch\n",
      "Epoch: 4/20...  Training Step: 674...  Training loss: 1.8134...  0.0925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 675...  Training loss: 1.7871...  0.0932 sec/batch\n",
      "Epoch: 4/20...  Training Step: 676...  Training loss: 1.8412...  0.0955 sec/batch\n",
      "Epoch: 4/20...  Training Step: 677...  Training loss: 1.7843...  0.0924 sec/batch\n",
      "Epoch: 4/20...  Training Step: 678...  Training loss: 1.8262...  0.0931 sec/batch\n",
      "Epoch: 4/20...  Training Step: 679...  Training loss: 1.7909...  0.0941 sec/batch\n",
      "Epoch: 4/20...  Training Step: 680...  Training loss: 1.8025...  0.0929 sec/batch\n",
      "Epoch: 4/20...  Training Step: 681...  Training loss: 1.8105...  0.0933 sec/batch\n",
      "Epoch: 4/20...  Training Step: 682...  Training loss: 1.7905...  0.0948 sec/batch\n",
      "Epoch: 4/20...  Training Step: 683...  Training loss: 1.7814...  0.0923 sec/batch\n",
      "Epoch: 4/20...  Training Step: 684...  Training loss: 1.8336...  0.0936 sec/batch\n",
      "Epoch: 4/20...  Training Step: 685...  Training loss: 1.7933...  0.0962 sec/batch\n",
      "Epoch: 4/20...  Training Step: 686...  Training loss: 1.8002...  0.0931 sec/batch\n",
      "Epoch: 4/20...  Training Step: 687...  Training loss: 1.7820...  0.0920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 688...  Training loss: 1.7825...  0.0951 sec/batch\n",
      "Epoch: 4/20...  Training Step: 689...  Training loss: 1.7924...  0.0934 sec/batch\n",
      "Epoch: 4/20...  Training Step: 690...  Training loss: 1.8026...  0.0933 sec/batch\n",
      "Epoch: 4/20...  Training Step: 691...  Training loss: 1.7953...  0.0945 sec/batch\n",
      "Epoch: 4/20...  Training Step: 692...  Training loss: 1.7687...  0.0932 sec/batch\n",
      "Epoch: 4/20...  Training Step: 693...  Training loss: 1.7935...  0.0924 sec/batch\n",
      "Epoch: 4/20...  Training Step: 694...  Training loss: 1.7607...  0.0948 sec/batch\n",
      "Epoch: 4/20...  Training Step: 695...  Training loss: 1.8054...  0.0925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 696...  Training loss: 1.8026...  0.0932 sec/batch\n",
      "Epoch: 4/20...  Training Step: 697...  Training loss: 1.7966...  0.0955 sec/batch\n",
      "Epoch: 4/20...  Training Step: 698...  Training loss: 1.7951...  0.0924 sec/batch\n",
      "Epoch: 4/20...  Training Step: 699...  Training loss: 1.7856...  0.0929 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.7978...  0.0953 sec/batch\n",
      "Epoch: 4/20...  Training Step: 701...  Training loss: 1.7911...  0.0936 sec/batch\n",
      "Epoch: 4/20...  Training Step: 702...  Training loss: 1.8098...  0.0928 sec/batch\n",
      "Epoch: 4/20...  Training Step: 703...  Training loss: 1.8076...  0.0955 sec/batch\n",
      "Epoch: 4/20...  Training Step: 704...  Training loss: 1.8131...  0.0935 sec/batch\n",
      "Epoch: 4/20...  Training Step: 705...  Training loss: 1.7912...  0.0925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 706...  Training loss: 1.7932...  0.0945 sec/batch\n",
      "Epoch: 4/20...  Training Step: 707...  Training loss: 1.7839...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 708...  Training loss: 1.7899...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 709...  Training loss: 1.7788...  0.0970 sec/batch\n",
      "Epoch: 4/20...  Training Step: 710...  Training loss: 1.7602...  0.0923 sec/batch\n",
      "Epoch: 4/20...  Training Step: 711...  Training loss: 1.8009...  0.0930 sec/batch\n",
      "Epoch: 4/20...  Training Step: 712...  Training loss: 1.7888...  0.0946 sec/batch\n",
      "Epoch: 4/20...  Training Step: 713...  Training loss: 1.7907...  0.0934 sec/batch\n",
      "Epoch: 4/20...  Training Step: 714...  Training loss: 1.7935...  0.0930 sec/batch\n",
      "Epoch: 4/20...  Training Step: 715...  Training loss: 1.8015...  0.0967 sec/batch\n",
      "Epoch: 4/20...  Training Step: 716...  Training loss: 1.7550...  0.0930 sec/batch\n",
      "Epoch: 4/20...  Training Step: 717...  Training loss: 1.7646...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 718...  Training loss: 1.8169...  0.0947 sec/batch\n",
      "Epoch: 4/20...  Training Step: 719...  Training loss: 1.7731...  0.0932 sec/batch\n",
      "Epoch: 4/20...  Training Step: 720...  Training loss: 1.7360...  0.0927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 721...  Training loss: 1.8024...  0.0953 sec/batch\n",
      "Epoch: 4/20...  Training Step: 722...  Training loss: 1.7910...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 723...  Training loss: 1.7816...  0.0932 sec/batch\n",
      "Epoch: 4/20...  Training Step: 724...  Training loss: 1.7794...  0.0948 sec/batch\n",
      "Epoch: 4/20...  Training Step: 725...  Training loss: 1.7509...  0.0927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 726...  Training loss: 1.7660...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 727...  Training loss: 1.8104...  0.0945 sec/batch\n",
      "Epoch: 4/20...  Training Step: 728...  Training loss: 1.7909...  0.0932 sec/batch\n",
      "Epoch: 4/20...  Training Step: 729...  Training loss: 1.7891...  0.0928 sec/batch\n",
      "Epoch: 4/20...  Training Step: 730...  Training loss: 1.7959...  0.0945 sec/batch\n",
      "Epoch: 4/20...  Training Step: 731...  Training loss: 1.7978...  0.0925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 732...  Training loss: 1.7914...  0.0934 sec/batch\n",
      "Epoch: 4/20...  Training Step: 733...  Training loss: 1.8060...  0.0950 sec/batch\n",
      "Epoch: 4/20...  Training Step: 734...  Training loss: 1.7757...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 735...  Training loss: 1.8263...  0.0928 sec/batch\n",
      "Epoch: 4/20...  Training Step: 736...  Training loss: 1.7705...  0.0951 sec/batch\n",
      "Epoch: 4/20...  Training Step: 737...  Training loss: 1.7859...  0.0927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 738...  Training loss: 1.7948...  0.0924 sec/batch\n",
      "Epoch: 4/20...  Training Step: 739...  Training loss: 1.7617...  0.0949 sec/batch\n",
      "Epoch: 4/20...  Training Step: 740...  Training loss: 1.7956...  0.0928 sec/batch\n",
      "Epoch: 4/20...  Training Step: 741...  Training loss: 1.7912...  0.0937 sec/batch\n",
      "Epoch: 4/20...  Training Step: 742...  Training loss: 1.8031...  0.0949 sec/batch\n",
      "Epoch: 4/20...  Training Step: 743...  Training loss: 1.7804...  0.0924 sec/batch\n",
      "Epoch: 4/20...  Training Step: 744...  Training loss: 1.7757...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 745...  Training loss: 1.7543...  0.0956 sec/batch\n",
      "Epoch: 4/20...  Training Step: 746...  Training loss: 1.7896...  0.0927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 747...  Training loss: 1.7859...  0.0929 sec/batch\n",
      "Epoch: 4/20...  Training Step: 748...  Training loss: 1.7754...  0.0944 sec/batch\n",
      "Epoch: 4/20...  Training Step: 749...  Training loss: 1.7674...  0.0930 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.7588...  0.0931 sec/batch\n",
      "Epoch: 4/20...  Training Step: 751...  Training loss: 1.7823...  0.0954 sec/batch\n",
      "Epoch: 4/20...  Training Step: 752...  Training loss: 1.7663...  0.0927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 753...  Training loss: 1.7278...  0.0928 sec/batch\n",
      "Epoch: 4/20...  Training Step: 754...  Training loss: 1.7941...  0.0954 sec/batch\n",
      "Epoch: 4/20...  Training Step: 755...  Training loss: 1.8050...  0.0932 sec/batch\n",
      "Epoch: 4/20...  Training Step: 756...  Training loss: 1.7642...  0.0928 sec/batch\n",
      "Epoch: 4/20...  Training Step: 757...  Training loss: 1.7827...  0.0959 sec/batch\n",
      "Epoch: 4/20...  Training Step: 758...  Training loss: 1.7753...  0.0928 sec/batch\n",
      "Epoch: 4/20...  Training Step: 759...  Training loss: 1.7713...  0.0931 sec/batch\n",
      "Epoch: 4/20...  Training Step: 760...  Training loss: 1.7593...  0.0945 sec/batch\n",
      "Epoch: 4/20...  Training Step: 761...  Training loss: 1.7848...  0.0925 sec/batch\n",
      "Epoch: 4/20...  Training Step: 762...  Training loss: 1.8298...  0.0930 sec/batch\n",
      "Epoch: 4/20...  Training Step: 763...  Training loss: 1.7646...  0.0946 sec/batch\n",
      "Epoch: 4/20...  Training Step: 764...  Training loss: 1.7646...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 765...  Training loss: 1.7455...  0.0934 sec/batch\n",
      "Epoch: 4/20...  Training Step: 766...  Training loss: 1.7550...  0.0948 sec/batch\n",
      "Epoch: 4/20...  Training Step: 767...  Training loss: 1.7863...  0.0927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 768...  Training loss: 1.7766...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 769...  Training loss: 1.7775...  0.0950 sec/batch\n",
      "Epoch: 4/20...  Training Step: 770...  Training loss: 1.7640...  0.0929 sec/batch\n",
      "Epoch: 4/20...  Training Step: 771...  Training loss: 1.7456...  0.0931 sec/batch\n",
      "Epoch: 4/20...  Training Step: 772...  Training loss: 1.7739...  0.0943 sec/batch\n",
      "Epoch: 4/20...  Training Step: 773...  Training loss: 1.7456...  0.0929 sec/batch\n",
      "Epoch: 4/20...  Training Step: 774...  Training loss: 1.7280...  0.0921 sec/batch\n",
      "Epoch: 4/20...  Training Step: 775...  Training loss: 1.7435...  0.0955 sec/batch\n",
      "Epoch: 4/20...  Training Step: 776...  Training loss: 1.7513...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 777...  Training loss: 1.7530...  0.0934 sec/batch\n",
      "Epoch: 4/20...  Training Step: 778...  Training loss: 1.7706...  0.0939 sec/batch\n",
      "Epoch: 4/20...  Training Step: 779...  Training loss: 1.7550...  0.0933 sec/batch\n",
      "Epoch: 4/20...  Training Step: 780...  Training loss: 1.7353...  0.0926 sec/batch\n",
      "Epoch: 4/20...  Training Step: 781...  Training loss: 1.7529...  0.0958 sec/batch\n",
      "Epoch: 4/20...  Training Step: 782...  Training loss: 1.7388...  0.0929 sec/batch\n",
      "Epoch: 4/20...  Training Step: 783...  Training loss: 1.7457...  0.0935 sec/batch\n",
      "Epoch: 4/20...  Training Step: 784...  Training loss: 1.7535...  0.0947 sec/batch\n",
      "Epoch: 4/20...  Training Step: 785...  Training loss: 1.7555...  0.0927 sec/batch\n",
      "Epoch: 4/20...  Training Step: 786...  Training loss: 1.7284...  0.0921 sec/batch\n",
      "Epoch: 4/20...  Training Step: 787...  Training loss: 1.7556...  0.0948 sec/batch\n",
      "Epoch: 4/20...  Training Step: 788...  Training loss: 1.7301...  0.0924 sec/batch\n",
      "Epoch: 4/20...  Training Step: 789...  Training loss: 1.7150...  0.0933 sec/batch\n",
      "Epoch: 4/20...  Training Step: 790...  Training loss: 1.7576...  0.0945 sec/batch\n",
      "Epoch: 4/20...  Training Step: 791...  Training loss: 1.7368...  0.0929 sec/batch\n",
      "Epoch: 4/20...  Training Step: 792...  Training loss: 1.7263...  0.0922 sec/batch\n",
      "Epoch: 5/20...  Training Step: 793...  Training loss: 1.8342...  0.0952 sec/batch\n",
      "Epoch: 5/20...  Training Step: 794...  Training loss: 1.7381...  0.0931 sec/batch\n",
      "Epoch: 5/20...  Training Step: 795...  Training loss: 1.7261...  0.0925 sec/batch\n",
      "Epoch: 5/20...  Training Step: 796...  Training loss: 1.7436...  0.0959 sec/batch\n",
      "Epoch: 5/20...  Training Step: 797...  Training loss: 1.7296...  0.0936 sec/batch\n",
      "Epoch: 5/20...  Training Step: 798...  Training loss: 1.6965...  0.0932 sec/batch\n",
      "Epoch: 5/20...  Training Step: 799...  Training loss: 1.7414...  0.0953 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.7235...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 801...  Training loss: 1.7670...  0.0932 sec/batch\n",
      "Epoch: 5/20...  Training Step: 802...  Training loss: 1.7268...  0.0923 sec/batch\n",
      "Epoch: 5/20...  Training Step: 803...  Training loss: 1.7059...  0.0926 sec/batch\n",
      "Epoch: 5/20...  Training Step: 804...  Training loss: 1.7094...  0.0953 sec/batch\n",
      "Epoch: 5/20...  Training Step: 805...  Training loss: 1.7273...  0.0934 sec/batch\n",
      "Epoch: 5/20...  Training Step: 806...  Training loss: 1.7717...  0.0925 sec/batch\n",
      "Epoch: 5/20...  Training Step: 807...  Training loss: 1.7306...  0.0948 sec/batch\n",
      "Epoch: 5/20...  Training Step: 808...  Training loss: 1.7207...  0.0930 sec/batch\n",
      "Epoch: 5/20...  Training Step: 809...  Training loss: 1.7356...  0.0934 sec/batch\n",
      "Epoch: 5/20...  Training Step: 810...  Training loss: 1.7671...  0.0957 sec/batch\n",
      "Epoch: 5/20...  Training Step: 811...  Training loss: 1.7324...  0.0929 sec/batch\n",
      "Epoch: 5/20...  Training Step: 812...  Training loss: 1.7383...  0.0925 sec/batch\n",
      "Epoch: 5/20...  Training Step: 813...  Training loss: 1.7228...  0.0961 sec/batch\n",
      "Epoch: 5/20...  Training Step: 814...  Training loss: 1.7635...  0.0933 sec/batch\n",
      "Epoch: 5/20...  Training Step: 815...  Training loss: 1.7292...  0.0930 sec/batch\n",
      "Epoch: 5/20...  Training Step: 816...  Training loss: 1.7295...  0.0952 sec/batch\n",
      "Epoch: 5/20...  Training Step: 817...  Training loss: 1.7307...  0.0923 sec/batch\n",
      "Epoch: 5/20...  Training Step: 818...  Training loss: 1.6922...  0.0928 sec/batch\n",
      "Epoch: 5/20...  Training Step: 819...  Training loss: 1.6855...  0.0954 sec/batch\n",
      "Epoch: 5/20...  Training Step: 820...  Training loss: 1.7360...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 821...  Training loss: 1.7545...  0.0928 sec/batch\n",
      "Epoch: 5/20...  Training Step: 822...  Training loss: 1.7422...  0.0947 sec/batch\n",
      "Epoch: 5/20...  Training Step: 823...  Training loss: 1.7399...  0.0929 sec/batch\n",
      "Epoch: 5/20...  Training Step: 824...  Training loss: 1.7000...  0.0924 sec/batch\n",
      "Epoch: 5/20...  Training Step: 825...  Training loss: 1.7496...  0.0950 sec/batch\n",
      "Epoch: 5/20...  Training Step: 826...  Training loss: 1.7442...  0.0926 sec/batch\n",
      "Epoch: 5/20...  Training Step: 827...  Training loss: 1.7212...  0.0924 sec/batch\n",
      "Epoch: 5/20...  Training Step: 828...  Training loss: 1.7227...  0.0947 sec/batch\n",
      "Epoch: 5/20...  Training Step: 829...  Training loss: 1.7085...  0.0932 sec/batch\n",
      "Epoch: 5/20...  Training Step: 830...  Training loss: 1.6865...  0.0926 sec/batch\n",
      "Epoch: 5/20...  Training Step: 831...  Training loss: 1.6777...  0.0950 sec/batch\n",
      "Epoch: 5/20...  Training Step: 832...  Training loss: 1.6996...  0.0930 sec/batch\n",
      "Epoch: 5/20...  Training Step: 833...  Training loss: 1.7020...  0.0931 sec/batch\n",
      "Epoch: 5/20...  Training Step: 834...  Training loss: 1.7455...  0.0955 sec/batch\n",
      "Epoch: 5/20...  Training Step: 835...  Training loss: 1.7047...  0.0941 sec/batch\n",
      "Epoch: 5/20...  Training Step: 836...  Training loss: 1.6877...  0.0929 sec/batch\n",
      "Epoch: 5/20...  Training Step: 837...  Training loss: 1.7244...  0.0949 sec/batch\n",
      "Epoch: 5/20...  Training Step: 838...  Training loss: 1.6843...  0.0923 sec/batch\n",
      "Epoch: 5/20...  Training Step: 839...  Training loss: 1.7086...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 840...  Training loss: 1.7091...  0.0948 sec/batch\n",
      "Epoch: 5/20...  Training Step: 841...  Training loss: 1.6970...  0.0932 sec/batch\n",
      "Epoch: 5/20...  Training Step: 842...  Training loss: 1.7548...  0.0924 sec/batch\n",
      "Epoch: 5/20...  Training Step: 843...  Training loss: 1.6886...  0.0939 sec/batch\n",
      "Epoch: 5/20...  Training Step: 844...  Training loss: 1.7666...  0.0926 sec/batch\n",
      "Epoch: 5/20...  Training Step: 845...  Training loss: 1.7097...  0.0932 sec/batch\n",
      "Epoch: 5/20...  Training Step: 846...  Training loss: 1.7131...  0.0951 sec/batch\n",
      "Epoch: 5/20...  Training Step: 847...  Training loss: 1.7052...  0.0936 sec/batch\n",
      "Epoch: 5/20...  Training Step: 848...  Training loss: 1.7188...  0.0926 sec/batch\n",
      "Epoch: 5/20...  Training Step: 849...  Training loss: 1.7413...  0.0951 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.6946...  0.0929 sec/batch\n",
      "Epoch: 5/20...  Training Step: 851...  Training loss: 1.6824...  0.0933 sec/batch\n",
      "Epoch: 5/20...  Training Step: 852...  Training loss: 1.7421...  0.0959 sec/batch\n",
      "Epoch: 5/20...  Training Step: 853...  Training loss: 1.7201...  0.0930 sec/batch\n",
      "Epoch: 5/20...  Training Step: 854...  Training loss: 1.7727...  0.0930 sec/batch\n",
      "Epoch: 5/20...  Training Step: 855...  Training loss: 1.7570...  0.0945 sec/batch\n",
      "Epoch: 5/20...  Training Step: 856...  Training loss: 1.7422...  0.0926 sec/batch\n",
      "Epoch: 5/20...  Training Step: 857...  Training loss: 1.7078...  0.0926 sec/batch\n",
      "Epoch: 5/20...  Training Step: 858...  Training loss: 1.7336...  0.0957 sec/batch\n",
      "Epoch: 5/20...  Training Step: 859...  Training loss: 1.7263...  0.0937 sec/batch\n",
      "Epoch: 5/20...  Training Step: 860...  Training loss: 1.6883...  0.0923 sec/batch\n",
      "Epoch: 5/20...  Training Step: 861...  Training loss: 1.7043...  0.0959 sec/batch\n",
      "Epoch: 5/20...  Training Step: 862...  Training loss: 1.6955...  0.0936 sec/batch\n",
      "Epoch: 5/20...  Training Step: 863...  Training loss: 1.7609...  0.0939 sec/batch\n",
      "Epoch: 5/20...  Training Step: 864...  Training loss: 1.7231...  0.0942 sec/batch\n",
      "Epoch: 5/20...  Training Step: 865...  Training loss: 1.7398...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 866...  Training loss: 1.6945...  0.0928 sec/batch\n",
      "Epoch: 5/20...  Training Step: 867...  Training loss: 1.6967...  0.0948 sec/batch\n",
      "Epoch: 5/20...  Training Step: 868...  Training loss: 1.7367...  0.0924 sec/batch\n",
      "Epoch: 5/20...  Training Step: 869...  Training loss: 1.7023...  0.0933 sec/batch\n",
      "Epoch: 5/20...  Training Step: 870...  Training loss: 1.6969...  0.0953 sec/batch\n",
      "Epoch: 5/20...  Training Step: 871...  Training loss: 1.6663...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 872...  Training loss: 1.6993...  0.0929 sec/batch\n",
      "Epoch: 5/20...  Training Step: 873...  Training loss: 1.6573...  0.0944 sec/batch\n",
      "Epoch: 5/20...  Training Step: 874...  Training loss: 1.7096...  0.0923 sec/batch\n",
      "Epoch: 5/20...  Training Step: 875...  Training loss: 1.6526...  0.0935 sec/batch\n",
      "Epoch: 5/20...  Training Step: 876...  Training loss: 1.6915...  0.0936 sec/batch\n",
      "Epoch: 5/20...  Training Step: 877...  Training loss: 1.6585...  0.0925 sec/batch\n",
      "Epoch: 5/20...  Training Step: 878...  Training loss: 1.6771...  0.0925 sec/batch\n",
      "Epoch: 5/20...  Training Step: 879...  Training loss: 1.6662...  0.0951 sec/batch\n",
      "Epoch: 5/20...  Training Step: 880...  Training loss: 1.6694...  0.0922 sec/batch\n",
      "Epoch: 5/20...  Training Step: 881...  Training loss: 1.6518...  0.0929 sec/batch\n",
      "Epoch: 5/20...  Training Step: 882...  Training loss: 1.7045...  0.0950 sec/batch\n",
      "Epoch: 5/20...  Training Step: 883...  Training loss: 1.6610...  0.0926 sec/batch\n",
      "Epoch: 5/20...  Training Step: 884...  Training loss: 1.6714...  0.0922 sec/batch\n",
      "Epoch: 5/20...  Training Step: 885...  Training loss: 1.6660...  0.0947 sec/batch\n",
      "Epoch: 5/20...  Training Step: 886...  Training loss: 1.6632...  0.0929 sec/batch\n",
      "Epoch: 5/20...  Training Step: 887...  Training loss: 1.6705...  0.0929 sec/batch\n",
      "Epoch: 5/20...  Training Step: 888...  Training loss: 1.6912...  0.0949 sec/batch\n",
      "Epoch: 5/20...  Training Step: 889...  Training loss: 1.6794...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 890...  Training loss: 1.6639...  0.0924 sec/batch\n",
      "Epoch: 5/20...  Training Step: 891...  Training loss: 1.6686...  0.0953 sec/batch\n",
      "Epoch: 5/20...  Training Step: 892...  Training loss: 1.6363...  0.0922 sec/batch\n",
      "Epoch: 5/20...  Training Step: 893...  Training loss: 1.6831...  0.0939 sec/batch\n",
      "Epoch: 5/20...  Training Step: 894...  Training loss: 1.6815...  0.0952 sec/batch\n",
      "Epoch: 5/20...  Training Step: 895...  Training loss: 1.6745...  0.0928 sec/batch\n",
      "Epoch: 5/20...  Training Step: 896...  Training loss: 1.6748...  0.0924 sec/batch\n",
      "Epoch: 5/20...  Training Step: 897...  Training loss: 1.6558...  0.0946 sec/batch\n",
      "Epoch: 5/20...  Training Step: 898...  Training loss: 1.6645...  0.0942 sec/batch\n",
      "Epoch: 5/20...  Training Step: 899...  Training loss: 1.6812...  0.0925 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.6772...  0.0950 sec/batch\n",
      "Epoch: 5/20...  Training Step: 901...  Training loss: 1.6875...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 902...  Training loss: 1.6953...  0.0925 sec/batch\n",
      "Epoch: 5/20...  Training Step: 903...  Training loss: 1.6749...  0.0953 sec/batch\n",
      "Epoch: 5/20...  Training Step: 904...  Training loss: 1.6713...  0.0961 sec/batch\n",
      "Epoch: 5/20...  Training Step: 905...  Training loss: 1.6654...  0.1001 sec/batch\n",
      "Epoch: 5/20...  Training Step: 906...  Training loss: 1.6585...  0.1158 sec/batch\n",
      "Epoch: 5/20...  Training Step: 907...  Training loss: 1.6472...  0.0964 sec/batch\n",
      "Epoch: 5/20...  Training Step: 908...  Training loss: 1.6248...  0.1045 sec/batch\n",
      "Epoch: 5/20...  Training Step: 909...  Training loss: 1.6715...  0.1122 sec/batch\n",
      "Epoch: 5/20...  Training Step: 910...  Training loss: 1.6660...  0.1116 sec/batch\n",
      "Epoch: 5/20...  Training Step: 911...  Training loss: 1.6604...  0.0950 sec/batch\n",
      "Epoch: 5/20...  Training Step: 912...  Training loss: 1.6617...  0.0928 sec/batch\n",
      "Epoch: 5/20...  Training Step: 913...  Training loss: 1.6687...  0.0948 sec/batch\n",
      "Epoch: 5/20...  Training Step: 914...  Training loss: 1.6319...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 915...  Training loss: 1.6261...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 916...  Training loss: 1.6859...  0.0924 sec/batch\n",
      "Epoch: 5/20...  Training Step: 917...  Training loss: 1.6575...  0.0923 sec/batch\n",
      "Epoch: 5/20...  Training Step: 918...  Training loss: 1.6095...  0.0928 sec/batch\n",
      "Epoch: 5/20...  Training Step: 919...  Training loss: 1.6695...  0.0931 sec/batch\n",
      "Epoch: 5/20...  Training Step: 920...  Training loss: 1.6744...  0.1033 sec/batch\n",
      "Epoch: 5/20...  Training Step: 921...  Training loss: 1.6612...  0.0962 sec/batch\n",
      "Epoch: 5/20...  Training Step: 922...  Training loss: 1.6398...  0.1054 sec/batch\n",
      "Epoch: 5/20...  Training Step: 923...  Training loss: 1.6279...  0.1018 sec/batch\n",
      "Epoch: 5/20...  Training Step: 924...  Training loss: 1.6320...  0.0937 sec/batch\n",
      "Epoch: 5/20...  Training Step: 925...  Training loss: 1.6793...  0.0944 sec/batch\n",
      "Epoch: 5/20...  Training Step: 926...  Training loss: 1.6574...  0.0964 sec/batch\n",
      "Epoch: 5/20...  Training Step: 927...  Training loss: 1.6700...  0.1097 sec/batch\n",
      "Epoch: 5/20...  Training Step: 928...  Training loss: 1.6920...  0.0938 sec/batch\n",
      "Epoch: 5/20...  Training Step: 929...  Training loss: 1.7098...  0.0937 sec/batch\n",
      "Epoch: 5/20...  Training Step: 930...  Training loss: 1.6993...  0.0940 sec/batch\n",
      "Epoch: 5/20...  Training Step: 931...  Training loss: 1.7090...  0.0959 sec/batch\n",
      "Epoch: 5/20...  Training Step: 932...  Training loss: 1.6839...  0.1246 sec/batch\n",
      "Epoch: 5/20...  Training Step: 933...  Training loss: 1.7352...  0.1295 sec/batch\n",
      "Epoch: 5/20...  Training Step: 934...  Training loss: 1.6790...  0.1168 sec/batch\n",
      "Epoch: 5/20...  Training Step: 935...  Training loss: 1.6729...  0.1339 sec/batch\n",
      "Epoch: 5/20...  Training Step: 936...  Training loss: 1.7048...  0.1344 sec/batch\n",
      "Epoch: 5/20...  Training Step: 937...  Training loss: 1.6612...  0.1147 sec/batch\n",
      "Epoch: 5/20...  Training Step: 938...  Training loss: 1.6918...  0.1300 sec/batch\n",
      "Epoch: 5/20...  Training Step: 939...  Training loss: 1.6923...  0.1346 sec/batch\n",
      "Epoch: 5/20...  Training Step: 940...  Training loss: 1.7002...  0.1324 sec/batch\n",
      "Epoch: 5/20...  Training Step: 941...  Training loss: 1.6768...  0.1348 sec/batch\n",
      "Epoch: 5/20...  Training Step: 942...  Training loss: 1.6679...  0.1282 sec/batch\n",
      "Epoch: 5/20...  Training Step: 943...  Training loss: 1.6422...  0.1339 sec/batch\n",
      "Epoch: 5/20...  Training Step: 944...  Training loss: 1.6741...  0.1261 sec/batch\n",
      "Epoch: 5/20...  Training Step: 945...  Training loss: 1.6727...  0.1312 sec/batch\n",
      "Epoch: 5/20...  Training Step: 946...  Training loss: 1.6745...  0.1340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 947...  Training loss: 1.6690...  0.1009 sec/batch\n",
      "Epoch: 5/20...  Training Step: 948...  Training loss: 1.6717...  0.1236 sec/batch\n",
      "Epoch: 5/20...  Training Step: 949...  Training loss: 1.6836...  0.1340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.6621...  0.1352 sec/batch\n",
      "Epoch: 5/20...  Training Step: 951...  Training loss: 1.6247...  0.1146 sec/batch\n",
      "Epoch: 5/20...  Training Step: 952...  Training loss: 1.6920...  0.1071 sec/batch\n",
      "Epoch: 5/20...  Training Step: 953...  Training loss: 1.6948...  0.1292 sec/batch\n",
      "Epoch: 5/20...  Training Step: 954...  Training loss: 1.6627...  0.1341 sec/batch\n",
      "Epoch: 5/20...  Training Step: 955...  Training loss: 1.6700...  0.1339 sec/batch\n",
      "Epoch: 5/20...  Training Step: 956...  Training loss: 1.6689...  0.0931 sec/batch\n",
      "Epoch: 5/20...  Training Step: 957...  Training loss: 1.6558...  0.0929 sec/batch\n",
      "Epoch: 5/20...  Training Step: 958...  Training loss: 1.6560...  0.0925 sec/batch\n",
      "Epoch: 5/20...  Training Step: 959...  Training loss: 1.6849...  0.1010 sec/batch\n",
      "Epoch: 5/20...  Training Step: 960...  Training loss: 1.7370...  0.0930 sec/batch\n",
      "Epoch: 5/20...  Training Step: 961...  Training loss: 1.6521...  0.1012 sec/batch\n",
      "Epoch: 5/20...  Training Step: 962...  Training loss: 1.6478...  0.1298 sec/batch\n",
      "Epoch: 5/20...  Training Step: 963...  Training loss: 1.6405...  0.1341 sec/batch\n",
      "Epoch: 5/20...  Training Step: 964...  Training loss: 1.6387...  0.1340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 965...  Training loss: 1.6711...  0.1003 sec/batch\n",
      "Epoch: 5/20...  Training Step: 966...  Training loss: 1.6674...  0.0928 sec/batch\n",
      "Epoch: 5/20...  Training Step: 967...  Training loss: 1.6654...  0.0928 sec/batch\n",
      "Epoch: 5/20...  Training Step: 968...  Training loss: 1.6470...  0.0932 sec/batch\n",
      "Epoch: 5/20...  Training Step: 969...  Training loss: 1.6332...  0.0922 sec/batch\n",
      "Epoch: 5/20...  Training Step: 970...  Training loss: 1.6726...  0.0922 sec/batch\n",
      "Epoch: 5/20...  Training Step: 971...  Training loss: 1.6416...  0.0932 sec/batch\n",
      "Epoch: 5/20...  Training Step: 972...  Training loss: 1.6197...  0.0976 sec/batch\n",
      "Epoch: 5/20...  Training Step: 973...  Training loss: 1.6242...  0.1275 sec/batch\n",
      "Epoch: 5/20...  Training Step: 974...  Training loss: 1.6495...  0.1341 sec/batch\n",
      "Epoch: 5/20...  Training Step: 975...  Training loss: 1.6462...  0.1181 sec/batch\n",
      "Epoch: 5/20...  Training Step: 976...  Training loss: 1.6593...  0.0933 sec/batch\n",
      "Epoch: 5/20...  Training Step: 977...  Training loss: 1.6454...  0.0933 sec/batch\n",
      "Epoch: 5/20...  Training Step: 978...  Training loss: 1.6377...  0.0926 sec/batch\n",
      "Epoch: 5/20...  Training Step: 979...  Training loss: 1.6645...  0.0932 sec/batch\n",
      "Epoch: 5/20...  Training Step: 980...  Training loss: 1.6380...  0.0925 sec/batch\n",
      "Epoch: 5/20...  Training Step: 981...  Training loss: 1.6386...  0.1139 sec/batch\n",
      "Epoch: 5/20...  Training Step: 982...  Training loss: 1.6515...  0.1341 sec/batch\n",
      "Epoch: 5/20...  Training Step: 983...  Training loss: 1.6311...  0.1346 sec/batch\n",
      "Epoch: 5/20...  Training Step: 984...  Training loss: 1.6274...  0.1274 sec/batch\n",
      "Epoch: 5/20...  Training Step: 985...  Training loss: 1.6465...  0.0925 sec/batch\n",
      "Epoch: 5/20...  Training Step: 986...  Training loss: 1.6241...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 987...  Training loss: 1.6121...  0.0921 sec/batch\n",
      "Epoch: 5/20...  Training Step: 988...  Training loss: 1.6521...  0.0928 sec/batch\n",
      "Epoch: 5/20...  Training Step: 989...  Training loss: 1.6297...  0.0927 sec/batch\n",
      "Epoch: 5/20...  Training Step: 990...  Training loss: 1.6217...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 991...  Training loss: 1.7388...  0.0925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 992...  Training loss: 1.6386...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 993...  Training loss: 1.6216...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 994...  Training loss: 1.6480...  0.0924 sec/batch\n",
      "Epoch: 6/20...  Training Step: 995...  Training loss: 1.6396...  0.0924 sec/batch\n",
      "Epoch: 6/20...  Training Step: 996...  Training loss: 1.6002...  0.0925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 997...  Training loss: 1.6409...  0.0926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 998...  Training loss: 1.6302...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 999...  Training loss: 1.6582...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.6296...  0.0924 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1001...  Training loss: 1.6059...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1002...  Training loss: 1.6182...  0.0925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1003...  Training loss: 1.6353...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1004...  Training loss: 1.6660...  0.0930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1005...  Training loss: 1.6240...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1006...  Training loss: 1.6122...  0.0921 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1007...  Training loss: 1.6446...  0.0925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1008...  Training loss: 1.6665...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1009...  Training loss: 1.6397...  0.0926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1010...  Training loss: 1.6567...  0.1104 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1011...  Training loss: 1.6332...  0.1339 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1012...  Training loss: 1.6542...  0.1343 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1013...  Training loss: 1.6316...  0.1191 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1014...  Training loss: 1.6322...  0.0933 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1015...  Training loss: 1.6426...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1016...  Training loss: 1.5943...  0.0925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1017...  Training loss: 1.5999...  0.0924 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1018...  Training loss: 1.6525...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1019...  Training loss: 1.6547...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1020...  Training loss: 1.6386...  0.0926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1021...  Training loss: 1.6350...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1022...  Training loss: 1.6061...  0.0925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1023...  Training loss: 1.6402...  0.1199 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1024...  Training loss: 1.6458...  0.1293 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1025...  Training loss: 1.6260...  0.1343 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1026...  Training loss: 1.6229...  0.1330 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1027...  Training loss: 1.6086...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1028...  Training loss: 1.5890...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1029...  Training loss: 1.5821...  0.0925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1030...  Training loss: 1.5995...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1031...  Training loss: 1.6001...  0.0921 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1032...  Training loss: 1.6504...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1033...  Training loss: 1.6012...  0.0934 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1034...  Training loss: 1.5964...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1035...  Training loss: 1.6313...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1036...  Training loss: 1.5828...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1037...  Training loss: 1.6052...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1038...  Training loss: 1.6014...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1039...  Training loss: 1.5971...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1040...  Training loss: 1.6533...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1041...  Training loss: 1.6104...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1042...  Training loss: 1.6723...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1043...  Training loss: 1.6238...  0.0930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1044...  Training loss: 1.6254...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1045...  Training loss: 1.6087...  0.0926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1046...  Training loss: 1.6276...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1047...  Training loss: 1.6440...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1048...  Training loss: 1.6040...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1049...  Training loss: 1.5966...  0.1022 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.6528...  0.1309 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1051...  Training loss: 1.6142...  0.1248 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1052...  Training loss: 1.6653...  0.1275 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1053...  Training loss: 1.6301...  0.1335 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1054...  Training loss: 1.6326...  0.1266 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1055...  Training loss: 1.6095...  0.1341 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1056...  Training loss: 1.6324...  0.1333 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1057...  Training loss: 1.6274...  0.1342 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1058...  Training loss: 1.5928...  0.0936 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1059...  Training loss: 1.6116...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1060...  Training loss: 1.6113...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1061...  Training loss: 1.6614...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1062...  Training loss: 1.6263...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1063...  Training loss: 1.6413...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1064...  Training loss: 1.6051...  0.1170 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1065...  Training loss: 1.6093...  0.1338 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1066...  Training loss: 1.6439...  0.1341 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1067...  Training loss: 1.6036...  0.0935 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1068...  Training loss: 1.6058...  0.1228 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1069...  Training loss: 1.5705...  0.1345 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1070...  Training loss: 1.5983...  0.1346 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1071...  Training loss: 1.5594...  0.1112 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1072...  Training loss: 1.6126...  0.1041 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1073...  Training loss: 1.5723...  0.1338 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1074...  Training loss: 1.6079...  0.1341 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1075...  Training loss: 1.5822...  0.1090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1076...  Training loss: 1.5932...  0.1325 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1077...  Training loss: 1.5775...  0.1345 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1078...  Training loss: 1.5796...  0.1239 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1079...  Training loss: 1.5738...  0.1344 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1080...  Training loss: 1.6269...  0.1340 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1081...  Training loss: 1.5760...  0.1342 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1082...  Training loss: 1.5869...  0.1242 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1083...  Training loss: 1.5757...  0.0934 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1084...  Training loss: 1.5761...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1085...  Training loss: 1.5727...  0.0924 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1086...  Training loss: 1.6104...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1087...  Training loss: 1.5924...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1088...  Training loss: 1.5589...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1089...  Training loss: 1.5798...  0.1177 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1090...  Training loss: 1.5588...  0.1341 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1091...  Training loss: 1.5990...  0.1347 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1092...  Training loss: 1.5866...  0.1321 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1093...  Training loss: 1.5835...  0.1340 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1094...  Training loss: 1.5961...  0.1340 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1095...  Training loss: 1.5778...  0.1344 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1096...  Training loss: 1.5933...  0.1165 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1097...  Training loss: 1.5909...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1098...  Training loss: 1.5924...  0.1185 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1099...  Training loss: 1.5927...  0.1349 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.6046...  0.1346 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1101...  Training loss: 1.5722...  0.1339 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1102...  Training loss: 1.5893...  0.1332 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1103...  Training loss: 1.5774...  0.1340 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1104...  Training loss: 1.5855...  0.1342 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1105...  Training loss: 1.5641...  0.1198 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1106...  Training loss: 1.5490...  0.1342 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1107...  Training loss: 1.5936...  0.1343 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1108...  Training loss: 1.5863...  0.1135 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1109...  Training loss: 1.5872...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1110...  Training loss: 1.5810...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1111...  Training loss: 1.5866...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1112...  Training loss: 1.5505...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1113...  Training loss: 1.5482...  0.0925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1114...  Training loss: 1.6004...  0.0924 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1115...  Training loss: 1.5860...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1116...  Training loss: 1.5382...  0.0934 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1117...  Training loss: 1.6071...  0.0930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1118...  Training loss: 1.5982...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1119...  Training loss: 1.5811...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1120...  Training loss: 1.5630...  0.0926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1121...  Training loss: 1.5383...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1122...  Training loss: 1.5591...  0.0930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1123...  Training loss: 1.6063...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1124...  Training loss: 1.5889...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1125...  Training loss: 1.5942...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1126...  Training loss: 1.5977...  0.0933 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1127...  Training loss: 1.6087...  0.0933 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1128...  Training loss: 1.5855...  0.1108 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1129...  Training loss: 1.6062...  0.1332 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1130...  Training loss: 1.5818...  0.1342 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1131...  Training loss: 1.6405...  0.1282 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1132...  Training loss: 1.5800...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1133...  Training loss: 1.5911...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1134...  Training loss: 1.6094...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1135...  Training loss: 1.5698...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1136...  Training loss: 1.6009...  0.0923 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1137...  Training loss: 1.5854...  0.0924 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1138...  Training loss: 1.6145...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1139...  Training loss: 1.5976...  0.1243 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1140...  Training loss: 1.5616...  0.1343 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1141...  Training loss: 1.5419...  0.0985 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1142...  Training loss: 1.5853...  0.0925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1143...  Training loss: 1.6020...  0.0929 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1144...  Training loss: 1.5900...  0.0930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1145...  Training loss: 1.5743...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1146...  Training loss: 1.5859...  0.0926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1147...  Training loss: 1.5985...  0.0930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1148...  Training loss: 1.5731...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1149...  Training loss: 1.5415...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.6041...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1151...  Training loss: 1.6113...  0.0930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1152...  Training loss: 1.5785...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1153...  Training loss: 1.5934...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1154...  Training loss: 1.5880...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1155...  Training loss: 1.5784...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1156...  Training loss: 1.5657...  0.0926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1157...  Training loss: 1.6037...  0.1249 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1158...  Training loss: 1.6477...  0.1316 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1159...  Training loss: 1.5660...  0.1132 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1160...  Training loss: 1.5754...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1161...  Training loss: 1.5657...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1162...  Training loss: 1.5515...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1163...  Training loss: 1.5905...  0.0930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1164...  Training loss: 1.5865...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1165...  Training loss: 1.5996...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1166...  Training loss: 1.5597...  0.0933 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1167...  Training loss: 1.5569...  0.0930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1168...  Training loss: 1.5995...  0.0930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1169...  Training loss: 1.5406...  0.0925 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1170...  Training loss: 1.5407...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1171...  Training loss: 1.5390...  0.0934 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1172...  Training loss: 1.5586...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1173...  Training loss: 1.5736...  0.0926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1174...  Training loss: 1.5633...  0.0922 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1175...  Training loss: 1.5562...  0.0926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1176...  Training loss: 1.5530...  0.0932 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1177...  Training loss: 1.5872...  0.0931 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1178...  Training loss: 1.5608...  0.0959 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1179...  Training loss: 1.5680...  0.0980 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1180...  Training loss: 1.5712...  0.0924 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1181...  Training loss: 1.5517...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1182...  Training loss: 1.5419...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1183...  Training loss: 1.5676...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1184...  Training loss: 1.5426...  0.0926 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1185...  Training loss: 1.5221...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1186...  Training loss: 1.5649...  0.0928 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1187...  Training loss: 1.5475...  0.0927 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1188...  Training loss: 1.5521...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1189...  Training loss: 1.6521...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1190...  Training loss: 1.5625...  0.0930 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1191...  Training loss: 1.5525...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1192...  Training loss: 1.5725...  0.1120 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1193...  Training loss: 1.5439...  0.0932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1194...  Training loss: 1.5212...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1195...  Training loss: 1.5632...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1196...  Training loss: 1.5441...  0.0933 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1197...  Training loss: 1.5796...  0.0959 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1198...  Training loss: 1.5528...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1199...  Training loss: 1.5338...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.5425...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1201...  Training loss: 1.5543...  0.1121 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1202...  Training loss: 1.5930...  0.1328 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1203...  Training loss: 1.5563...  0.1212 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1204...  Training loss: 1.5429...  0.1216 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1205...  Training loss: 1.5515...  0.1339 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1206...  Training loss: 1.5780...  0.1342 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1207...  Training loss: 1.5672...  0.1067 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1208...  Training loss: 1.5826...  0.1027 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1209...  Training loss: 1.5547...  0.1331 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1210...  Training loss: 1.5860...  0.1343 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1211...  Training loss: 1.5552...  0.1000 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1212...  Training loss: 1.5619...  0.1233 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1213...  Training loss: 1.5724...  0.1304 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1214...  Training loss: 1.5158...  0.1343 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1215...  Training loss: 1.5268...  0.1013 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1216...  Training loss: 1.5723...  0.0932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1217...  Training loss: 1.5719...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1218...  Training loss: 1.5674...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1219...  Training loss: 1.5542...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1220...  Training loss: 1.5272...  0.0925 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1221...  Training loss: 1.5709...  0.0924 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1222...  Training loss: 1.5639...  0.0933 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1223...  Training loss: 1.5541...  0.0966 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1224...  Training loss: 1.5573...  0.1287 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1225...  Training loss: 1.5385...  0.1342 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1226...  Training loss: 1.5185...  0.1063 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1227...  Training loss: 1.5033...  0.0925 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1228...  Training loss: 1.5372...  0.1181 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1229...  Training loss: 1.5197...  0.1270 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1230...  Training loss: 1.5847...  0.1346 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1231...  Training loss: 1.5314...  0.0985 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1232...  Training loss: 1.5276...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1233...  Training loss: 1.5613...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1234...  Training loss: 1.5142...  0.0923 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1235...  Training loss: 1.5456...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1236...  Training loss: 1.5330...  0.0932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1237...  Training loss: 1.5319...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1238...  Training loss: 1.5813...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1239...  Training loss: 1.5254...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1240...  Training loss: 1.6014...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1241...  Training loss: 1.5514...  0.0933 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1242...  Training loss: 1.5555...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1243...  Training loss: 1.5372...  0.0933 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1244...  Training loss: 1.5416...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1245...  Training loss: 1.5710...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1246...  Training loss: 1.5339...  0.1261 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1247...  Training loss: 1.5304...  0.1342 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1248...  Training loss: 1.5833...  0.1261 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1249...  Training loss: 1.5457...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.5949...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1251...  Training loss: 1.5683...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1252...  Training loss: 1.5605...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1253...  Training loss: 1.5387...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1254...  Training loss: 1.5585...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1255...  Training loss: 1.5539...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1256...  Training loss: 1.5218...  0.1296 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1257...  Training loss: 1.5532...  0.1288 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1258...  Training loss: 1.5370...  0.1344 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1259...  Training loss: 1.5818...  0.1018 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1260...  Training loss: 1.5589...  0.1340 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1261...  Training loss: 1.5680...  0.1316 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1262...  Training loss: 1.5320...  0.1126 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1263...  Training loss: 1.5321...  0.0933 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1264...  Training loss: 1.5572...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1265...  Training loss: 1.5315...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1266...  Training loss: 1.5329...  0.1239 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1267...  Training loss: 1.4920...  0.1306 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1268...  Training loss: 1.5325...  0.1340 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1269...  Training loss: 1.4992...  0.1244 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1270...  Training loss: 1.5503...  0.1270 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1271...  Training loss: 1.4997...  0.1341 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1272...  Training loss: 1.5304...  0.0935 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1273...  Training loss: 1.5177...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1274...  Training loss: 1.5394...  0.1147 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1275...  Training loss: 1.5149...  0.1344 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1276...  Training loss: 1.5117...  0.1345 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1277...  Training loss: 1.5060...  0.1140 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1278...  Training loss: 1.5480...  0.1299 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1279...  Training loss: 1.5142...  0.1336 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1280...  Training loss: 1.5191...  0.1186 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1281...  Training loss: 1.5161...  0.1300 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1282...  Training loss: 1.5045...  0.1287 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1283...  Training loss: 1.5166...  0.1269 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1284...  Training loss: 1.5376...  0.1310 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1285...  Training loss: 1.5320...  0.1302 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1286...  Training loss: 1.4970...  0.1225 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1287...  Training loss: 1.5177...  0.1334 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1288...  Training loss: 1.4996...  0.1234 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1289...  Training loss: 1.5305...  0.1311 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1290...  Training loss: 1.5230...  0.1310 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1291...  Training loss: 1.5275...  0.1260 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1292...  Training loss: 1.5238...  0.1309 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1293...  Training loss: 1.5134...  0.1244 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1294...  Training loss: 1.5178...  0.1327 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1295...  Training loss: 1.5304...  0.1323 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1296...  Training loss: 1.5259...  0.1237 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1297...  Training loss: 1.5169...  0.1295 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1298...  Training loss: 1.5450...  0.1273 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1299...  Training loss: 1.5053...  0.1319 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.5301...  0.1319 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1301...  Training loss: 1.5240...  0.1229 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1302...  Training loss: 1.5121...  0.1332 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1303...  Training loss: 1.4958...  0.1300 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1304...  Training loss: 1.4862...  0.1341 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1305...  Training loss: 1.5285...  0.1253 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1306...  Training loss: 1.5342...  0.1307 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1307...  Training loss: 1.5120...  0.1342 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1308...  Training loss: 1.5068...  0.1129 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1309...  Training loss: 1.5267...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1310...  Training loss: 1.4818...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1311...  Training loss: 1.4733...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1312...  Training loss: 1.5331...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1313...  Training loss: 1.5232...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1314...  Training loss: 1.4810...  0.0932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1315...  Training loss: 1.5364...  0.0930 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1316...  Training loss: 1.5330...  0.0932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1317...  Training loss: 1.5171...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1318...  Training loss: 1.4953...  0.0924 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1319...  Training loss: 1.4812...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1320...  Training loss: 1.4846...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1321...  Training loss: 1.5403...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1322...  Training loss: 1.5296...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1323...  Training loss: 1.5303...  0.0933 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1324...  Training loss: 1.5223...  0.0930 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1325...  Training loss: 1.5543...  0.0930 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1326...  Training loss: 1.5333...  0.0924 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1327...  Training loss: 1.5296...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1328...  Training loss: 1.5195...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1329...  Training loss: 1.5726...  0.0933 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1330...  Training loss: 1.5165...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1331...  Training loss: 1.5184...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1332...  Training loss: 1.5420...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1333...  Training loss: 1.4988...  0.0933 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1334...  Training loss: 1.5451...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1335...  Training loss: 1.5284...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1336...  Training loss: 1.5617...  0.0937 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1337...  Training loss: 1.5363...  0.0923 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1338...  Training loss: 1.5051...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1339...  Training loss: 1.4840...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1340...  Training loss: 1.5090...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1341...  Training loss: 1.5319...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1342...  Training loss: 1.5166...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1343...  Training loss: 1.5239...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1344...  Training loss: 1.5208...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1345...  Training loss: 1.5276...  0.0935 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1346...  Training loss: 1.5211...  0.0934 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1347...  Training loss: 1.4804...  0.0932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1348...  Training loss: 1.5455...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1349...  Training loss: 1.5492...  0.0923 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.5211...  0.0925 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1351...  Training loss: 1.5343...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1352...  Training loss: 1.5146...  0.0923 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1353...  Training loss: 1.5250...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1354...  Training loss: 1.5160...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1355...  Training loss: 1.5347...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1356...  Training loss: 1.5867...  0.0932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1357...  Training loss: 1.5097...  0.0925 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1358...  Training loss: 1.5154...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1359...  Training loss: 1.5061...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1360...  Training loss: 1.4939...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1361...  Training loss: 1.5306...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1362...  Training loss: 1.5210...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1363...  Training loss: 1.5315...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1364...  Training loss: 1.4914...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1365...  Training loss: 1.4894...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1366...  Training loss: 1.5447...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1367...  Training loss: 1.4880...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1368...  Training loss: 1.4734...  0.0932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1369...  Training loss: 1.4900...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1370...  Training loss: 1.4949...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1371...  Training loss: 1.5086...  0.0927 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1372...  Training loss: 1.5036...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1373...  Training loss: 1.4905...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1374...  Training loss: 1.4853...  0.0926 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1375...  Training loss: 1.5206...  0.0925 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1376...  Training loss: 1.5009...  0.0924 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1377...  Training loss: 1.5078...  0.0931 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1378...  Training loss: 1.5076...  0.0925 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1379...  Training loss: 1.4957...  0.0930 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1380...  Training loss: 1.4843...  0.0932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1381...  Training loss: 1.5019...  0.0929 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1382...  Training loss: 1.4774...  0.0932 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1383...  Training loss: 1.4792...  0.0925 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1384...  Training loss: 1.5176...  0.0928 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1385...  Training loss: 1.4911...  0.0923 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1386...  Training loss: 1.4886...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1387...  Training loss: 1.5953...  0.0925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1388...  Training loss: 1.5040...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1389...  Training loss: 1.4874...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1390...  Training loss: 1.5132...  0.0925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1391...  Training loss: 1.4900...  0.0923 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1392...  Training loss: 1.4621...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1393...  Training loss: 1.4953...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1394...  Training loss: 1.4856...  0.0927 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1395...  Training loss: 1.5104...  0.0926 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1396...  Training loss: 1.4875...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1397...  Training loss: 1.4645...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1398...  Training loss: 1.4826...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1399...  Training loss: 1.4845...  0.0926 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.5215...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1401...  Training loss: 1.4828...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1402...  Training loss: 1.4733...  0.0927 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1403...  Training loss: 1.5087...  0.0925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1404...  Training loss: 1.5137...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1405...  Training loss: 1.4956...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1406...  Training loss: 1.5181...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1407...  Training loss: 1.4912...  0.0926 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1408...  Training loss: 1.5086...  0.0925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1409...  Training loss: 1.4871...  0.0926 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1410...  Training loss: 1.5114...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1411...  Training loss: 1.4927...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1412...  Training loss: 1.4590...  0.0925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1413...  Training loss: 1.4592...  0.0927 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1414...  Training loss: 1.5036...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1415...  Training loss: 1.5092...  0.0926 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1416...  Training loss: 1.5063...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1417...  Training loss: 1.4884...  0.0927 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1418...  Training loss: 1.4573...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1419...  Training loss: 1.5084...  0.0925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1420...  Training loss: 1.5138...  0.0927 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1421...  Training loss: 1.4907...  0.0927 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1422...  Training loss: 1.4969...  0.0924 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1423...  Training loss: 1.4602...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1424...  Training loss: 1.4534...  0.0925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1425...  Training loss: 1.4378...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1426...  Training loss: 1.4716...  0.0927 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1427...  Training loss: 1.4675...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1428...  Training loss: 1.5143...  0.0927 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1429...  Training loss: 1.4622...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1430...  Training loss: 1.4555...  0.0925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1431...  Training loss: 1.4896...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1432...  Training loss: 1.4457...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1433...  Training loss: 1.4804...  0.0925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1434...  Training loss: 1.4710...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1435...  Training loss: 1.4699...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1436...  Training loss: 1.5095...  0.0926 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1437...  Training loss: 1.4600...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1438...  Training loss: 1.5254...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1439...  Training loss: 1.4891...  0.0926 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1440...  Training loss: 1.4892...  0.0926 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1441...  Training loss: 1.4791...  0.0925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1442...  Training loss: 1.4853...  0.0923 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1443...  Training loss: 1.4996...  0.0924 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1444...  Training loss: 1.4710...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1445...  Training loss: 1.4497...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1446...  Training loss: 1.5114...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1447...  Training loss: 1.4932...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1448...  Training loss: 1.5359...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1449...  Training loss: 1.5136...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.4942...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1451...  Training loss: 1.4767...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1452...  Training loss: 1.4880...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1453...  Training loss: 1.4967...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1454...  Training loss: 1.4587...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1455...  Training loss: 1.4812...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1456...  Training loss: 1.4716...  0.0937 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1457...  Training loss: 1.5256...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1458...  Training loss: 1.4925...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1459...  Training loss: 1.5130...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1460...  Training loss: 1.4551...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1461...  Training loss: 1.4719...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1462...  Training loss: 1.5041...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1463...  Training loss: 1.4735...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1464...  Training loss: 1.4687...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1465...  Training loss: 1.4280...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1466...  Training loss: 1.4704...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1467...  Training loss: 1.4357...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1468...  Training loss: 1.4799...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1469...  Training loss: 1.4418...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1470...  Training loss: 1.4599...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1471...  Training loss: 1.4425...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1472...  Training loss: 1.4650...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1473...  Training loss: 1.4433...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1474...  Training loss: 1.4449...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1475...  Training loss: 1.4424...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1476...  Training loss: 1.4813...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1477...  Training loss: 1.4490...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1478...  Training loss: 1.4532...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1479...  Training loss: 1.4472...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1480...  Training loss: 1.4451...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1481...  Training loss: 1.4441...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1482...  Training loss: 1.4757...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1483...  Training loss: 1.4648...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1484...  Training loss: 1.4408...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1485...  Training loss: 1.4596...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1486...  Training loss: 1.4423...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1487...  Training loss: 1.4734...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1488...  Training loss: 1.4542...  0.0927 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1489...  Training loss: 1.4647...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1490...  Training loss: 1.4616...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1491...  Training loss: 1.4554...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1492...  Training loss: 1.4587...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1493...  Training loss: 1.4602...  0.0937 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1494...  Training loss: 1.4713...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1495...  Training loss: 1.4559...  0.0937 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1496...  Training loss: 1.4778...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1497...  Training loss: 1.4540...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1498...  Training loss: 1.4572...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1499...  Training loss: 1.4579...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.4516...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1501...  Training loss: 1.4430...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1502...  Training loss: 1.4216...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1503...  Training loss: 1.4560...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1504...  Training loss: 1.4711...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1505...  Training loss: 1.4626...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1506...  Training loss: 1.4645...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1507...  Training loss: 1.4664...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1508...  Training loss: 1.4313...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1509...  Training loss: 1.4243...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1510...  Training loss: 1.4699...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1511...  Training loss: 1.4646...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1512...  Training loss: 1.4128...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1513...  Training loss: 1.4719...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1514...  Training loss: 1.4702...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1515...  Training loss: 1.4509...  0.0938 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1516...  Training loss: 1.4229...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1517...  Training loss: 1.4207...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1518...  Training loss: 1.4297...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1519...  Training loss: 1.4821...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1520...  Training loss: 1.4675...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1521...  Training loss: 1.4672...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1522...  Training loss: 1.4642...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1523...  Training loss: 1.4918...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1524...  Training loss: 1.4803...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1525...  Training loss: 1.4582...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1526...  Training loss: 1.4551...  0.0937 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1527...  Training loss: 1.5100...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1528...  Training loss: 1.4590...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1529...  Training loss: 1.4517...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1530...  Training loss: 1.4839...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1531...  Training loss: 1.4381...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1532...  Training loss: 1.4857...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1533...  Training loss: 1.4703...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1534...  Training loss: 1.4881...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1535...  Training loss: 1.4746...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1536...  Training loss: 1.4465...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1537...  Training loss: 1.4157...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1538...  Training loss: 1.4444...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1539...  Training loss: 1.4655...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1540...  Training loss: 1.4458...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1541...  Training loss: 1.4500...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1542...  Training loss: 1.4452...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1543...  Training loss: 1.4671...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1544...  Training loss: 1.4553...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1545...  Training loss: 1.4181...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1546...  Training loss: 1.4703...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1547...  Training loss: 1.4886...  0.0928 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1548...  Training loss: 1.4471...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1549...  Training loss: 1.4660...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.4514...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1551...  Training loss: 1.4506...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1552...  Training loss: 1.4442...  0.0929 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1553...  Training loss: 1.4713...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1554...  Training loss: 1.5173...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1555...  Training loss: 1.4519...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1556...  Training loss: 1.4434...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1557...  Training loss: 1.4383...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1558...  Training loss: 1.4321...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1559...  Training loss: 1.4698...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1560...  Training loss: 1.4603...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1561...  Training loss: 1.4597...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1562...  Training loss: 1.4240...  0.0938 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1563...  Training loss: 1.4269...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1564...  Training loss: 1.4800...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1565...  Training loss: 1.4288...  0.0936 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1566...  Training loss: 1.4158...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1567...  Training loss: 1.4242...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1568...  Training loss: 1.4366...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1569...  Training loss: 1.4458...  0.0932 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1570...  Training loss: 1.4414...  0.0934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1571...  Training loss: 1.4440...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1572...  Training loss: 1.4336...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1573...  Training loss: 1.4644...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1574...  Training loss: 1.4422...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1575...  Training loss: 1.4368...  0.0933 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1576...  Training loss: 1.4460...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1577...  Training loss: 1.4360...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1578...  Training loss: 1.4268...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1579...  Training loss: 1.4465...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1580...  Training loss: 1.4209...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1581...  Training loss: 1.4142...  0.0931 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1582...  Training loss: 1.4507...  0.0930 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1583...  Training loss: 1.4264...  0.0935 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1584...  Training loss: 1.4311...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1585...  Training loss: 1.5688...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1586...  Training loss: 1.4554...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1587...  Training loss: 1.4494...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1588...  Training loss: 1.4605...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1589...  Training loss: 1.4219...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1590...  Training loss: 1.4175...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1591...  Training loss: 1.4503...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1592...  Training loss: 1.4305...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1593...  Training loss: 1.4531...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1594...  Training loss: 1.4299...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1595...  Training loss: 1.4236...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1596...  Training loss: 1.4432...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1597...  Training loss: 1.4338...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1598...  Training loss: 1.4607...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1599...  Training loss: 1.4269...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.4182...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1601...  Training loss: 1.4567...  0.0938 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1602...  Training loss: 1.4549...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1603...  Training loss: 1.4485...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1604...  Training loss: 1.4585...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1605...  Training loss: 1.4340...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1606...  Training loss: 1.4575...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1607...  Training loss: 1.4389...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1608...  Training loss: 1.4561...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1609...  Training loss: 1.4411...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1610...  Training loss: 1.4002...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1611...  Training loss: 1.4072...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1612...  Training loss: 1.4546...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1613...  Training loss: 1.4690...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1614...  Training loss: 1.4626...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1615...  Training loss: 1.4336...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1616...  Training loss: 1.4057...  0.0928 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1617...  Training loss: 1.4535...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1618...  Training loss: 1.4543...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1619...  Training loss: 1.4398...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1620...  Training loss: 1.4358...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1621...  Training loss: 1.4160...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1622...  Training loss: 1.3997...  0.0928 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1623...  Training loss: 1.3849...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1624...  Training loss: 1.4136...  0.0928 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1625...  Training loss: 1.4142...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1626...  Training loss: 1.4635...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1627...  Training loss: 1.4156...  0.0938 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1628...  Training loss: 1.4135...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1629...  Training loss: 1.4423...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1630...  Training loss: 1.4122...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1631...  Training loss: 1.4299...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1632...  Training loss: 1.4304...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1633...  Training loss: 1.4281...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1634...  Training loss: 1.4645...  0.0928 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1635...  Training loss: 1.4062...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1636...  Training loss: 1.4797...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1637...  Training loss: 1.4365...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1638...  Training loss: 1.4448...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1639...  Training loss: 1.4297...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1640...  Training loss: 1.4281...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1641...  Training loss: 1.4592...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1642...  Training loss: 1.4151...  0.0928 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1643...  Training loss: 1.4078...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1644...  Training loss: 1.4606...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1645...  Training loss: 1.4420...  0.0938 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1646...  Training loss: 1.4763...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1647...  Training loss: 1.4597...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1648...  Training loss: 1.4406...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1649...  Training loss: 1.4244...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.4429...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1651...  Training loss: 1.4447...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1652...  Training loss: 1.4126...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1653...  Training loss: 1.4347...  0.0926 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1654...  Training loss: 1.4117...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1655...  Training loss: 1.4816...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1656...  Training loss: 1.4467...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1657...  Training loss: 1.4644...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1658...  Training loss: 1.4181...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1659...  Training loss: 1.4239...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1660...  Training loss: 1.4486...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1661...  Training loss: 1.4208...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1662...  Training loss: 1.4185...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1663...  Training loss: 1.3829...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1664...  Training loss: 1.4333...  0.0927 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1665...  Training loss: 1.3837...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1666...  Training loss: 1.4286...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1667...  Training loss: 1.4013...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1668...  Training loss: 1.4201...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1669...  Training loss: 1.4047...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1670...  Training loss: 1.4213...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1671...  Training loss: 1.3991...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1672...  Training loss: 1.4059...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1673...  Training loss: 1.4020...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1674...  Training loss: 1.4309...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1675...  Training loss: 1.4035...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1676...  Training loss: 1.4111...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1677...  Training loss: 1.3904...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1678...  Training loss: 1.3972...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1679...  Training loss: 1.4056...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1680...  Training loss: 1.4257...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1681...  Training loss: 1.4223...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1682...  Training loss: 1.3950...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1683...  Training loss: 1.4015...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1684...  Training loss: 1.3893...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1685...  Training loss: 1.4249...  0.0927 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1686...  Training loss: 1.4086...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1687...  Training loss: 1.4248...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1688...  Training loss: 1.4189...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1689...  Training loss: 1.4086...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1690...  Training loss: 1.4114...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1691...  Training loss: 1.4309...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1692...  Training loss: 1.4163...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1693...  Training loss: 1.4193...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1694...  Training loss: 1.4347...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1695...  Training loss: 1.3961...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1696...  Training loss: 1.4139...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1697...  Training loss: 1.4116...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1698...  Training loss: 1.4021...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1699...  Training loss: 1.3934...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.3823...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1701...  Training loss: 1.4193...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1702...  Training loss: 1.4218...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1703...  Training loss: 1.4111...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1704...  Training loss: 1.4108...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1705...  Training loss: 1.4142...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1706...  Training loss: 1.3747...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1707...  Training loss: 1.3722...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1708...  Training loss: 1.4401...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1709...  Training loss: 1.4080...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1710...  Training loss: 1.3813...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1711...  Training loss: 1.4200...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1712...  Training loss: 1.4298...  0.0928 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1713...  Training loss: 1.4075...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1714...  Training loss: 1.3805...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1715...  Training loss: 1.3733...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1716...  Training loss: 1.4005...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1717...  Training loss: 1.4361...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1718...  Training loss: 1.4284...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1719...  Training loss: 1.4140...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1720...  Training loss: 1.4142...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1721...  Training loss: 1.4328...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1722...  Training loss: 1.4261...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1723...  Training loss: 1.4240...  0.0927 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1724...  Training loss: 1.4206...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1725...  Training loss: 1.4621...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1726...  Training loss: 1.4055...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1727...  Training loss: 1.4029...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1728...  Training loss: 1.4380...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1729...  Training loss: 1.3947...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1730...  Training loss: 1.4401...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1731...  Training loss: 1.4200...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1732...  Training loss: 1.4496...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1733...  Training loss: 1.4356...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1734...  Training loss: 1.4028...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1735...  Training loss: 1.3831...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1736...  Training loss: 1.3957...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1737...  Training loss: 1.4296...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1738...  Training loss: 1.4105...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1739...  Training loss: 1.4107...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1740...  Training loss: 1.4030...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1741...  Training loss: 1.4134...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1742...  Training loss: 1.4062...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1743...  Training loss: 1.3720...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1744...  Training loss: 1.4438...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1745...  Training loss: 1.4429...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1746...  Training loss: 1.4161...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1747...  Training loss: 1.4180...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1748...  Training loss: 1.4205...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1749...  Training loss: 1.4188...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.4089...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1751...  Training loss: 1.4371...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1752...  Training loss: 1.4821...  0.0933 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1753...  Training loss: 1.4049...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1754...  Training loss: 1.4148...  0.0935 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1755...  Training loss: 1.4058...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1756...  Training loss: 1.4000...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1757...  Training loss: 1.4316...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1758...  Training loss: 1.4228...  0.0941 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1759...  Training loss: 1.4205...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1760...  Training loss: 1.3873...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1761...  Training loss: 1.3882...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1762...  Training loss: 1.4520...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1763...  Training loss: 1.3787...  0.0927 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1764...  Training loss: 1.3706...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1765...  Training loss: 1.3810...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1766...  Training loss: 1.3897...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1767...  Training loss: 1.4144...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1768...  Training loss: 1.3920...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1769...  Training loss: 1.4014...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1770...  Training loss: 1.3899...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1771...  Training loss: 1.4287...  0.0937 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1772...  Training loss: 1.3940...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1773...  Training loss: 1.4005...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1774...  Training loss: 1.3947...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1775...  Training loss: 1.3872...  0.0929 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1776...  Training loss: 1.3922...  0.0934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1777...  Training loss: 1.4081...  0.0930 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1778...  Training loss: 1.3922...  0.0936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1779...  Training loss: 1.3777...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1780...  Training loss: 1.4183...  0.0931 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1781...  Training loss: 1.3971...  0.0932 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1782...  Training loss: 1.3945...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1783...  Training loss: 1.5484...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1784...  Training loss: 1.4442...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1785...  Training loss: 1.4262...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1786...  Training loss: 1.4388...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1787...  Training loss: 1.3928...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1788...  Training loss: 1.3805...  0.0938 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1789...  Training loss: 1.4066...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1790...  Training loss: 1.4015...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1791...  Training loss: 1.4263...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1792...  Training loss: 1.4090...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1793...  Training loss: 1.3934...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1794...  Training loss: 1.4073...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1795...  Training loss: 1.4063...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1796...  Training loss: 1.4341...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1797...  Training loss: 1.3988...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1798...  Training loss: 1.3934...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1799...  Training loss: 1.4192...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.4208...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1801...  Training loss: 1.4060...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1802...  Training loss: 1.4296...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1803...  Training loss: 1.4084...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1804...  Training loss: 1.4192...  0.0928 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1805...  Training loss: 1.4040...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1806...  Training loss: 1.4255...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1807...  Training loss: 1.4112...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1808...  Training loss: 1.3664...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1809...  Training loss: 1.3681...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1810...  Training loss: 1.4118...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1811...  Training loss: 1.4173...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1812...  Training loss: 1.4151...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1813...  Training loss: 1.3955...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1814...  Training loss: 1.3729...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1815...  Training loss: 1.4105...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1816...  Training loss: 1.4101...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1817...  Training loss: 1.3975...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1818...  Training loss: 1.4026...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1819...  Training loss: 1.3758...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1820...  Training loss: 1.3602...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1821...  Training loss: 1.3448...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1822...  Training loss: 1.3797...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1823...  Training loss: 1.3796...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1824...  Training loss: 1.4241...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1825...  Training loss: 1.3802...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1826...  Training loss: 1.3707...  0.0926 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1827...  Training loss: 1.4088...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1828...  Training loss: 1.3637...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1829...  Training loss: 1.4003...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1830...  Training loss: 1.3845...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1831...  Training loss: 1.3922...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1832...  Training loss: 1.4169...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1833...  Training loss: 1.3732...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1834...  Training loss: 1.4357...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1835...  Training loss: 1.3986...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1836...  Training loss: 1.4057...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1837...  Training loss: 1.3841...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1838...  Training loss: 1.3903...  0.0928 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1839...  Training loss: 1.4185...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1840...  Training loss: 1.3870...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1841...  Training loss: 1.3668...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1842...  Training loss: 1.4222...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1843...  Training loss: 1.4016...  0.0938 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1844...  Training loss: 1.4362...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1845...  Training loss: 1.4153...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1846...  Training loss: 1.4018...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1847...  Training loss: 1.3902...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1848...  Training loss: 1.4030...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1849...  Training loss: 1.4075...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.3800...  0.0928 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1851...  Training loss: 1.3994...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1852...  Training loss: 1.3760...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1853...  Training loss: 1.4394...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1854...  Training loss: 1.4074...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1855...  Training loss: 1.4355...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1856...  Training loss: 1.3764...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1857...  Training loss: 1.3858...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1858...  Training loss: 1.4137...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1859...  Training loss: 1.3926...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1860...  Training loss: 1.3764...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1861...  Training loss: 1.3390...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1862...  Training loss: 1.4063...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1863...  Training loss: 1.3548...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1864...  Training loss: 1.3909...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1865...  Training loss: 1.3539...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1866...  Training loss: 1.3869...  0.0939 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1867...  Training loss: 1.3744...  0.0938 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1868...  Training loss: 1.3871...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1869...  Training loss: 1.3622...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1870...  Training loss: 1.3685...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1871...  Training loss: 1.3589...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1872...  Training loss: 1.3925...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1873...  Training loss: 1.3712...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1874...  Training loss: 1.3752...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1875...  Training loss: 1.3634...  0.0938 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1876...  Training loss: 1.3635...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1877...  Training loss: 1.3643...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1878...  Training loss: 1.3925...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1879...  Training loss: 1.3916...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1880...  Training loss: 1.3616...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1881...  Training loss: 1.3720...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1882...  Training loss: 1.3644...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1883...  Training loss: 1.3926...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1884...  Training loss: 1.3885...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1885...  Training loss: 1.3834...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1886...  Training loss: 1.3816...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1887...  Training loss: 1.3786...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1888...  Training loss: 1.3807...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1889...  Training loss: 1.3827...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1890...  Training loss: 1.3851...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1891...  Training loss: 1.3769...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1892...  Training loss: 1.3982...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1893...  Training loss: 1.3771...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1894...  Training loss: 1.3896...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1895...  Training loss: 1.3815...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1896...  Training loss: 1.3695...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1897...  Training loss: 1.3550...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1898...  Training loss: 1.3388...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1899...  Training loss: 1.3834...  0.0947 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.3925...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1901...  Training loss: 1.3779...  0.0927 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1902...  Training loss: 1.3763...  0.0928 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1903...  Training loss: 1.3788...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1904...  Training loss: 1.3524...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1905...  Training loss: 1.3356...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1906...  Training loss: 1.3897...  0.0928 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1907...  Training loss: 1.3785...  0.0926 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1908...  Training loss: 1.3398...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1909...  Training loss: 1.4020...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1910...  Training loss: 1.3929...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1911...  Training loss: 1.3756...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1912...  Training loss: 1.3572...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1913...  Training loss: 1.3391...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1914...  Training loss: 1.3499...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1915...  Training loss: 1.4076...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1916...  Training loss: 1.3999...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1917...  Training loss: 1.3867...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1918...  Training loss: 1.3884...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1919...  Training loss: 1.4105...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1920...  Training loss: 1.4009...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1921...  Training loss: 1.3906...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1922...  Training loss: 1.3855...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1923...  Training loss: 1.4389...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1924...  Training loss: 1.3839...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1925...  Training loss: 1.3789...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1926...  Training loss: 1.4087...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1927...  Training loss: 1.3649...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1928...  Training loss: 1.4076...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1929...  Training loss: 1.3911...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1930...  Training loss: 1.4254...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1931...  Training loss: 1.4109...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1932...  Training loss: 1.3711...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1933...  Training loss: 1.3459...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1934...  Training loss: 1.3653...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1935...  Training loss: 1.3834...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1936...  Training loss: 1.3874...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1937...  Training loss: 1.3879...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1938...  Training loss: 1.3771...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1939...  Training loss: 1.3869...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1940...  Training loss: 1.3778...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1941...  Training loss: 1.3466...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1942...  Training loss: 1.4038...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1943...  Training loss: 1.4109...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1944...  Training loss: 1.3822...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1945...  Training loss: 1.3865...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1946...  Training loss: 1.3828...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1947...  Training loss: 1.3804...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1948...  Training loss: 1.3698...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1949...  Training loss: 1.4043...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.4438...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1951...  Training loss: 1.3875...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1952...  Training loss: 1.3818...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1953...  Training loss: 1.3674...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1954...  Training loss: 1.3719...  0.0935 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1955...  Training loss: 1.4092...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1956...  Training loss: 1.3852...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1957...  Training loss: 1.3915...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1958...  Training loss: 1.3504...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1959...  Training loss: 1.3621...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1960...  Training loss: 1.4135...  0.0934 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1961...  Training loss: 1.3523...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1962...  Training loss: 1.3401...  0.0930 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1963...  Training loss: 1.3527...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1964...  Training loss: 1.3641...  0.0932 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1965...  Training loss: 1.3814...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1966...  Training loss: 1.3662...  0.0929 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1967...  Training loss: 1.3718...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1968...  Training loss: 1.3694...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1969...  Training loss: 1.4010...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1970...  Training loss: 1.3711...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1971...  Training loss: 1.3708...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1972...  Training loss: 1.3708...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1973...  Training loss: 1.3552...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1974...  Training loss: 1.3538...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1975...  Training loss: 1.3748...  0.0933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1976...  Training loss: 1.3488...  0.0928 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1977...  Training loss: 1.3437...  0.0937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1978...  Training loss: 1.3933...  0.0936 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1979...  Training loss: 1.3671...  0.0931 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1980...  Training loss: 1.3666...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1981...  Training loss: 1.5226...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1982...  Training loss: 1.4034...  0.0937 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1983...  Training loss: 1.3858...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1984...  Training loss: 1.3958...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1985...  Training loss: 1.3563...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1986...  Training loss: 1.3479...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1987...  Training loss: 1.3758...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1988...  Training loss: 1.3731...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1989...  Training loss: 1.3926...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1990...  Training loss: 1.3791...  0.0928 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1991...  Training loss: 1.3575...  0.0928 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1992...  Training loss: 1.3666...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1993...  Training loss: 1.3788...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1994...  Training loss: 1.3998...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1995...  Training loss: 1.3675...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1996...  Training loss: 1.3597...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1997...  Training loss: 1.3962...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1998...  Training loss: 1.4000...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1999...  Training loss: 1.3753...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.4027...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2001...  Training loss: 1.3785...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2002...  Training loss: 1.3905...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2003...  Training loss: 1.3544...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2004...  Training loss: 1.3902...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2005...  Training loss: 1.3800...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2006...  Training loss: 1.3355...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2007...  Training loss: 1.3378...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2008...  Training loss: 1.3875...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2009...  Training loss: 1.3827...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2010...  Training loss: 1.4001...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2011...  Training loss: 1.3679...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2012...  Training loss: 1.3476...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2013...  Training loss: 1.3778...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2014...  Training loss: 1.3879...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2015...  Training loss: 1.3671...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2016...  Training loss: 1.3631...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2017...  Training loss: 1.3416...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2018...  Training loss: 1.3312...  0.0937 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2019...  Training loss: 1.3235...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2020...  Training loss: 1.3568...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2021...  Training loss: 1.3498...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2022...  Training loss: 1.3998...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2023...  Training loss: 1.3560...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2024...  Training loss: 1.3305...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2025...  Training loss: 1.3809...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2026...  Training loss: 1.3360...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2027...  Training loss: 1.3579...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2028...  Training loss: 1.3568...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2029...  Training loss: 1.3542...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2030...  Training loss: 1.3821...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2031...  Training loss: 1.3418...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2032...  Training loss: 1.4124...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2033...  Training loss: 1.3717...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2034...  Training loss: 1.3818...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2035...  Training loss: 1.3544...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2036...  Training loss: 1.3657...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2037...  Training loss: 1.3881...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2038...  Training loss: 1.3502...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2039...  Training loss: 1.3502...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2040...  Training loss: 1.3986...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2041...  Training loss: 1.3711...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2042...  Training loss: 1.4044...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2043...  Training loss: 1.3856...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2044...  Training loss: 1.3672...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2045...  Training loss: 1.3529...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2046...  Training loss: 1.3681...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2047...  Training loss: 1.3774...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2048...  Training loss: 1.3452...  0.0937 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2049...  Training loss: 1.3765...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.3576...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2051...  Training loss: 1.4165...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2052...  Training loss: 1.3848...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2053...  Training loss: 1.4087...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2054...  Training loss: 1.3453...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2055...  Training loss: 1.3608...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2056...  Training loss: 1.3882...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2057...  Training loss: 1.3517...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2058...  Training loss: 1.3491...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2059...  Training loss: 1.3163...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2060...  Training loss: 1.3764...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2061...  Training loss: 1.3208...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2062...  Training loss: 1.3663...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2063...  Training loss: 1.3349...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2064...  Training loss: 1.3544...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2065...  Training loss: 1.3367...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2066...  Training loss: 1.3583...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2067...  Training loss: 1.3296...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2068...  Training loss: 1.3440...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2069...  Training loss: 1.3275...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2070...  Training loss: 1.3720...  0.0937 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2071...  Training loss: 1.3386...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2072...  Training loss: 1.3478...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2073...  Training loss: 1.3321...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2074...  Training loss: 1.3368...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2075...  Training loss: 1.3337...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2076...  Training loss: 1.3708...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2077...  Training loss: 1.3672...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2078...  Training loss: 1.3288...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2079...  Training loss: 1.3441...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2080...  Training loss: 1.3302...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2081...  Training loss: 1.3698...  0.0928 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2082...  Training loss: 1.3514...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2083...  Training loss: 1.3610...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2084...  Training loss: 1.3479...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2085...  Training loss: 1.3483...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2086...  Training loss: 1.3491...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2087...  Training loss: 1.3576...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2088...  Training loss: 1.3596...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2089...  Training loss: 1.3478...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2090...  Training loss: 1.3749...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2091...  Training loss: 1.3463...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2092...  Training loss: 1.3678...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2093...  Training loss: 1.3489...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2094...  Training loss: 1.3410...  0.0928 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2095...  Training loss: 1.3352...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2096...  Training loss: 1.3159...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2097...  Training loss: 1.3644...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2098...  Training loss: 1.3577...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2099...  Training loss: 1.3395...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.3506...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2101...  Training loss: 1.3521...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2102...  Training loss: 1.3225...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2103...  Training loss: 1.3120...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2104...  Training loss: 1.3664...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2105...  Training loss: 1.3453...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2106...  Training loss: 1.3060...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2107...  Training loss: 1.3625...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2108...  Training loss: 1.3637...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2109...  Training loss: 1.3452...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2110...  Training loss: 1.3184...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2111...  Training loss: 1.3125...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2112...  Training loss: 1.3336...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2113...  Training loss: 1.3777...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2114...  Training loss: 1.3666...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2115...  Training loss: 1.3636...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2116...  Training loss: 1.3622...  0.0928 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2117...  Training loss: 1.3875...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2118...  Training loss: 1.3704...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2119...  Training loss: 1.3742...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2120...  Training loss: 1.3579...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2121...  Training loss: 1.4037...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2122...  Training loss: 1.3724...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2123...  Training loss: 1.3530...  0.0937 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2124...  Training loss: 1.3841...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2125...  Training loss: 1.3322...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2126...  Training loss: 1.3842...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2127...  Training loss: 1.3652...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2128...  Training loss: 1.3897...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2129...  Training loss: 1.3775...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2130...  Training loss: 1.3454...  0.0925 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2131...  Training loss: 1.3273...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2132...  Training loss: 1.3333...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2133...  Training loss: 1.3687...  0.0937 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2134...  Training loss: 1.3521...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2135...  Training loss: 1.3535...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2136...  Training loss: 1.3491...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2137...  Training loss: 1.3575...  0.0937 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2138...  Training loss: 1.3425...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2139...  Training loss: 1.3175...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2140...  Training loss: 1.3720...  0.0937 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2141...  Training loss: 1.3801...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2142...  Training loss: 1.3539...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2143...  Training loss: 1.3596...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2144...  Training loss: 1.3547...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2145...  Training loss: 1.3521...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2146...  Training loss: 1.3536...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2147...  Training loss: 1.3740...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2148...  Training loss: 1.4104...  0.0928 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2149...  Training loss: 1.3602...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.3499...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2151...  Training loss: 1.3434...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2152...  Training loss: 1.3393...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2153...  Training loss: 1.3779...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2154...  Training loss: 1.3579...  0.0926 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2155...  Training loss: 1.3595...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2156...  Training loss: 1.3202...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2157...  Training loss: 1.3377...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2158...  Training loss: 1.3899...  0.0930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2159...  Training loss: 1.3402...  0.0929 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2160...  Training loss: 1.3191...  0.0938 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2161...  Training loss: 1.3324...  0.0937 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2162...  Training loss: 1.3471...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2163...  Training loss: 1.3487...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2164...  Training loss: 1.3457...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2165...  Training loss: 1.3463...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2166...  Training loss: 1.3343...  0.0932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2167...  Training loss: 1.3784...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2168...  Training loss: 1.3347...  0.0931 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2169...  Training loss: 1.3462...  0.0934 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2170...  Training loss: 1.3480...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2171...  Training loss: 1.3257...  0.0935 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2172...  Training loss: 1.3207...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2173...  Training loss: 1.3486...  0.0938 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2174...  Training loss: 1.3228...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2175...  Training loss: 1.3182...  0.0938 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2176...  Training loss: 1.3516...  0.0933 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2177...  Training loss: 1.3426...  0.0936 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2178...  Training loss: 1.3388...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2179...  Training loss: 1.4862...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2180...  Training loss: 1.3719...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2181...  Training loss: 1.3465...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2182...  Training loss: 1.3766...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2183...  Training loss: 1.3296...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2184...  Training loss: 1.3188...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2185...  Training loss: 1.3569...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2186...  Training loss: 1.3399...  0.0924 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2187...  Training loss: 1.3518...  0.0927 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2188...  Training loss: 1.3495...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2189...  Training loss: 1.3235...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2190...  Training loss: 1.3379...  0.0927 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2191...  Training loss: 1.3463...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2192...  Training loss: 1.3718...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2193...  Training loss: 1.3236...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2194...  Training loss: 1.3172...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2195...  Training loss: 1.3652...  0.0938 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2196...  Training loss: 1.3709...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2197...  Training loss: 1.3454...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2198...  Training loss: 1.3688...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2199...  Training loss: 1.3428...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.3563...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2201...  Training loss: 1.3375...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2202...  Training loss: 1.3700...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2203...  Training loss: 1.3457...  0.0939 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2204...  Training loss: 1.2968...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2205...  Training loss: 1.3188...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2206...  Training loss: 1.3620...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2207...  Training loss: 1.3643...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2208...  Training loss: 1.3681...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2209...  Training loss: 1.3348...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2210...  Training loss: 1.3120...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2211...  Training loss: 1.3539...  0.0926 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2212...  Training loss: 1.3607...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2213...  Training loss: 1.3320...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2214...  Training loss: 1.3476...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2215...  Training loss: 1.3271...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2216...  Training loss: 1.3056...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2217...  Training loss: 1.3011...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2218...  Training loss: 1.3215...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2219...  Training loss: 1.3112...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2220...  Training loss: 1.3677...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2221...  Training loss: 1.3275...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2222...  Training loss: 1.3165...  0.0928 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2223...  Training loss: 1.3557...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2224...  Training loss: 1.3280...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2225...  Training loss: 1.3319...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2226...  Training loss: 1.3399...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2227...  Training loss: 1.3310...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2228...  Training loss: 1.3622...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2229...  Training loss: 1.3143...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2230...  Training loss: 1.3903...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2231...  Training loss: 1.3490...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2232...  Training loss: 1.3569...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2233...  Training loss: 1.3353...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2234...  Training loss: 1.3498...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2235...  Training loss: 1.3591...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2236...  Training loss: 1.3286...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2237...  Training loss: 1.3210...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2238...  Training loss: 1.3767...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2239...  Training loss: 1.3493...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2240...  Training loss: 1.3767...  0.0928 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2241...  Training loss: 1.3626...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2242...  Training loss: 1.3513...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2243...  Training loss: 1.3395...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2244...  Training loss: 1.3612...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2245...  Training loss: 1.3676...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2246...  Training loss: 1.3334...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2247...  Training loss: 1.3438...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2248...  Training loss: 1.3281...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2249...  Training loss: 1.3840...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.3552...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2251...  Training loss: 1.3780...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2252...  Training loss: 1.3237...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2253...  Training loss: 1.3408...  0.0927 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2254...  Training loss: 1.3605...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2255...  Training loss: 1.3394...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2256...  Training loss: 1.3158...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2257...  Training loss: 1.2863...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2258...  Training loss: 1.3589...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2259...  Training loss: 1.3005...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2260...  Training loss: 1.3472...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2261...  Training loss: 1.2961...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2262...  Training loss: 1.3337...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2263...  Training loss: 1.3195...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2264...  Training loss: 1.3392...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2265...  Training loss: 1.3170...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2266...  Training loss: 1.3102...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2267...  Training loss: 1.3051...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2268...  Training loss: 1.3443...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2269...  Training loss: 1.3177...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2270...  Training loss: 1.3219...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2271...  Training loss: 1.3154...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2272...  Training loss: 1.3071...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2273...  Training loss: 1.3139...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2274...  Training loss: 1.3433...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2275...  Training loss: 1.3418...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2276...  Training loss: 1.3104...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2277...  Training loss: 1.3192...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2278...  Training loss: 1.3074...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2279...  Training loss: 1.3441...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2280...  Training loss: 1.3326...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2281...  Training loss: 1.3314...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2282...  Training loss: 1.3270...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2283...  Training loss: 1.3383...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2284...  Training loss: 1.3319...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2285...  Training loss: 1.3423...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2286...  Training loss: 1.3402...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2287...  Training loss: 1.3168...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2288...  Training loss: 1.3499...  0.0926 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2289...  Training loss: 1.3233...  0.0927 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2290...  Training loss: 1.3498...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2291...  Training loss: 1.3429...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2292...  Training loss: 1.3240...  0.0926 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2293...  Training loss: 1.3035...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2294...  Training loss: 1.2886...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2295...  Training loss: 1.3425...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2296...  Training loss: 1.3394...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2297...  Training loss: 1.3240...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2298...  Training loss: 1.3305...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2299...  Training loss: 1.3364...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.2916...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2301...  Training loss: 1.2824...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2302...  Training loss: 1.3424...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2303...  Training loss: 1.3230...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2304...  Training loss: 1.2986...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2305...  Training loss: 1.3410...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2306...  Training loss: 1.3339...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2307...  Training loss: 1.3226...  0.0976 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2308...  Training loss: 1.2916...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2309...  Training loss: 1.2858...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2310...  Training loss: 1.3150...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2311...  Training loss: 1.3439...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2312...  Training loss: 1.3390...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2313...  Training loss: 1.3338...  0.0928 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2314...  Training loss: 1.3336...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2315...  Training loss: 1.3616...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2316...  Training loss: 1.3532...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2317...  Training loss: 1.3430...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2318...  Training loss: 1.3401...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2319...  Training loss: 1.3851...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2320...  Training loss: 1.3340...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2321...  Training loss: 1.3332...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2322...  Training loss: 1.3636...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2323...  Training loss: 1.3167...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2324...  Training loss: 1.3618...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2325...  Training loss: 1.3412...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2326...  Training loss: 1.3715...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2327...  Training loss: 1.3439...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2328...  Training loss: 1.3198...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2329...  Training loss: 1.2985...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2330...  Training loss: 1.3116...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2331...  Training loss: 1.3536...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2332...  Training loss: 1.3325...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2333...  Training loss: 1.3269...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2334...  Training loss: 1.3270...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2335...  Training loss: 1.3332...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2336...  Training loss: 1.3219...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2337...  Training loss: 1.2997...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2338...  Training loss: 1.3608...  0.0926 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2339...  Training loss: 1.3617...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2340...  Training loss: 1.3380...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2341...  Training loss: 1.3333...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2342...  Training loss: 1.3357...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2343...  Training loss: 1.3309...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2344...  Training loss: 1.3153...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2345...  Training loss: 1.3562...  0.0927 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2346...  Training loss: 1.4009...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2347...  Training loss: 1.3278...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2348...  Training loss: 1.3315...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2349...  Training loss: 1.3175...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.3105...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2351...  Training loss: 1.3502...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2352...  Training loss: 1.3349...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2353...  Training loss: 1.3368...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2354...  Training loss: 1.3051...  0.0930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2355...  Training loss: 1.3216...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2356...  Training loss: 1.3698...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2357...  Training loss: 1.3102...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2358...  Training loss: 1.2953...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2359...  Training loss: 1.3088...  0.0927 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2360...  Training loss: 1.3188...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2361...  Training loss: 1.3291...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2362...  Training loss: 1.3130...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2363...  Training loss: 1.3181...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2364...  Training loss: 1.3197...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2365...  Training loss: 1.3568...  0.0929 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2366...  Training loss: 1.3272...  0.0934 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2367...  Training loss: 1.3294...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2368...  Training loss: 1.3260...  0.0933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2369...  Training loss: 1.3028...  0.0938 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2370...  Training loss: 1.3069...  0.0937 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2371...  Training loss: 1.3229...  0.0932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2372...  Training loss: 1.2955...  0.0926 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2373...  Training loss: 1.2895...  0.0936 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2374...  Training loss: 1.3319...  0.0935 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2375...  Training loss: 1.3219...  0.0931 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2376...  Training loss: 1.3128...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2377...  Training loss: 1.4610...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2378...  Training loss: 1.3468...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2379...  Training loss: 1.3378...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2380...  Training loss: 1.3461...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2381...  Training loss: 1.3062...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2382...  Training loss: 1.2896...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2383...  Training loss: 1.3345...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2384...  Training loss: 1.3198...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2385...  Training loss: 1.3349...  0.0942 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2386...  Training loss: 1.3230...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2387...  Training loss: 1.3143...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2388...  Training loss: 1.3247...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2389...  Training loss: 1.3326...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2390...  Training loss: 1.3389...  0.0927 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2391...  Training loss: 1.3237...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2392...  Training loss: 1.3008...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2393...  Training loss: 1.3409...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2394...  Training loss: 1.3482...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2395...  Training loss: 1.3211...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2396...  Training loss: 1.3524...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2397...  Training loss: 1.3214...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2398...  Training loss: 1.3502...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2399...  Training loss: 1.3142...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.3349...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2401...  Training loss: 1.3254...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2402...  Training loss: 1.2888...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2403...  Training loss: 1.2989...  0.0938 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2404...  Training loss: 1.3426...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2405...  Training loss: 1.3440...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2406...  Training loss: 1.3494...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2407...  Training loss: 1.3096...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2408...  Training loss: 1.2983...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2409...  Training loss: 1.3346...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2410...  Training loss: 1.3265...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2411...  Training loss: 1.3153...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2412...  Training loss: 1.3254...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2413...  Training loss: 1.2950...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2414...  Training loss: 1.2820...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2415...  Training loss: 1.2855...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2416...  Training loss: 1.3029...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2417...  Training loss: 1.2890...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2418...  Training loss: 1.3609...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2419...  Training loss: 1.3122...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2420...  Training loss: 1.2968...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2421...  Training loss: 1.3207...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2422...  Training loss: 1.2976...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2423...  Training loss: 1.3064...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2424...  Training loss: 1.3187...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2425...  Training loss: 1.3137...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2426...  Training loss: 1.3268...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2427...  Training loss: 1.2972...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2428...  Training loss: 1.3649...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2429...  Training loss: 1.3263...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2430...  Training loss: 1.3314...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2431...  Training loss: 1.3198...  0.0928 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2432...  Training loss: 1.3248...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2433...  Training loss: 1.3385...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2434...  Training loss: 1.3173...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2435...  Training loss: 1.3008...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2436...  Training loss: 1.3537...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2437...  Training loss: 1.3276...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2438...  Training loss: 1.3630...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2439...  Training loss: 1.3482...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2440...  Training loss: 1.3329...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2441...  Training loss: 1.3176...  0.0928 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2442...  Training loss: 1.3248...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2443...  Training loss: 1.3390...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2444...  Training loss: 1.3020...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2445...  Training loss: 1.3220...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2446...  Training loss: 1.3051...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2447...  Training loss: 1.3591...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2448...  Training loss: 1.3262...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2449...  Training loss: 1.3581...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.3061...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2451...  Training loss: 1.3165...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2452...  Training loss: 1.3324...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2453...  Training loss: 1.3154...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2454...  Training loss: 1.3075...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2455...  Training loss: 1.2735...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2456...  Training loss: 1.3228...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2457...  Training loss: 1.2811...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2458...  Training loss: 1.3127...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2459...  Training loss: 1.2934...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2460...  Training loss: 1.3221...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2461...  Training loss: 1.2993...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2462...  Training loss: 1.3177...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2463...  Training loss: 1.2985...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2464...  Training loss: 1.3036...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2465...  Training loss: 1.2794...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2466...  Training loss: 1.3261...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2467...  Training loss: 1.2944...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2468...  Training loss: 1.3065...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2469...  Training loss: 1.2989...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2470...  Training loss: 1.2841...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2471...  Training loss: 1.2985...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2472...  Training loss: 1.3315...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2473...  Training loss: 1.3119...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2474...  Training loss: 1.2857...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2475...  Training loss: 1.3020...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2476...  Training loss: 1.2839...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2477...  Training loss: 1.3201...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2478...  Training loss: 1.3098...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2479...  Training loss: 1.3161...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2480...  Training loss: 1.3023...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2481...  Training loss: 1.3036...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2482...  Training loss: 1.3105...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2483...  Training loss: 1.3248...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2484...  Training loss: 1.3182...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2485...  Training loss: 1.3007...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2486...  Training loss: 1.3260...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2487...  Training loss: 1.3026...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2488...  Training loss: 1.3277...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2489...  Training loss: 1.3123...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2490...  Training loss: 1.3066...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2491...  Training loss: 1.2913...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2492...  Training loss: 1.2789...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2493...  Training loss: 1.3130...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2494...  Training loss: 1.3238...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2495...  Training loss: 1.3118...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2496...  Training loss: 1.3114...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2497...  Training loss: 1.3109...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2498...  Training loss: 1.2832...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2499...  Training loss: 1.2645...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.3176...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2501...  Training loss: 1.3087...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2502...  Training loss: 1.2708...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2503...  Training loss: 1.3139...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2504...  Training loss: 1.3128...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2505...  Training loss: 1.3007...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2506...  Training loss: 1.2709...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2507...  Training loss: 1.2699...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2508...  Training loss: 1.2983...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2509...  Training loss: 1.3371...  0.0927 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2510...  Training loss: 1.3218...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2511...  Training loss: 1.3210...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2512...  Training loss: 1.3147...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2513...  Training loss: 1.3384...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2514...  Training loss: 1.3351...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2515...  Training loss: 1.3193...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2516...  Training loss: 1.3250...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2517...  Training loss: 1.3616...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2518...  Training loss: 1.3268...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2519...  Training loss: 1.3087...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2520...  Training loss: 1.3441...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2521...  Training loss: 1.2908...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2522...  Training loss: 1.3323...  0.0927 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2523...  Training loss: 1.3287...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2524...  Training loss: 1.3521...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2525...  Training loss: 1.3362...  0.0926 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2526...  Training loss: 1.3072...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2527...  Training loss: 1.2780...  0.0928 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2528...  Training loss: 1.2926...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2529...  Training loss: 1.3238...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2530...  Training loss: 1.3109...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2531...  Training loss: 1.3109...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2532...  Training loss: 1.3131...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2533...  Training loss: 1.3147...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2534...  Training loss: 1.3050...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2535...  Training loss: 1.2702...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2536...  Training loss: 1.3373...  0.0926 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2537...  Training loss: 1.3468...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2538...  Training loss: 1.3158...  0.0926 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2539...  Training loss: 1.3105...  0.0928 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2540...  Training loss: 1.3166...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2541...  Training loss: 1.3189...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2542...  Training loss: 1.3073...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2543...  Training loss: 1.3337...  0.0928 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2544...  Training loss: 1.3788...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2545...  Training loss: 1.3218...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2546...  Training loss: 1.3208...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2547...  Training loss: 1.2982...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2548...  Training loss: 1.2946...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2549...  Training loss: 1.3364...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.3131...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2551...  Training loss: 1.3174...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2552...  Training loss: 1.2773...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2553...  Training loss: 1.2957...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2554...  Training loss: 1.3411...  0.0933 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2555...  Training loss: 1.2930...  0.0927 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2556...  Training loss: 1.2676...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2557...  Training loss: 1.3029...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2558...  Training loss: 1.2939...  0.0934 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2559...  Training loss: 1.3069...  0.0929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2560...  Training loss: 1.3061...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2561...  Training loss: 1.3029...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2562...  Training loss: 1.2905...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2563...  Training loss: 1.3424...  0.0936 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2564...  Training loss: 1.3029...  0.0928 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2565...  Training loss: 1.3069...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2566...  Training loss: 1.2987...  0.0937 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2567...  Training loss: 1.2906...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2568...  Training loss: 1.2910...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2569...  Training loss: 1.3093...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2570...  Training loss: 1.2834...  0.0931 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2571...  Training loss: 1.2690...  0.0932 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2572...  Training loss: 1.3101...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2573...  Training loss: 1.2958...  0.0935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2574...  Training loss: 1.2963...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2575...  Training loss: 1.4474...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2576...  Training loss: 1.3376...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2577...  Training loss: 1.3069...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2578...  Training loss: 1.3224...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2579...  Training loss: 1.2966...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2580...  Training loss: 1.2782...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2581...  Training loss: 1.3073...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2582...  Training loss: 1.2996...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2583...  Training loss: 1.3190...  0.0926 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2584...  Training loss: 1.3066...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2585...  Training loss: 1.2930...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2586...  Training loss: 1.3072...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2587...  Training loss: 1.3062...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2588...  Training loss: 1.3198...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2589...  Training loss: 1.2960...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2590...  Training loss: 1.2907...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2591...  Training loss: 1.3227...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2592...  Training loss: 1.3299...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2593...  Training loss: 1.3148...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2594...  Training loss: 1.3259...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2595...  Training loss: 1.3114...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2596...  Training loss: 1.3123...  0.0921 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2597...  Training loss: 1.3042...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2598...  Training loss: 1.3226...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2599...  Training loss: 1.3058...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.2701...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2601...  Training loss: 1.2757...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2602...  Training loss: 1.3118...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2603...  Training loss: 1.3162...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2604...  Training loss: 1.3304...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2605...  Training loss: 1.2939...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2606...  Training loss: 1.2775...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2607...  Training loss: 1.3141...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2608...  Training loss: 1.3150...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2609...  Training loss: 1.2918...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2610...  Training loss: 1.3103...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2611...  Training loss: 1.2802...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2612...  Training loss: 1.2693...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2613...  Training loss: 1.2598...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2614...  Training loss: 1.2963...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2615...  Training loss: 1.2711...  0.0928 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2616...  Training loss: 1.3496...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2617...  Training loss: 1.2941...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2618...  Training loss: 1.2736...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2619...  Training loss: 1.3068...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2620...  Training loss: 1.2778...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2621...  Training loss: 1.2928...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2622...  Training loss: 1.2953...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2623...  Training loss: 1.3002...  0.0928 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2624...  Training loss: 1.3157...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2625...  Training loss: 1.2735...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2626...  Training loss: 1.3477...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2627...  Training loss: 1.3112...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2628...  Training loss: 1.3149...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2629...  Training loss: 1.2931...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2630...  Training loss: 1.3082...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2631...  Training loss: 1.3136...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2632...  Training loss: 1.2963...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2633...  Training loss: 1.2842...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2634...  Training loss: 1.3358...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2635...  Training loss: 1.3100...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2636...  Training loss: 1.3437...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2637...  Training loss: 1.3336...  0.0938 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2638...  Training loss: 1.3072...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2639...  Training loss: 1.3086...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2640...  Training loss: 1.3147...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2641...  Training loss: 1.3248...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2642...  Training loss: 1.2877...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2643...  Training loss: 1.3022...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2644...  Training loss: 1.2869...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2645...  Training loss: 1.3532...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2646...  Training loss: 1.3305...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2647...  Training loss: 1.3406...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2648...  Training loss: 1.2855...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2649...  Training loss: 1.2935...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.3324...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2651...  Training loss: 1.3023...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2652...  Training loss: 1.2819...  0.0928 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2653...  Training loss: 1.2598...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2654...  Training loss: 1.3017...  0.0938 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2655...  Training loss: 1.2624...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2656...  Training loss: 1.3064...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2657...  Training loss: 1.2679...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2658...  Training loss: 1.2884...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2659...  Training loss: 1.2803...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2660...  Training loss: 1.2975...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2661...  Training loss: 1.2790...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2662...  Training loss: 1.2799...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2663...  Training loss: 1.2694...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2664...  Training loss: 1.3071...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2665...  Training loss: 1.2796...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2666...  Training loss: 1.2956...  0.0938 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2667...  Training loss: 1.2776...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2668...  Training loss: 1.2706...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2669...  Training loss: 1.2740...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2670...  Training loss: 1.3133...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2671...  Training loss: 1.3139...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2672...  Training loss: 1.2709...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2673...  Training loss: 1.2880...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2674...  Training loss: 1.2740...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2675...  Training loss: 1.3164...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2676...  Training loss: 1.2912...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2677...  Training loss: 1.2960...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2678...  Training loss: 1.3015...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2679...  Training loss: 1.2885...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2680...  Training loss: 1.2918...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2681...  Training loss: 1.3164...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2682...  Training loss: 1.3041...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2683...  Training loss: 1.2878...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2684...  Training loss: 1.3119...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2685...  Training loss: 1.2849...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2686...  Training loss: 1.3099...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2687...  Training loss: 1.2993...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2688...  Training loss: 1.2953...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2689...  Training loss: 1.2750...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2690...  Training loss: 1.2603...  0.0928 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2691...  Training loss: 1.2981...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2692...  Training loss: 1.3049...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2693...  Training loss: 1.2808...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2694...  Training loss: 1.2903...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2695...  Training loss: 1.3076...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2696...  Training loss: 1.2731...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2697...  Training loss: 1.2585...  0.0925 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2698...  Training loss: 1.3043...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2699...  Training loss: 1.2809...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.2505...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2701...  Training loss: 1.3007...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2702...  Training loss: 1.3033...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2703...  Training loss: 1.2748...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2704...  Training loss: 1.2606...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2705...  Training loss: 1.2413...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2706...  Training loss: 1.2765...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2707...  Training loss: 1.3115...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2708...  Training loss: 1.3047...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2709...  Training loss: 1.2977...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2710...  Training loss: 1.3013...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2711...  Training loss: 1.3273...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2712...  Training loss: 1.3095...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2713...  Training loss: 1.3005...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2714...  Training loss: 1.3013...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2715...  Training loss: 1.3455...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2716...  Training loss: 1.3111...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2717...  Training loss: 1.2873...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2718...  Training loss: 1.3202...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2719...  Training loss: 1.2719...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2720...  Training loss: 1.3240...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2721...  Training loss: 1.3046...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2722...  Training loss: 1.3324...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2723...  Training loss: 1.3249...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2724...  Training loss: 1.2765...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2725...  Training loss: 1.2607...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2726...  Training loss: 1.2791...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2727...  Training loss: 1.3134...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2728...  Training loss: 1.2796...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2729...  Training loss: 1.3035...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2730...  Training loss: 1.2830...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2731...  Training loss: 1.2976...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2732...  Training loss: 1.2881...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2733...  Training loss: 1.2708...  0.0928 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2734...  Training loss: 1.3191...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2735...  Training loss: 1.3211...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2736...  Training loss: 1.3019...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2737...  Training loss: 1.2989...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2738...  Training loss: 1.2996...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2739...  Training loss: 1.3043...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2740...  Training loss: 1.2879...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2741...  Training loss: 1.3249...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2742...  Training loss: 1.3640...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2743...  Training loss: 1.3086...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2744...  Training loss: 1.3011...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2745...  Training loss: 1.2866...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2746...  Training loss: 1.2789...  0.0933 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2747...  Training loss: 1.3212...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2748...  Training loss: 1.2956...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2749...  Training loss: 1.2992...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.2618...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2751...  Training loss: 1.2813...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2752...  Training loss: 1.3280...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2753...  Training loss: 1.2740...  0.0937 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2754...  Training loss: 1.2550...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2755...  Training loss: 1.2757...  0.0931 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2756...  Training loss: 1.2863...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2757...  Training loss: 1.2859...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2758...  Training loss: 1.2822...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2759...  Training loss: 1.2878...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2760...  Training loss: 1.2736...  0.0928 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2761...  Training loss: 1.3163...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2762...  Training loss: 1.2901...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2763...  Training loss: 1.2850...  0.0934 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2764...  Training loss: 1.2906...  0.0938 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2765...  Training loss: 1.2652...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2766...  Training loss: 1.2729...  0.0936 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2767...  Training loss: 1.2882...  0.0930 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2768...  Training loss: 1.2703...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2769...  Training loss: 1.2603...  0.0929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2770...  Training loss: 1.2966...  0.0932 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2771...  Training loss: 1.2741...  0.0935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2772...  Training loss: 1.2769...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2773...  Training loss: 1.4116...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2774...  Training loss: 1.3218...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2775...  Training loss: 1.2997...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2776...  Training loss: 1.3182...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2777...  Training loss: 1.2727...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2778...  Training loss: 1.2614...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2779...  Training loss: 1.2938...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2780...  Training loss: 1.2828...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2781...  Training loss: 1.2946...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2782...  Training loss: 1.2884...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2783...  Training loss: 1.2671...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2784...  Training loss: 1.2878...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2785...  Training loss: 1.2969...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2786...  Training loss: 1.2951...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2787...  Training loss: 1.2817...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2788...  Training loss: 1.2661...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2789...  Training loss: 1.3038...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2790...  Training loss: 1.3085...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2791...  Training loss: 1.2868...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2792...  Training loss: 1.3221...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2793...  Training loss: 1.2884...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2794...  Training loss: 1.3096...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2795...  Training loss: 1.2801...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2796...  Training loss: 1.3094...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2797...  Training loss: 1.2898...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2798...  Training loss: 1.2496...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2799...  Training loss: 1.2650...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.2994...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2801...  Training loss: 1.3034...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2802...  Training loss: 1.3078...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2803...  Training loss: 1.2705...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2804...  Training loss: 1.2588...  0.0937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2805...  Training loss: 1.2838...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2806...  Training loss: 1.3004...  0.0928 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2807...  Training loss: 1.2639...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2808...  Training loss: 1.2945...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2809...  Training loss: 1.2625...  0.0928 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2810...  Training loss: 1.2517...  0.0937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2811...  Training loss: 1.2462...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2812...  Training loss: 1.2719...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2813...  Training loss: 1.2581...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2814...  Training loss: 1.3285...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2815...  Training loss: 1.2769...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2816...  Training loss: 1.2526...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2817...  Training loss: 1.2944...  0.0928 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2818...  Training loss: 1.2623...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2819...  Training loss: 1.2702...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2820...  Training loss: 1.2765...  0.0937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2821...  Training loss: 1.2916...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2822...  Training loss: 1.3051...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2823...  Training loss: 1.2680...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2824...  Training loss: 1.3183...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2825...  Training loss: 1.2947...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2826...  Training loss: 1.2983...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2827...  Training loss: 1.2846...  0.0928 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2828...  Training loss: 1.2874...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2829...  Training loss: 1.3102...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2830...  Training loss: 1.2832...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2831...  Training loss: 1.2729...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2832...  Training loss: 1.3171...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2833...  Training loss: 1.2894...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2834...  Training loss: 1.3210...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2835...  Training loss: 1.3095...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2836...  Training loss: 1.2957...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2837...  Training loss: 1.2887...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2838...  Training loss: 1.2980...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2839...  Training loss: 1.3047...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2840...  Training loss: 1.2761...  0.0928 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2841...  Training loss: 1.2935...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2842...  Training loss: 1.2730...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2843...  Training loss: 1.3283...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2844...  Training loss: 1.2984...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2845...  Training loss: 1.3250...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2846...  Training loss: 1.2732...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2847...  Training loss: 1.2814...  0.0937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2848...  Training loss: 1.3109...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2849...  Training loss: 1.2787...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.2815...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2851...  Training loss: 1.2415...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2852...  Training loss: 1.2951...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2853...  Training loss: 1.2604...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2854...  Training loss: 1.2912...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2855...  Training loss: 1.2591...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2856...  Training loss: 1.2725...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2857...  Training loss: 1.2603...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2858...  Training loss: 1.2864...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2859...  Training loss: 1.2601...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2860...  Training loss: 1.2692...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2861...  Training loss: 1.2526...  0.0927 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2862...  Training loss: 1.2864...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2863...  Training loss: 1.2578...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2864...  Training loss: 1.2704...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2865...  Training loss: 1.2541...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2866...  Training loss: 1.2578...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2867...  Training loss: 1.2617...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2868...  Training loss: 1.2908...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2869...  Training loss: 1.2851...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2870...  Training loss: 1.2503...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2871...  Training loss: 1.2710...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2872...  Training loss: 1.2572...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2873...  Training loss: 1.2811...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2874...  Training loss: 1.2758...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2875...  Training loss: 1.2899...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2876...  Training loss: 1.2721...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2877...  Training loss: 1.2824...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2878...  Training loss: 1.2714...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2879...  Training loss: 1.2856...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2880...  Training loss: 1.2831...  0.0928 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2881...  Training loss: 1.2627...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2882...  Training loss: 1.2942...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2883...  Training loss: 1.2655...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2884...  Training loss: 1.2997...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2885...  Training loss: 1.2876...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2886...  Training loss: 1.2701...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2887...  Training loss: 1.2539...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2888...  Training loss: 1.2485...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2889...  Training loss: 1.2835...  0.0926 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2890...  Training loss: 1.2867...  0.0937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2891...  Training loss: 1.2797...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2892...  Training loss: 1.2774...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2893...  Training loss: 1.2773...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2894...  Training loss: 1.2389...  0.0938 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2895...  Training loss: 1.2393...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2896...  Training loss: 1.2869...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2897...  Training loss: 1.2817...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2898...  Training loss: 1.2399...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2899...  Training loss: 1.2892...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.2833...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2901...  Training loss: 1.2657...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2902...  Training loss: 1.2443...  0.0937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2903...  Training loss: 1.2338...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2904...  Training loss: 1.2613...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2905...  Training loss: 1.2916...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2906...  Training loss: 1.2870...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2907...  Training loss: 1.2883...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2908...  Training loss: 1.2772...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2909...  Training loss: 1.3056...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2910...  Training loss: 1.3000...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2911...  Training loss: 1.2847...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2912...  Training loss: 1.2882...  0.0927 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2913...  Training loss: 1.3378...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2914...  Training loss: 1.3011...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2915...  Training loss: 1.2764...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2916...  Training loss: 1.3202...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2917...  Training loss: 1.2670...  0.0938 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2918...  Training loss: 1.3117...  0.0937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2919...  Training loss: 1.2894...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2920...  Training loss: 1.3188...  0.0925 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2921...  Training loss: 1.3017...  0.0938 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2922...  Training loss: 1.2697...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2923...  Training loss: 1.2532...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2924...  Training loss: 1.2588...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2925...  Training loss: 1.2955...  0.0938 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2926...  Training loss: 1.2696...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2927...  Training loss: 1.2779...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2928...  Training loss: 1.2760...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2929...  Training loss: 1.2860...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2930...  Training loss: 1.2683...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2931...  Training loss: 1.2459...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2932...  Training loss: 1.3025...  0.0937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2933...  Training loss: 1.3074...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2934...  Training loss: 1.2863...  0.0929 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2935...  Training loss: 1.2846...  0.0928 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2936...  Training loss: 1.2797...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2937...  Training loss: 1.2839...  0.0937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2938...  Training loss: 1.2721...  0.0937 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2939...  Training loss: 1.3045...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2940...  Training loss: 1.3490...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2941...  Training loss: 1.2895...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2942...  Training loss: 1.2825...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2943...  Training loss: 1.2744...  0.0938 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2944...  Training loss: 1.2692...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2945...  Training loss: 1.3072...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2946...  Training loss: 1.2770...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2947...  Training loss: 1.2937...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2948...  Training loss: 1.2434...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2949...  Training loss: 1.2750...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.3127...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2951...  Training loss: 1.2607...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2952...  Training loss: 1.2341...  0.0928 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2953...  Training loss: 1.2606...  0.0936 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2954...  Training loss: 1.2724...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2955...  Training loss: 1.2755...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2956...  Training loss: 1.2537...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2957...  Training loss: 1.2710...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2958...  Training loss: 1.2476...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2959...  Training loss: 1.3011...  0.0938 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2960...  Training loss: 1.2758...  0.0930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2961...  Training loss: 1.2752...  0.0932 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2962...  Training loss: 1.2702...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2963...  Training loss: 1.2506...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2964...  Training loss: 1.2514...  0.0938 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2965...  Training loss: 1.2828...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2966...  Training loss: 1.2521...  0.0934 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2967...  Training loss: 1.2438...  0.0935 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2968...  Training loss: 1.2817...  0.0933 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2969...  Training loss: 1.2684...  0.0931 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2970...  Training loss: 1.2586...  0.0937 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2971...  Training loss: 1.4047...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2972...  Training loss: 1.2938...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2973...  Training loss: 1.2773...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2974...  Training loss: 1.3073...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2975...  Training loss: 1.2549...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2976...  Training loss: 1.2365...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2977...  Training loss: 1.2748...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2978...  Training loss: 1.2755...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2979...  Training loss: 1.2885...  0.0938 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2980...  Training loss: 1.2750...  0.0937 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2981...  Training loss: 1.2577...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2982...  Training loss: 1.2643...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2983...  Training loss: 1.2839...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2984...  Training loss: 1.2858...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2985...  Training loss: 1.2592...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2986...  Training loss: 1.2539...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2987...  Training loss: 1.2926...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2988...  Training loss: 1.2897...  0.0927 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2989...  Training loss: 1.2745...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2990...  Training loss: 1.3052...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2991...  Training loss: 1.2799...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2992...  Training loss: 1.2944...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2993...  Training loss: 1.2655...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2994...  Training loss: 1.2870...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2995...  Training loss: 1.2770...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2996...  Training loss: 1.2346...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2997...  Training loss: 1.2542...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2998...  Training loss: 1.2876...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2999...  Training loss: 1.2871...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.3000...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3001...  Training loss: 1.2559...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3002...  Training loss: 1.2404...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3003...  Training loss: 1.2792...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3004...  Training loss: 1.2888...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3005...  Training loss: 1.2607...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3006...  Training loss: 1.2846...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3007...  Training loss: 1.2414...  0.0939 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3008...  Training loss: 1.2383...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3009...  Training loss: 1.2278...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3010...  Training loss: 1.2588...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3011...  Training loss: 1.2428...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3012...  Training loss: 1.3178...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3013...  Training loss: 1.2664...  0.0927 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3014...  Training loss: 1.2434...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3015...  Training loss: 1.2823...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3016...  Training loss: 1.2464...  0.0938 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3017...  Training loss: 1.2621...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3018...  Training loss: 1.2665...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3019...  Training loss: 1.2604...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3020...  Training loss: 1.2842...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3021...  Training loss: 1.2549...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3022...  Training loss: 1.3100...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3023...  Training loss: 1.2747...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3024...  Training loss: 1.2779...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3025...  Training loss: 1.2653...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3026...  Training loss: 1.2718...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3027...  Training loss: 1.2800...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3028...  Training loss: 1.2705...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3029...  Training loss: 1.2516...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3030...  Training loss: 1.3095...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3031...  Training loss: 1.2777...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3032...  Training loss: 1.3147...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3033...  Training loss: 1.2902...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3034...  Training loss: 1.2767...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3035...  Training loss: 1.2703...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3036...  Training loss: 1.2794...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3037...  Training loss: 1.2959...  0.0928 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3038...  Training loss: 1.2554...  0.0938 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3039...  Training loss: 1.2790...  0.0937 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3040...  Training loss: 1.2621...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3041...  Training loss: 1.3153...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3042...  Training loss: 1.2906...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3043...  Training loss: 1.3042...  0.0937 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3044...  Training loss: 1.2547...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3045...  Training loss: 1.2727...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3046...  Training loss: 1.2944...  0.0928 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3047...  Training loss: 1.2726...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3048...  Training loss: 1.2592...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3049...  Training loss: 1.2307...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.2828...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3051...  Training loss: 1.2341...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3052...  Training loss: 1.2666...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3053...  Training loss: 1.2467...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3054...  Training loss: 1.2594...  0.0925 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3055...  Training loss: 1.2502...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3056...  Training loss: 1.2693...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3057...  Training loss: 1.2482...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3058...  Training loss: 1.2548...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3059...  Training loss: 1.2284...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3060...  Training loss: 1.2765...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3061...  Training loss: 1.2468...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3062...  Training loss: 1.2533...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3063...  Training loss: 1.2318...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3064...  Training loss: 1.2390...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3065...  Training loss: 1.2509...  0.0928 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3066...  Training loss: 1.2775...  0.0937 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3067...  Training loss: 1.2850...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3068...  Training loss: 1.2376...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3069...  Training loss: 1.2509...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3070...  Training loss: 1.2543...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3071...  Training loss: 1.2880...  0.0938 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3072...  Training loss: 1.2551...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3073...  Training loss: 1.2595...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3074...  Training loss: 1.2592...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3075...  Training loss: 1.2689...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3076...  Training loss: 1.2586...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3077...  Training loss: 1.2682...  0.0928 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3078...  Training loss: 1.2818...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3079...  Training loss: 1.2591...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3080...  Training loss: 1.2839...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3081...  Training loss: 1.2574...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3082...  Training loss: 1.2780...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3083...  Training loss: 1.2718...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3084...  Training loss: 1.2605...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3085...  Training loss: 1.2419...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3086...  Training loss: 1.2258...  0.0928 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3087...  Training loss: 1.2771...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3088...  Training loss: 1.2740...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3089...  Training loss: 1.2617...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3090...  Training loss: 1.2714...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3091...  Training loss: 1.2667...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3092...  Training loss: 1.2368...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3093...  Training loss: 1.2208...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3094...  Training loss: 1.2659...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3095...  Training loss: 1.2571...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3096...  Training loss: 1.2231...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3097...  Training loss: 1.2810...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3098...  Training loss: 1.2645...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3099...  Training loss: 1.2497...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.2289...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3101...  Training loss: 1.2202...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3102...  Training loss: 1.2469...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3103...  Training loss: 1.2889...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3104...  Training loss: 1.2691...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3105...  Training loss: 1.2704...  0.0938 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3106...  Training loss: 1.2774...  0.0928 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3107...  Training loss: 1.2993...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3108...  Training loss: 1.2730...  0.0937 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3109...  Training loss: 1.2671...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3110...  Training loss: 1.2762...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3111...  Training loss: 1.3174...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3112...  Training loss: 1.2729...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3113...  Training loss: 1.2656...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3114...  Training loss: 1.3063...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3115...  Training loss: 1.2479...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3116...  Training loss: 1.2993...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3117...  Training loss: 1.2716...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3118...  Training loss: 1.3018...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3119...  Training loss: 1.3045...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3120...  Training loss: 1.2644...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3121...  Training loss: 1.2380...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3122...  Training loss: 1.2491...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3123...  Training loss: 1.2779...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3124...  Training loss: 1.2501...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3125...  Training loss: 1.2575...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3126...  Training loss: 1.2600...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3127...  Training loss: 1.2695...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3128...  Training loss: 1.2589...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3129...  Training loss: 1.2345...  0.0938 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3130...  Training loss: 1.2863...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3131...  Training loss: 1.2890...  0.0928 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3132...  Training loss: 1.2795...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3133...  Training loss: 1.2594...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3134...  Training loss: 1.2719...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3135...  Training loss: 1.2639...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3136...  Training loss: 1.2580...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3137...  Training loss: 1.2860...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3138...  Training loss: 1.3310...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3139...  Training loss: 1.2718...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3140...  Training loss: 1.2647...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3141...  Training loss: 1.2474...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3142...  Training loss: 1.2539...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3143...  Training loss: 1.2941...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3144...  Training loss: 1.2706...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3145...  Training loss: 1.2800...  0.0937 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3146...  Training loss: 1.2407...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3147...  Training loss: 1.2620...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3148...  Training loss: 1.3027...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3149...  Training loss: 1.2508...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.2363...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3151...  Training loss: 1.2546...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3152...  Training loss: 1.2591...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3153...  Training loss: 1.2647...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3154...  Training loss: 1.2476...  0.0937 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3155...  Training loss: 1.2553...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3156...  Training loss: 1.2392...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3157...  Training loss: 1.2993...  0.0933 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3158...  Training loss: 1.2558...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3159...  Training loss: 1.2606...  0.0932 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3160...  Training loss: 1.2616...  0.0935 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3161...  Training loss: 1.2415...  0.0931 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3162...  Training loss: 1.2415...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3163...  Training loss: 1.2628...  0.0934 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3164...  Training loss: 1.2487...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3165...  Training loss: 1.2310...  0.0930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3166...  Training loss: 1.2698...  0.0929 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3167...  Training loss: 1.2550...  0.0936 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3168...  Training loss: 1.2495...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3169...  Training loss: 1.3845...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3170...  Training loss: 1.2748...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3171...  Training loss: 1.2603...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3172...  Training loss: 1.2886...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3173...  Training loss: 1.2437...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3174...  Training loss: 1.2312...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3175...  Training loss: 1.2570...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3176...  Training loss: 1.2540...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3177...  Training loss: 1.2686...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3178...  Training loss: 1.2522...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3179...  Training loss: 1.2473...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3180...  Training loss: 1.2573...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3181...  Training loss: 1.2655...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3182...  Training loss: 1.2734...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3183...  Training loss: 1.2385...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3184...  Training loss: 1.2351...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3185...  Training loss: 1.2827...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3186...  Training loss: 1.2871...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3187...  Training loss: 1.2662...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3188...  Training loss: 1.2821...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3189...  Training loss: 1.2659...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3190...  Training loss: 1.2784...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3191...  Training loss: 1.2591...  0.0938 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3192...  Training loss: 1.2784...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3193...  Training loss: 1.2650...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3194...  Training loss: 1.2257...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3195...  Training loss: 1.2358...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3196...  Training loss: 1.2733...  0.0938 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3197...  Training loss: 1.2711...  0.0926 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3198...  Training loss: 1.2896...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3199...  Training loss: 1.2506...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.2360...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3201...  Training loss: 1.2693...  0.0938 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3202...  Training loss: 1.2697...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3203...  Training loss: 1.2500...  0.0938 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3204...  Training loss: 1.2660...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3205...  Training loss: 1.2348...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3206...  Training loss: 1.2247...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3207...  Training loss: 1.2196...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3208...  Training loss: 1.2410...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3209...  Training loss: 1.2389...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3210...  Training loss: 1.2987...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3211...  Training loss: 1.2505...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3212...  Training loss: 1.2326...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3213...  Training loss: 1.2658...  0.0926 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3214...  Training loss: 1.2381...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3215...  Training loss: 1.2580...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3216...  Training loss: 1.2590...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3217...  Training loss: 1.2641...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3218...  Training loss: 1.2765...  0.0928 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3219...  Training loss: 1.2402...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3220...  Training loss: 1.3074...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3221...  Training loss: 1.2722...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3222...  Training loss: 1.2776...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3223...  Training loss: 1.2516...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3224...  Training loss: 1.2550...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3225...  Training loss: 1.2789...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3226...  Training loss: 1.2459...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3227...  Training loss: 1.2456...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3228...  Training loss: 1.3029...  0.0925 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3229...  Training loss: 1.2769...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3230...  Training loss: 1.3061...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3231...  Training loss: 1.2931...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3232...  Training loss: 1.2655...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3233...  Training loss: 1.2591...  0.0925 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3234...  Training loss: 1.2721...  0.0928 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3235...  Training loss: 1.2858...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3236...  Training loss: 1.2458...  0.0938 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3237...  Training loss: 1.2644...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3238...  Training loss: 1.2444...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3239...  Training loss: 1.3065...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3240...  Training loss: 1.2701...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3241...  Training loss: 1.2919...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3242...  Training loss: 1.2388...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3243...  Training loss: 1.2636...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3244...  Training loss: 1.2736...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3245...  Training loss: 1.2546...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3246...  Training loss: 1.2525...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3247...  Training loss: 1.2167...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3248...  Training loss: 1.2641...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3249...  Training loss: 1.2264...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.2576...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3251...  Training loss: 1.2386...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3252...  Training loss: 1.2546...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3253...  Training loss: 1.2264...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3254...  Training loss: 1.2618...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3255...  Training loss: 1.2431...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3256...  Training loss: 1.2467...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3257...  Training loss: 1.2283...  0.0927 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3258...  Training loss: 1.2613...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3259...  Training loss: 1.2332...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3260...  Training loss: 1.2391...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3261...  Training loss: 1.2309...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3262...  Training loss: 1.2311...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3263...  Training loss: 1.2406...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3264...  Training loss: 1.2709...  0.0938 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3265...  Training loss: 1.2712...  0.0938 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3266...  Training loss: 1.2245...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3267...  Training loss: 1.2478...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3268...  Training loss: 1.2334...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3269...  Training loss: 1.2614...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3270...  Training loss: 1.2456...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3271...  Training loss: 1.2576...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3272...  Training loss: 1.2488...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3273...  Training loss: 1.2470...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3274...  Training loss: 1.2495...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3275...  Training loss: 1.2571...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3276...  Training loss: 1.2650...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3277...  Training loss: 1.2413...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3278...  Training loss: 1.2653...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3279...  Training loss: 1.2449...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3280...  Training loss: 1.2719...  0.0927 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3281...  Training loss: 1.2608...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3282...  Training loss: 1.2424...  0.0926 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3283...  Training loss: 1.2292...  0.0928 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3284...  Training loss: 1.2229...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3285...  Training loss: 1.2597...  0.0928 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3286...  Training loss: 1.2636...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3287...  Training loss: 1.2583...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3288...  Training loss: 1.2509...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3289...  Training loss: 1.2602...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3290...  Training loss: 1.2141...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3291...  Training loss: 1.2093...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3292...  Training loss: 1.2515...  0.0928 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3293...  Training loss: 1.2445...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3294...  Training loss: 1.2073...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3295...  Training loss: 1.2619...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3296...  Training loss: 1.2503...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3297...  Training loss: 1.2485...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3298...  Training loss: 1.2188...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3299...  Training loss: 1.2157...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.2424...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3301...  Training loss: 1.2791...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3302...  Training loss: 1.2597...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3303...  Training loss: 1.2529...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3304...  Training loss: 1.2578...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3305...  Training loss: 1.2905...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3306...  Training loss: 1.2697...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3307...  Training loss: 1.2582...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3308...  Training loss: 1.2642...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3309...  Training loss: 1.3040...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3310...  Training loss: 1.2704...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3311...  Training loss: 1.2507...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3312...  Training loss: 1.2995...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3313...  Training loss: 1.2447...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3314...  Training loss: 1.2835...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3315...  Training loss: 1.2728...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3316...  Training loss: 1.2901...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3317...  Training loss: 1.2676...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3318...  Training loss: 1.2547...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3319...  Training loss: 1.2288...  0.0927 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3320...  Training loss: 1.2302...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3321...  Training loss: 1.2758...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3322...  Training loss: 1.2402...  0.0937 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3323...  Training loss: 1.2476...  0.0938 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3324...  Training loss: 1.2416...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3325...  Training loss: 1.2626...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3326...  Training loss: 1.2424...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3327...  Training loss: 1.2161...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3328...  Training loss: 1.2739...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3329...  Training loss: 1.2784...  0.0929 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3330...  Training loss: 1.2670...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3331...  Training loss: 1.2530...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3332...  Training loss: 1.2678...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3333...  Training loss: 1.2529...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3334...  Training loss: 1.2452...  0.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3335...  Training loss: 1.2871...  0.0927 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3336...  Training loss: 1.3194...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3337...  Training loss: 1.2587...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3338...  Training loss: 1.2544...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3339...  Training loss: 1.2493...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3340...  Training loss: 1.2425...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3341...  Training loss: 1.2893...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3342...  Training loss: 1.2554...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3343...  Training loss: 1.2677...  0.0936 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3344...  Training loss: 1.2209...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3345...  Training loss: 1.2554...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3346...  Training loss: 1.2949...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3347...  Training loss: 1.2309...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3348...  Training loss: 1.2224...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3349...  Training loss: 1.2302...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.2397...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3351...  Training loss: 1.2459...  0.0933 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3352...  Training loss: 1.2433...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3353...  Training loss: 1.2512...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3354...  Training loss: 1.2379...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3355...  Training loss: 1.2798...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3356...  Training loss: 1.2444...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3357...  Training loss: 1.2594...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3358...  Training loss: 1.2517...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3359...  Training loss: 1.2212...  0.0934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3360...  Training loss: 1.2306...  0.0931 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3361...  Training loss: 1.2577...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3362...  Training loss: 1.2303...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3363...  Training loss: 1.2148...  0.0935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3364...  Training loss: 1.2488...  0.0928 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3365...  Training loss: 1.2427...  0.0932 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3366...  Training loss: 1.2422...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3367...  Training loss: 1.3872...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3368...  Training loss: 1.2718...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3369...  Training loss: 1.2536...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3370...  Training loss: 1.2884...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3371...  Training loss: 1.2287...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3372...  Training loss: 1.2222...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3373...  Training loss: 1.2507...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3374...  Training loss: 1.2572...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3375...  Training loss: 1.2656...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3376...  Training loss: 1.2443...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3377...  Training loss: 1.2427...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3378...  Training loss: 1.2511...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3379...  Training loss: 1.2604...  0.0937 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3380...  Training loss: 1.2594...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3381...  Training loss: 1.2323...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3382...  Training loss: 1.2330...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3383...  Training loss: 1.2671...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3384...  Training loss: 1.2701...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3385...  Training loss: 1.2473...  0.0938 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3386...  Training loss: 1.2800...  0.0929 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3387...  Training loss: 1.2631...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3388...  Training loss: 1.2693...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3389...  Training loss: 1.2467...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3390...  Training loss: 1.2738...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3391...  Training loss: 1.2433...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3392...  Training loss: 1.2094...  0.0927 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3393...  Training loss: 1.2192...  0.0937 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3394...  Training loss: 1.2668...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3395...  Training loss: 1.2643...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3396...  Training loss: 1.2728...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3397...  Training loss: 1.2378...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3398...  Training loss: 1.2284...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3399...  Training loss: 1.2565...  0.0929 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.2553...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3401...  Training loss: 1.2300...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3402...  Training loss: 1.2537...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3403...  Training loss: 1.2201...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3404...  Training loss: 1.2194...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3405...  Training loss: 1.2010...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3406...  Training loss: 1.2381...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3407...  Training loss: 1.2185...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3408...  Training loss: 1.2904...  0.0928 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3409...  Training loss: 1.2340...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3410...  Training loss: 1.2265...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3411...  Training loss: 1.2668...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3412...  Training loss: 1.2211...  0.0929 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3413...  Training loss: 1.2289...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3414...  Training loss: 1.2458...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3415...  Training loss: 1.2495...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3416...  Training loss: 1.2610...  0.0929 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3417...  Training loss: 1.2264...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3418...  Training loss: 1.2904...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3419...  Training loss: 1.2551...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3420...  Training loss: 1.2633...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3421...  Training loss: 1.2454...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3422...  Training loss: 1.2507...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3423...  Training loss: 1.2591...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3424...  Training loss: 1.2431...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3425...  Training loss: 1.2303...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3426...  Training loss: 1.2813...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3427...  Training loss: 1.2619...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3428...  Training loss: 1.2979...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3429...  Training loss: 1.2667...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3430...  Training loss: 1.2532...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3431...  Training loss: 1.2530...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3432...  Training loss: 1.2582...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3433...  Training loss: 1.2706...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3434...  Training loss: 1.2365...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3435...  Training loss: 1.2584...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3436...  Training loss: 1.2424...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3437...  Training loss: 1.2864...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3438...  Training loss: 1.2677...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3439...  Training loss: 1.2789...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3440...  Training loss: 1.2384...  0.0937 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3441...  Training loss: 1.2511...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3442...  Training loss: 1.2602...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3443...  Training loss: 1.2491...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3444...  Training loss: 1.2265...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3445...  Training loss: 1.2128...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3446...  Training loss: 1.2542...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3447...  Training loss: 1.2176...  0.0937 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3448...  Training loss: 1.2419...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3449...  Training loss: 1.2263...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.2485...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3451...  Training loss: 1.2189...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3452...  Training loss: 1.2411...  0.0937 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3453...  Training loss: 1.2297...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3454...  Training loss: 1.2234...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3455...  Training loss: 1.2160...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3456...  Training loss: 1.2497...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3457...  Training loss: 1.2191...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3458...  Training loss: 1.2362...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3459...  Training loss: 1.2187...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3460...  Training loss: 1.2163...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3461...  Training loss: 1.2253...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3462...  Training loss: 1.2597...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3463...  Training loss: 1.2441...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3464...  Training loss: 1.2141...  0.0937 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3465...  Training loss: 1.2222...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3466...  Training loss: 1.2284...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3467...  Training loss: 1.2474...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3468...  Training loss: 1.2361...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3469...  Training loss: 1.2449...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3470...  Training loss: 1.2449...  0.0928 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3471...  Training loss: 1.2434...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3472...  Training loss: 1.2438...  0.0928 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3473...  Training loss: 1.2470...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3474...  Training loss: 1.2470...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3475...  Training loss: 1.2400...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3476...  Training loss: 1.2621...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3477...  Training loss: 1.2364...  0.0929 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3478...  Training loss: 1.2480...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3479...  Training loss: 1.2519...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3480...  Training loss: 1.2328...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3481...  Training loss: 1.2242...  0.0938 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3482...  Training loss: 1.2121...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3483...  Training loss: 1.2535...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3484...  Training loss: 1.2521...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3485...  Training loss: 1.2398...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3486...  Training loss: 1.2397...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3487...  Training loss: 1.2411...  0.0929 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3488...  Training loss: 1.2068...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3489...  Training loss: 1.1935...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3490...  Training loss: 1.2526...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3491...  Training loss: 1.2361...  0.0928 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3492...  Training loss: 1.2039...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3493...  Training loss: 1.2508...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3494...  Training loss: 1.2478...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3495...  Training loss: 1.2279...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3496...  Training loss: 1.1985...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3497...  Training loss: 1.1934...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3498...  Training loss: 1.2198...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3499...  Training loss: 1.2665...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.2535...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3501...  Training loss: 1.2488...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3502...  Training loss: 1.2416...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3503...  Training loss: 1.2772...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3504...  Training loss: 1.2579...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3505...  Training loss: 1.2488...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3506...  Training loss: 1.2436...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3507...  Training loss: 1.2926...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3508...  Training loss: 1.2586...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3509...  Training loss: 1.2346...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3510...  Training loss: 1.2754...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3511...  Training loss: 1.2301...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3512...  Training loss: 1.2749...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3513...  Training loss: 1.2548...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3514...  Training loss: 1.2754...  0.0959 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3515...  Training loss: 1.2683...  0.0956 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3516...  Training loss: 1.2334...  0.0939 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3517...  Training loss: 1.2193...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3518...  Training loss: 1.2171...  0.0929 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3519...  Training loss: 1.2551...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3520...  Training loss: 1.2371...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3521...  Training loss: 1.2292...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3522...  Training loss: 1.2405...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3523...  Training loss: 1.2428...  0.0999 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3524...  Training loss: 1.2313...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3525...  Training loss: 1.2106...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3526...  Training loss: 1.2622...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3527...  Training loss: 1.2658...  0.0945 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3528...  Training loss: 1.2544...  0.0933 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3529...  Training loss: 1.2351...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3530...  Training loss: 1.2524...  0.0942 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3531...  Training loss: 1.2454...  0.0934 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3532...  Training loss: 1.2427...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3533...  Training loss: 1.2632...  0.0935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3534...  Training loss: 1.3030...  0.0930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3535...  Training loss: 1.2552...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3536...  Training loss: 1.2405...  0.0937 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3537...  Training loss: 1.2392...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3538...  Training loss: 1.2447...  0.0940 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3539...  Training loss: 1.2767...  0.0929 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3540...  Training loss: 1.2418...  0.1125 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3541...  Training loss: 1.2527...  0.1160 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3542...  Training loss: 1.2130...  0.1184 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3543...  Training loss: 1.2394...  0.1163 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3544...  Training loss: 1.2871...  0.1214 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3545...  Training loss: 1.2257...  0.1147 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3546...  Training loss: 1.2222...  0.1133 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3547...  Training loss: 1.2325...  0.1130 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3548...  Training loss: 1.2434...  0.1155 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3549...  Training loss: 1.2378...  0.1131 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.2310...  0.1128 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3551...  Training loss: 1.2246...  0.1241 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3552...  Training loss: 1.2229...  0.1126 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3553...  Training loss: 1.2752...  0.1204 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3554...  Training loss: 1.2269...  0.0949 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3555...  Training loss: 1.2357...  0.0936 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3556...  Training loss: 1.2336...  0.0927 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3557...  Training loss: 1.2196...  0.0953 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3558...  Training loss: 1.2162...  0.1019 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3559...  Training loss: 1.2401...  0.1196 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3560...  Training loss: 1.2200...  0.0941 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3561...  Training loss: 1.2065...  0.0932 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3562...  Training loss: 1.2427...  0.0931 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3563...  Training loss: 1.2340...  0.0942 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3564...  Training loss: 1.2323...  0.0929 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3565...  Training loss: 1.3628...  0.0930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3566...  Training loss: 1.2569...  0.0930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3567...  Training loss: 1.2395...  0.1242 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3568...  Training loss: 1.2636...  0.1305 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3569...  Training loss: 1.2218...  0.1295 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3570...  Training loss: 1.1952...  0.1281 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3571...  Training loss: 1.2409...  0.1268 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3572...  Training loss: 1.2353...  0.1346 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3573...  Training loss: 1.2420...  0.1295 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3574...  Training loss: 1.2382...  0.1351 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3575...  Training loss: 1.2261...  0.1213 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3576...  Training loss: 1.2329...  0.1270 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3577...  Training loss: 1.2374...  0.1346 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3578...  Training loss: 1.2494...  0.0939 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3579...  Training loss: 1.2278...  0.1288 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3580...  Training loss: 1.2184...  0.1349 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3581...  Training loss: 1.2550...  0.1249 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3582...  Training loss: 1.2647...  0.0940 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3583...  Training loss: 1.2478...  0.0939 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3584...  Training loss: 1.2568...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3585...  Training loss: 1.2321...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3586...  Training loss: 1.2566...  0.0928 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3587...  Training loss: 1.2348...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3588...  Training loss: 1.2503...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3589...  Training loss: 1.2305...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3590...  Training loss: 1.2012...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3591...  Training loss: 1.2210...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3592...  Training loss: 1.2492...  0.0954 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3593...  Training loss: 1.2492...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3594...  Training loss: 1.2590...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3595...  Training loss: 1.2254...  0.0929 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3596...  Training loss: 1.2047...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3597...  Training loss: 1.2415...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3598...  Training loss: 1.2488...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3599...  Training loss: 1.2198...  0.0927 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.2314...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3601...  Training loss: 1.2099...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3602...  Training loss: 1.2019...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3603...  Training loss: 1.1935...  0.0937 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3604...  Training loss: 1.2277...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3605...  Training loss: 1.2069...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3606...  Training loss: 1.2741...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3607...  Training loss: 1.2247...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3608...  Training loss: 1.1983...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3609...  Training loss: 1.2395...  0.0941 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3610...  Training loss: 1.2183...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3611...  Training loss: 1.2337...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3612...  Training loss: 1.2325...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3613...  Training loss: 1.2256...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3614...  Training loss: 1.2533...  0.0937 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3615...  Training loss: 1.2157...  0.0945 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3616...  Training loss: 1.2744...  0.0939 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3617...  Training loss: 1.2424...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3618...  Training loss: 1.2522...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3619...  Training loss: 1.2374...  0.0927 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3620...  Training loss: 1.2343...  0.0940 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3621...  Training loss: 1.2606...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3622...  Training loss: 1.2273...  0.1082 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3623...  Training loss: 1.2233...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3624...  Training loss: 1.2826...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3625...  Training loss: 1.2505...  0.0943 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3626...  Training loss: 1.2843...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3627...  Training loss: 1.2676...  0.0930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3628...  Training loss: 1.2353...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3629...  Training loss: 1.2399...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3630...  Training loss: 1.2460...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3631...  Training loss: 1.2604...  0.0929 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3632...  Training loss: 1.2258...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3633...  Training loss: 1.2422...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3634...  Training loss: 1.2276...  0.0938 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3635...  Training loss: 1.2806...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3636...  Training loss: 1.2572...  0.0978 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3637...  Training loss: 1.2692...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3638...  Training loss: 1.2171...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3639...  Training loss: 1.2309...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3640...  Training loss: 1.2585...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3641...  Training loss: 1.2386...  0.0930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3642...  Training loss: 1.2184...  0.0940 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3643...  Training loss: 1.1943...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3644...  Training loss: 1.2476...  0.0930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3645...  Training loss: 1.1982...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3646...  Training loss: 1.2371...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3647...  Training loss: 1.2146...  0.0939 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3648...  Training loss: 1.2374...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3649...  Training loss: 1.2060...  0.0937 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.2394...  0.0937 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3651...  Training loss: 1.2095...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3652...  Training loss: 1.2231...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3653...  Training loss: 1.1999...  0.0944 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3654...  Training loss: 1.2412...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3655...  Training loss: 1.2154...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3656...  Training loss: 1.2221...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3657...  Training loss: 1.2097...  0.0929 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3658...  Training loss: 1.2024...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3659...  Training loss: 1.2220...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3660...  Training loss: 1.2443...  0.0928 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3661...  Training loss: 1.2417...  0.0930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3662...  Training loss: 1.1937...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3663...  Training loss: 1.2134...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3664...  Training loss: 1.2139...  0.0937 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3665...  Training loss: 1.2452...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3666...  Training loss: 1.2326...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3667...  Training loss: 1.2375...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3668...  Training loss: 1.2253...  0.0929 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3669...  Training loss: 1.2279...  0.0928 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3670...  Training loss: 1.2330...  0.1221 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3671...  Training loss: 1.2383...  0.1338 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3672...  Training loss: 1.2386...  0.1116 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3673...  Training loss: 1.2255...  0.1214 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3674...  Training loss: 1.2540...  0.1346 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3675...  Training loss: 1.2171...  0.1351 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3676...  Training loss: 1.2456...  0.0930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3677...  Training loss: 1.2342...  0.0928 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3678...  Training loss: 1.2206...  0.0929 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3679...  Training loss: 1.2033...  0.0939 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3680...  Training loss: 1.2008...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3681...  Training loss: 1.2390...  0.0928 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3682...  Training loss: 1.2442...  0.0927 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3683...  Training loss: 1.2284...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3684...  Training loss: 1.2268...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3685...  Training loss: 1.2303...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3686...  Training loss: 1.2068...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3687...  Training loss: 1.1921...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3688...  Training loss: 1.2372...  0.0929 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3689...  Training loss: 1.2234...  0.0937 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3690...  Training loss: 1.1881...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3691...  Training loss: 1.2344...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3692...  Training loss: 1.2349...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3693...  Training loss: 1.2199...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3694...  Training loss: 1.1933...  0.0925 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3695...  Training loss: 1.1917...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3696...  Training loss: 1.2210...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3697...  Training loss: 1.2453...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3698...  Training loss: 1.2420...  0.0944 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3699...  Training loss: 1.2478...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.2363...  0.0946 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3701...  Training loss: 1.2585...  0.0988 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3702...  Training loss: 1.2539...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3703...  Training loss: 1.2363...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3704...  Training loss: 1.2388...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3705...  Training loss: 1.2741...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3706...  Training loss: 1.2570...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3707...  Training loss: 1.2339...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3708...  Training loss: 1.2653...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3709...  Training loss: 1.2161...  0.0930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3710...  Training loss: 1.2652...  0.1035 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3711...  Training loss: 1.2418...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3712...  Training loss: 1.2712...  0.0930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3713...  Training loss: 1.2582...  0.0930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3714...  Training loss: 1.2230...  0.0929 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3715...  Training loss: 1.2058...  0.0934 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3716...  Training loss: 1.2107...  0.0939 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3717...  Training loss: 1.2463...  0.0932 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3718...  Training loss: 1.2255...  0.1225 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3719...  Training loss: 1.2229...  0.1297 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3720...  Training loss: 1.2315...  0.1346 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3721...  Training loss: 1.2265...  0.1296 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3722...  Training loss: 1.2246...  0.1324 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3723...  Training loss: 1.2043...  0.1348 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3724...  Training loss: 1.2466...  0.1285 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3725...  Training loss: 1.2603...  0.1290 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3726...  Training loss: 1.2437...  0.1313 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3727...  Training loss: 1.2291...  0.1310 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3728...  Training loss: 1.2370...  0.1219 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3729...  Training loss: 1.2346...  0.1335 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3730...  Training loss: 1.2200...  0.1246 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3731...  Training loss: 1.2703...  0.1343 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3732...  Training loss: 1.2934...  0.1306 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3733...  Training loss: 1.2381...  0.1316 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3734...  Training loss: 1.2387...  0.1353 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3735...  Training loss: 1.2307...  0.1292 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3736...  Training loss: 1.2236...  0.1247 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3737...  Training loss: 1.2603...  0.1342 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3738...  Training loss: 1.2393...  0.1250 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3739...  Training loss: 1.2393...  0.1330 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3740...  Training loss: 1.2029...  0.1302 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3741...  Training loss: 1.2249...  0.1351 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3742...  Training loss: 1.2681...  0.1291 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3743...  Training loss: 1.2117...  0.1348 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3744...  Training loss: 1.1967...  0.1307 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3745...  Training loss: 1.2177...  0.1346 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3746...  Training loss: 1.2281...  0.1278 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3747...  Training loss: 1.2192...  0.1284 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3748...  Training loss: 1.2202...  0.1319 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3749...  Training loss: 1.2178...  0.1332 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.2156...  0.1043 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3751...  Training loss: 1.2592...  0.0933 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3752...  Training loss: 1.2294...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3753...  Training loss: 1.2173...  0.0942 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3754...  Training loss: 1.2260...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3755...  Training loss: 1.2040...  0.0936 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3756...  Training loss: 1.2090...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3757...  Training loss: 1.2293...  0.0939 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3758...  Training loss: 1.2129...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3759...  Training loss: 1.1928...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3760...  Training loss: 1.2317...  0.0935 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3761...  Training loss: 1.2245...  0.0931 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3762...  Training loss: 1.2184...  0.1258 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3763...  Training loss: 1.3438...  0.1158 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3764...  Training loss: 1.2547...  0.1157 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3765...  Training loss: 1.2325...  0.1152 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3766...  Training loss: 1.2517...  0.1153 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3767...  Training loss: 1.2131...  0.1229 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3768...  Training loss: 1.1926...  0.1180 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3769...  Training loss: 1.2209...  0.1246 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3770...  Training loss: 1.2196...  0.1148 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3771...  Training loss: 1.2451...  0.1136 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3772...  Training loss: 1.2228...  0.1133 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3773...  Training loss: 1.2148...  0.1242 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3774...  Training loss: 1.2253...  0.1192 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3775...  Training loss: 1.2346...  0.1199 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3776...  Training loss: 1.2408...  0.1179 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3777...  Training loss: 1.2151...  0.1176 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3778...  Training loss: 1.2095...  0.1233 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3779...  Training loss: 1.2432...  0.1054 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3780...  Training loss: 1.2420...  0.1035 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3781...  Training loss: 1.2282...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3782...  Training loss: 1.2578...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3783...  Training loss: 1.2337...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3784...  Training loss: 1.2440...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3785...  Training loss: 1.2237...  0.0940 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3786...  Training loss: 1.2585...  0.0936 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3787...  Training loss: 1.2306...  0.0983 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3788...  Training loss: 1.1878...  0.1347 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3789...  Training loss: 1.1993...  0.1293 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3790...  Training loss: 1.2414...  0.1292 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3791...  Training loss: 1.2408...  0.1330 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3792...  Training loss: 1.2449...  0.1305 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3793...  Training loss: 1.2173...  0.1039 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3794...  Training loss: 1.2128...  0.1227 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3795...  Training loss: 1.2334...  0.1316 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3796...  Training loss: 1.2366...  0.1348 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3797...  Training loss: 1.2082...  0.0944 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3798...  Training loss: 1.2331...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3799...  Training loss: 1.2004...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.1993...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3801...  Training loss: 1.1854...  0.1022 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3802...  Training loss: 1.2122...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3803...  Training loss: 1.2023...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3804...  Training loss: 1.2645...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3805...  Training loss: 1.2190...  0.0936 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3806...  Training loss: 1.1948...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3807...  Training loss: 1.2421...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3808...  Training loss: 1.2070...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3809...  Training loss: 1.2199...  0.0933 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3810...  Training loss: 1.2209...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3811...  Training loss: 1.2179...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3812...  Training loss: 1.2373...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3813...  Training loss: 1.2076...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3814...  Training loss: 1.2679...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3815...  Training loss: 1.2347...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3816...  Training loss: 1.2327...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3817...  Training loss: 1.2255...  0.1208 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3818...  Training loss: 1.2301...  0.1348 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3819...  Training loss: 1.2455...  0.1307 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3820...  Training loss: 1.2237...  0.1040 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3821...  Training loss: 1.2057...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3822...  Training loss: 1.2719...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3823...  Training loss: 1.2307...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3824...  Training loss: 1.2596...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3825...  Training loss: 1.2588...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3826...  Training loss: 1.2345...  0.0927 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3827...  Training loss: 1.2179...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3828...  Training loss: 1.2355...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3829...  Training loss: 1.2432...  0.0933 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3830...  Training loss: 1.2195...  0.0928 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3831...  Training loss: 1.2354...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3832...  Training loss: 1.2140...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3833...  Training loss: 1.2745...  0.0938 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3834...  Training loss: 1.2475...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3835...  Training loss: 1.2619...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3836...  Training loss: 1.2179...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3837...  Training loss: 1.2404...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3838...  Training loss: 1.2524...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3839...  Training loss: 1.2209...  0.0936 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3840...  Training loss: 1.2184...  0.0929 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3841...  Training loss: 1.1879...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3842...  Training loss: 1.2499...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3843...  Training loss: 1.1923...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3844...  Training loss: 1.2299...  0.0929 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3845...  Training loss: 1.2018...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3846...  Training loss: 1.2316...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3847...  Training loss: 1.1993...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3848...  Training loss: 1.2241...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3849...  Training loss: 1.2101...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.2116...  0.0936 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3851...  Training loss: 1.2029...  0.0929 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3852...  Training loss: 1.2315...  0.0936 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3853...  Training loss: 1.2037...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3854...  Training loss: 1.2064...  0.0936 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3855...  Training loss: 1.1973...  0.0933 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3856...  Training loss: 1.1924...  0.0939 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3857...  Training loss: 1.2057...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3858...  Training loss: 1.2458...  0.0942 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3859...  Training loss: 1.2303...  0.1115 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3860...  Training loss: 1.1945...  0.1152 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3861...  Training loss: 1.2083...  0.1161 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3862...  Training loss: 1.2062...  0.1223 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3863...  Training loss: 1.2264...  0.0937 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3864...  Training loss: 1.2145...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3865...  Training loss: 1.2280...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3866...  Training loss: 1.2151...  0.0933 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3867...  Training loss: 1.2099...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3868...  Training loss: 1.2134...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3869...  Training loss: 1.2265...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3870...  Training loss: 1.2308...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3871...  Training loss: 1.2034...  0.0937 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3872...  Training loss: 1.2389...  0.0937 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3873...  Training loss: 1.2101...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3874...  Training loss: 1.2395...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3875...  Training loss: 1.2329...  0.0928 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3876...  Training loss: 1.2240...  0.0929 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3877...  Training loss: 1.2041...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3878...  Training loss: 1.2004...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3879...  Training loss: 1.2333...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3880...  Training loss: 1.2280...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3881...  Training loss: 1.2220...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3882...  Training loss: 1.2138...  0.0933 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3883...  Training loss: 1.2253...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3884...  Training loss: 1.1920...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3885...  Training loss: 1.1769...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3886...  Training loss: 1.2308...  0.0936 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3887...  Training loss: 1.2051...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3888...  Training loss: 1.1829...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3889...  Training loss: 1.2363...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3890...  Training loss: 1.2259...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3891...  Training loss: 1.2115...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3892...  Training loss: 1.1800...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3893...  Training loss: 1.1765...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3894...  Training loss: 1.2090...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3895...  Training loss: 1.2479...  0.0924 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3896...  Training loss: 1.2373...  0.0929 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3897...  Training loss: 1.2288...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3898...  Training loss: 1.2317...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3899...  Training loss: 1.2604...  0.0929 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.2340...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3901...  Training loss: 1.2343...  0.0941 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3902...  Training loss: 1.2341...  0.0928 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3903...  Training loss: 1.2675...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3904...  Training loss: 1.2308...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3905...  Training loss: 1.2154...  0.1229 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3906...  Training loss: 1.2555...  0.1334 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3907...  Training loss: 1.2052...  0.1227 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3908...  Training loss: 1.2498...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3909...  Training loss: 1.2324...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3910...  Training loss: 1.2597...  0.0937 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3911...  Training loss: 1.2403...  0.0928 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3912...  Training loss: 1.2112...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3913...  Training loss: 1.1937...  0.0937 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3914...  Training loss: 1.2059...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3915...  Training loss: 1.2450...  0.0936 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3916...  Training loss: 1.2144...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3917...  Training loss: 1.2069...  0.0929 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3918...  Training loss: 1.2255...  0.1125 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3919...  Training loss: 1.2268...  0.1319 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3920...  Training loss: 1.2163...  0.1256 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3921...  Training loss: 1.1845...  0.1276 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3922...  Training loss: 1.2478...  0.1255 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3923...  Training loss: 1.2364...  0.1288 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3924...  Training loss: 1.2433...  0.1273 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3925...  Training loss: 1.2166...  0.1314 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3926...  Training loss: 1.2346...  0.1044 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3927...  Training loss: 1.2237...  0.0923 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3928...  Training loss: 1.2249...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3929...  Training loss: 1.2423...  0.0928 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3930...  Training loss: 1.2833...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3931...  Training loss: 1.2290...  0.1244 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3932...  Training loss: 1.2267...  0.1301 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3933...  Training loss: 1.2185...  0.1345 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3934...  Training loss: 1.2239...  0.1042 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3935...  Training loss: 1.2563...  0.0941 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3936...  Training loss: 1.2141...  0.1293 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3937...  Training loss: 1.2370...  0.1346 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3938...  Training loss: 1.1960...  0.1321 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3939...  Training loss: 1.2210...  0.1296 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3940...  Training loss: 1.2737...  0.1322 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3941...  Training loss: 1.1991...  0.1339 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3942...  Training loss: 1.2043...  0.1348 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3943...  Training loss: 1.2055...  0.1350 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3944...  Training loss: 1.2164...  0.0982 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3945...  Training loss: 1.2162...  0.0933 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3946...  Training loss: 1.2093...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3947...  Training loss: 1.2133...  0.0933 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3948...  Training loss: 1.1914...  0.0932 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3949...  Training loss: 1.2404...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.2229...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3951...  Training loss: 1.2197...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3952...  Training loss: 1.2273...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3953...  Training loss: 1.1909...  0.0930 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3954...  Training loss: 1.1960...  0.0937 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3955...  Training loss: 1.2302...  0.0935 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3956...  Training loss: 1.2041...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3957...  Training loss: 1.1848...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3958...  Training loss: 1.2251...  0.0931 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3959...  Training loss: 1.2171...  0.0934 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3960...  Training loss: 1.2095...  0.0932 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), \n",
    "                batch_size=batch_size, \n",
    "                num_steps=num_steps,\n",
    "                lstm_size=lstm_size, \n",
    "                num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x, model.targets: y, model.keep_prob: keep_prob, model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, model.final_state, model.optimizer],  feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3960_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3960_l512.ckpt\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling 取样\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "现在网络被训练了，我们可以用它来生成新的文本。 这个想法是我们传递一个字符，那么网络将预测下一个字符。 我们可以用新的来预测下一个。 我们继续这样做来生成所有新的文本。 我还包括一些功能，通过传递一个字符串并建立一个状态，通过一些文本来填充网络。\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n",
    "网络给了我们每个角色的预测。 为了减少噪音，让事情稍微随机一些，我将只从最可能的N个角色中选择一个新角色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network.\n",
    "\n",
    "在这里，将路径传递到检查点并从网络中抽取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i3960_l512.ckpt'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrance, was\n",
      "insisting and as it was a little.\n",
      "\n",
      "They were not for anywhere and take a promise, to see him as to be displayed\n",
      "and a long while of tenterness. She seemed to stop it. And all the\n",
      "promosents ought to think of them, waring to the doctor, he went\n",
      "into the days of all the strength and they were standing. \"That's not\n",
      "when your suppress will be so much. He can think of the ploughing\n",
      "tries to see me so wording in my contrary. I don't could be so assumate\n",
      "in any man.\"\n",
      "\n",
      "\"Well, then I shale seem in my child, and I chuldrong in all the whole\n",
      "same than yourself, and he's not sister, that that would be silent,\n",
      "tell me that you may be so left into the party. And this was it a sight\n",
      "of the propostine of all to humor.... What is a stepar are to me that\n",
      "I consider that I should not be disturbed with her. I stay them. But\n",
      "that's that is that it, it's answer and to say to the side of himself, and\n",
      "there were a service of such a party of answer. The might make how the\n",
      "servants and all she has no room...\"\n",
      "\n",
      "\"No, I'm going,\" she said to him with the corred officialit forwating\n",
      "her, as he caught him at once, but that though he had seen his\n",
      "ways to her and seen her to the street that they were asking himself\n",
      "would be a stretch from the side of her. But the carriage took her to\n",
      "say, but there, the complecess of the companies of the streems of his\n",
      "face were a sort of comminsion, but when his houre a fine hand or will\n",
      "had honorable was so much, he had been the moment, and the sick man's\n",
      "face, though she had then the sorts were something because all the past\n",
      "of his own conscious horses, which he was sitting about the chiech\n",
      "assurable of her sheal, and that she had been deceiving to spoke off\n",
      "the coming out of the creature of the professor of a peasant against\n",
      "the clushing trees they here all she.\n",
      "\n",
      "At his shirt said and all she could not confine herself in a conversation\n",
      "with him. The doctor treed his face about the morning, the door at the\n",
      "second, were alone as he had not bad several p\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fard ito th at touthe and te warer,. \n",
      "oho he and th the hhe he sin tas tha weran ho asing wor he sane heren hod, wid hhe wer than wot he shime won te ther,, hod he hhe serese sore ant his she the shed whe sarins ond heser, and had terit ans ath and, art al shering thin tar tha whimis on th wasd sit he wiss ount on ou ho he ardim hor, har thas at outhes\n",
      "tous, the sores anthas. An as the as ande whos shing on he thede th wer otiter shes so os oute he alin the sis ar and he he sher on the the he whe witon he wang and tha ad tind hhe\n",
      "ser tas sor wang and and ans ant ot hore se hare he tand the\n",
      "san to simes or the whime\n",
      "tit has and tas sar anet, than sared,, tha sot and the sime thorere ta shere an heren ot har ans orasithe the the sile the ased\n",
      "toustit an ter hhe\n",
      "sasid an ho an hhare wha he ther heed ated al tho hiss wat her sothe he has and asd an hot ans aset an or hhis ange the wherind wand he sosan her ar the whad are to te the wasthe an to te set he shes he whe shis ood sans atorins. \"\"\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faring, her ald the sonte word on she cumtinn of stere, the someting and\n",
      "andestice had the sarded, and as had to the\n",
      "perincing\n",
      "wers of his allisithers. Hus she comed a charst of she was that ho contersed theme her his, and hume as heard the\n",
      "cheathous of her timents whomess, and as had a tho dins with she seeling his was ta tares at to a steling of\n",
      "the chate him has and the pesteress in her to this\n",
      "atenesser, with hir, a thing\n",
      "ald the sarces it\n",
      "whe the soond was in the senter a sand was how shate than the peaser, who comes take to\n",
      "shice at the camlerses. He sould at that her him. The\n",
      "rastion to ham to him stonditg to his\n",
      "aberted the seaters which who he cond op timest and then.\n",
      "\n",
      "He had he sald him tole that his about the pillaster, and shating have of his foce onthe thoug her sace and sampition.\n",
      "\n",
      "\"You said, stictlest, to sang the some, while that was a ling that ally\n",
      "with her told and his beghaters, and the porsining wore, and wish to should ste he was to drase\n",
      "was to atered at and that a w\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fardy\n",
      "him, but to the more to alr thissing in the sent to hus\n",
      "say to\n",
      "to be is heart on its, and all any, what it sumpressed becimen that\n",
      "she wanted to the stroughest of the canter, and without this, and witt a spress\n",
      "the bast on the mone stance, she was said a diffient of his sorm of\n",
      "the searing with his hore, and to his face with a marry was\n",
      "as the princess of the state, which said his haste of his feeling to his\n",
      "stand toother,\n",
      "starding that he could not huld the his hand as a luct as to a solt their\n",
      "husbond, who heart the marere of this peased--whe mashe to say their\n",
      "wonder of the same andicultrain to the much while though and attered his\n",
      "ene and had been at the mone and so suthing with seeming of\n",
      "the\n",
      "pettirugion of her suct and stepling at the mirst, he wanting. He had say\n",
      "seresticing at thrigg at hour at him will breas too, as this\n",
      "in the rand he heart, and his starded in the same of the real though had\n",
      "a\n",
      "preamor, taked\n",
      "it, and the beath of her she had been happy, and welk to thilk he\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farding\n",
      "the posertance of the high trat would back that the hand with a clover to a coust as\n",
      "he\n",
      "was all a sate in the count sending all with the country.\n",
      "\n",
      "\"Well, what along, the cheets,\" said Anna.\n",
      "\n",
      "\"It's a generant of the labor of all,\" said the same smire, which was\n",
      "commoned with which he was so little attange in to him, and were\n",
      "streem in his hasse, before it in his wife.\n",
      "\n",
      "\"I did not lade it, a mund of their cours of her, and a door as he selm\n",
      "she had been the simely of the moment and things, as that we couldn't\n",
      "go and she saw that? How is the princess were true to brought the clear,\n",
      "and the more of sonien, to be stood on in the prince, and had to spak in a\n",
      "brother settled in a streng in spinish. But, but he was time that was not\n",
      "so she was to a pensing as it was a man taken in all the stand, that\n",
      "that is needed to the pate, was such it to the prover of the stands.\n",
      "\n",
      "\n",
      "\n",
      "Countess Bordsension that he was a geast was that he was anything so that\n",
      "she saw them to be an indident the position; a\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i2000_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrace he had sooned\n",
      "him.\n",
      "\n",
      "\"I can't the marrale then what's the province. I say so it's an and of\n",
      "her feers in howrer, as you were not into the carriage to her, I shall\n",
      "can any thee and marralen of me to tear to be in anything.\"\n",
      "\n",
      "The middle of his soft, went into what she was attempted to them, and\n",
      "he was stronger at the same a creetures of his shirt, and went on to\n",
      "the states, though he was doing her tears, hid his face with which had been\n",
      "calming to her all of the composure to her hands it.\n",
      "\n",
      "And seemed to the principle of his carming, who strongen so told him that\n",
      "he should never say that there alone went to the prince and this\n",
      "words were not between the carriage, and she had said to his hands to\n",
      "the same starrations of his fasting that he could not came to the\n",
      "path.\n",
      "\n",
      "They were naid as a people, and he was telling the care of any calriage\n",
      "and shaming, and were too stopping and so as he had not telling her son.\n",
      "The disconfent of the same and had been to be asked, and that was netted\n",
      "him,\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i3000_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
